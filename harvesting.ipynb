{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Papers Visualization - Phase 1\n",
    "\n",
    "This notebook implements Phase 1 of the AI Safety Papers visualization project:\n",
    "1. Metadata Collection using arXiv OAI-PMH API\n",
    "2. Abstract Embedding Generation using ModernBERT-large\n",
    "3. Initial Clustering using UMAP and HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Clone the repository\n",
    "    !git clone https://github.com/moiri-gamboni/ai-safety-landscape.git\n",
    "    %cd ai-safety-landscape\n",
    "\n",
    "# Install required packages\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Load Existing Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check if database exists in Drive\n",
    "db_path = \"/content/drive/MyDrive/ai-safety-papers/papers.db\"\n",
    "if os.path.exists(db_path):\n",
    "    print(f\"Found existing database at {db_path}\")\n",
    "    !cp \"{db_path}\" papers.db\n",
    "    \n",
    "    # Print existing data summary\n",
    "    conn = sqlite3.connect('papers.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT COUNT(*) FROM papers')\n",
    "    print(f\"Database contains {c.fetchone()[0]} papers\")\n",
    "else:\n",
    "    print(\"No existing database found in Drive. Will create new one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Create Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "def create_database():\n",
    "    \"\"\"Create SQLite database with necessary tables\"\"\"\n",
    "    conn = sqlite3.connect('papers.db')\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Print SQLite variable limit\n",
    "    max_vars = get_sqlite_variable_limit(conn)\n",
    "    print(f\"SQLite max variables per query: {max_vars}\")\n",
    "    \n",
    "    # Create papers table with all arXiv metadata fields\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS papers (\n",
    "            id TEXT PRIMARY KEY,           -- Required, maxOccurs=1\n",
    "            title TEXT,                    -- Optional, maxOccurs=1\n",
    "            abstract TEXT,                 -- Optional, maxOccurs=1\n",
    "            categories TEXT,               -- Optional, maxOccurs=1\n",
    "            msc_class TEXT,                -- Optional, maxOccurs=1\n",
    "            acm_class TEXT,                -- Optional, maxOccurs=1\n",
    "            doi TEXT,                      -- Optional, maxOccurs=1\n",
    "            license TEXT,                  -- Optional, maxOccurs=1\n",
    "            comments TEXT,                 -- Optional, maxOccurs=1\n",
    "            created TEXT,                  -- Date of first version\n",
    "            updated TEXT,                  -- Date of latest version\n",
    "            withdrawn BOOLEAN DEFAULT 0,    -- Indicates if paper is withdrawn\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create paper versions table to track version history\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS paper_versions (\n",
    "            paper_id TEXT,\n",
    "            version INTEGER,\n",
    "            source_type TEXT,              -- D for Document, I for Inactive\n",
    "            size TEXT,                     -- Size in kb\n",
    "            date TEXT,                     -- Submission date of this version\n",
    "            PRIMARY KEY (paper_id, version),\n",
    "            FOREIGN KEY (paper_id) REFERENCES papers(id)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create authors table matching arXiv schema\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS authors (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            keyname TEXT NOT NULL,          -- Required, maxOccurs=1\n",
    "            forenames TEXT,                 -- Optional, maxOccurs=1\n",
    "            suffix TEXT,                    -- Optional, maxOccurs=1\n",
    "            UNIQUE(keyname, forenames, suffix)  -- Avoid duplicate authors\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create paper_authors junction table\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS paper_authors (\n",
    "            paper_id TEXT,\n",
    "            author_id INTEGER,\n",
    "            author_position INTEGER,        -- Track author order in paper\n",
    "            PRIMARY KEY (paper_id, author_id),\n",
    "            FOREIGN KEY (paper_id) REFERENCES papers(id),\n",
    "            FOREIGN KEY (author_id) REFERENCES authors(id)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create indices for common queries after all tables are created\n",
    "    c.execute(\"CREATE INDEX IF NOT EXISTS idx_categories ON papers(categories)\")\n",
    "    c.execute(\"CREATE INDEX IF NOT EXISTS idx_withdrawn ON papers(withdrawn)\")\n",
    "    c.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS idx_authors_unique ON authors(keyname, forenames, suffix)\")\n",
    "    \n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "# Initialize database\n",
    "conn = create_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Harvest Metadata\n",
    "\n",
    "### Configuration\n",
    "- **Number of Papers**: Control how many papers to fetch. Set to 0 to fetch all CS papers (warning: this will take a long time).\n",
    "- **Resumption Token**: If harvesting was interrupted, paste the resumption token here to continue from where you left off.\n",
    "  You can paste the full URL or just the token - URL-encoded characters (like %7C) will be automatically converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Harvesting Configuration  {\"run\":\"auto\"}\n",
    "num_papers = 100 # @param {type:\"slider\", min:0, max:10000, step:100}\n",
    "resumption_token = \"\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from sickle import Sickle\n",
    "from sickle.models import Record\n",
    "from tqdm import tqdm\n",
    "import urllib.parse\n",
    "import re\n",
    "\n",
    "def get_sqlite_variable_limit(conn):\n",
    "    \"\"\"Get the maximum number of variables allowed in a SQLite query\"\"\"\n",
    "    c = conn.cursor()\n",
    "    c.execute('PRAGMA compile_options')\n",
    "    compile_options = c.fetchall()\n",
    "    for option in compile_options:\n",
    "        if 'MAX_VARIABLE_NUMBER=' in option[0]:\n",
    "            return int(option[0].split('=')[1])\n",
    "    return 999  # Default SQLite limit if not found\n",
    "\n",
    "def get_safe_batch_size(conn, vars_per_item=1):\n",
    "    \"\"\"Calculate a safe batch size based on SQLite's variable limit\n",
    "    \n",
    "    Args:\n",
    "        conn: SQLite connection\n",
    "        vars_per_item: Number of variables needed per item in a batch\n",
    "        \n",
    "    Returns:\n",
    "        int: Safe batch size that won't exceed SQLite's variable limit\n",
    "    \"\"\"\n",
    "    max_vars = get_sqlite_variable_limit(conn)\n",
    "    # Use 90% of the limit to be safe\n",
    "    safe_vars = int(max_vars * 0.9)\n",
    "    return safe_vars // vars_per_item\n",
    "\n",
    "class ArxivRecord(Record):\n",
    "    \"\"\"Custom record class for arXiv metadata format\"\"\"\n",
    "    def get_metadata(self):\n",
    "        # Get the arXiv metadata namespace\n",
    "        ns = {'arxiv': 'http://arxiv.org/OAI/arXiv/'}\n",
    "        \n",
    "        # Get the arXiv element which contains all metadata\n",
    "        arxiv = self.xml.find('.//{http://arxiv.org/OAI/arXiv/}arXiv')\n",
    "        if arxiv is None:\n",
    "            raise ValueError(\"Could not find arXiv metadata element\")\n",
    "            \n",
    "        metadata = {}\n",
    "        \n",
    "        # Required field - use identifier from header\n",
    "        metadata['id'] = self.header.identifier\n",
    "        if metadata['id'].startswith('oai:arXiv.org:'):\n",
    "            metadata['id'] = metadata['id'].replace('oai:arXiv.org:', '')\n",
    "        \n",
    "        # Map arXiv metadata fields according to schema\n",
    "        field_mapping = {\n",
    "            'created': 'created',\n",
    "            'updated': 'updated',\n",
    "            'title': 'title',\n",
    "            'abstract': 'abstract',\n",
    "            'categories': 'categories',\n",
    "            'msc_class': 'msc-class',\n",
    "            'acm_class': 'acm-class',\n",
    "            'report_no': 'report-no',\n",
    "            'journal_ref': 'journal-ref',\n",
    "            'doi': 'doi',\n",
    "            'comments': 'comments',\n",
    "            'license': 'license'\n",
    "        }\n",
    "        \n",
    "        for field, xml_field in field_mapping.items():\n",
    "            elem = arxiv.find(f'arxiv:{xml_field}', namespaces=ns)\n",
    "            metadata[field] = elem.text if elem is not None and elem.text else None\n",
    "        \n",
    "        # Extract authors according to schema\n",
    "        authors = []\n",
    "        authors_elem = arxiv.find('arxiv:authors', namespaces=ns)\n",
    "        if authors_elem is not None:\n",
    "            for author_elem in authors_elem.findall('arxiv:author', namespaces=ns):\n",
    "                if author_elem is None:\n",
    "                    continue\n",
    "                    \n",
    "                author = {}\n",
    "                \n",
    "                # Required keyname field\n",
    "                keyname_elem = author_elem.find('arxiv:keyname', namespaces=ns)\n",
    "                if keyname_elem is None or not keyname_elem.text:\n",
    "                    continue  # Skip authors without keyname\n",
    "                author['keyname'] = keyname_elem.text.rstrip(',')  # Remove trailing comma if present\n",
    "                \n",
    "                # Optional author fields\n",
    "                forenames_elem = author_elem.find('arxiv:forenames', namespaces=ns)\n",
    "                author['forenames'] = forenames_elem.text if forenames_elem is not None and forenames_elem.text else None\n",
    "                \n",
    "                suffix_elem = author_elem.find('arxiv:suffix', namespaces=ns)\n",
    "                author['suffix'] = suffix_elem.text if suffix_elem is not None and suffix_elem.text else None\n",
    "                \n",
    "                authors.append(author)\n",
    "        \n",
    "        metadata['authors'] = authors\n",
    "        return metadata\n",
    "\n",
    "class ArxivRawRecord(Record):\n",
    "    \"\"\"Custom record class for arXivRaw metadata format\"\"\"\n",
    "    def get_metadata(self):\n",
    "        # Get the arXivRaw metadata namespace\n",
    "        ns = {'arxiv': 'http://arxiv.org/OAI/arXivRaw/'}\n",
    "        \n",
    "        # Get the arXivRaw element which contains all metadata\n",
    "        arxiv = self.xml.find('.//{http://arxiv.org/OAI/arXivRaw/}arXivRaw')\n",
    "        if arxiv is None:\n",
    "            raise ValueError(\"Could not find arXivRaw metadata element\")\n",
    "            \n",
    "        metadata = {}\n",
    "        \n",
    "        # Required field - use identifier from header\n",
    "        metadata['id'] = self.header.identifier\n",
    "        if metadata['id'].startswith('oai:arXiv.org:'):\n",
    "            metadata['id'] = metadata['id'].replace('oai:arXiv.org:', '')\n",
    "        \n",
    "        # Get all versions and sort by version number\n",
    "        versions = []\n",
    "        for version_elem in arxiv.findall('.//arxiv:version', namespaces=ns):\n",
    "            version_num = version_elem.get('version', 'v1').lstrip('v')\n",
    "            try:\n",
    "                version_num = int(version_num)\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            version_info = {\n",
    "                'version': version_num,\n",
    "                'date': version_elem.find('arxiv:date', namespaces=ns).text if version_elem.find('arxiv:date', namespaces=ns) is not None else None,\n",
    "                'size': version_elem.find('arxiv:size', namespaces=ns).text if version_elem.find('arxiv:size', namespaces=ns) is not None else None,\n",
    "                'source_type': version_elem.find('arxiv:source_type', namespaces=ns).text if version_elem.find('arxiv:source_type', namespaces=ns) is not None else 'D'\n",
    "            }\n",
    "            versions.append(version_info)\n",
    "        \n",
    "        # Sort versions by version number\n",
    "        versions.sort(key=lambda x: x['version'])\n",
    "        metadata['versions'] = versions\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "def save_papers(papers, conn):\n",
    "    \"\"\"Save papers and authors to SQLite database\"\"\"\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    def normalize_suffix(suffix):\n",
    "        \"\"\"Normalize author suffixes to a standard format\"\"\"\n",
    "        if not suffix:\n",
    "            return None\n",
    "        suffix = suffix.strip()\n",
    "        # Normalize Jr variations\n",
    "        if suffix.upper() in ['JR', 'JR.', 'JR ', 'JUNIOR']:\n",
    "            return 'Jr.'\n",
    "        # Normalize Sr variations\n",
    "        if suffix.upper() in ['SR', 'SR.', 'SR ', 'SENIOR']:\n",
    "            return 'Sr.'\n",
    "        # Normalize roman numerals\n",
    "        if suffix.upper() in ['I', 'II', 'III', 'IV', 'V']:\n",
    "            return suffix.upper()\n",
    "        return suffix\n",
    "    \n",
    "    print(f\"\\nSaving {len(papers)} papers and their authors...\")\n",
    "    with tqdm(total=len(papers), desc=\"Saving papers\", unit=\" papers\",\n",
    "              miniters=500,\n",
    "              smoothing=0.8\n",
    "              ) as pbar:\n",
    "        for paper in papers:\n",
    "            try:\n",
    "                # Insert paper with all fields\n",
    "                c.execute('''\n",
    "                    INSERT OR IGNORE INTO papers (\n",
    "                        id, title, abstract, categories,\n",
    "                        msc_class, acm_class, doi, license, comments\n",
    "                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                ''', (\n",
    "                    paper['id'], \n",
    "                    paper.get('title'),\n",
    "                    paper.get('abstract'),\n",
    "                    paper.get('categories'),\n",
    "                    paper.get('msc_class'),\n",
    "                    paper.get('acm_class'),\n",
    "                    paper.get('doi'),\n",
    "                    paper.get('license'),\n",
    "                    paper.get('comments')\n",
    "                ))\n",
    "                \n",
    "                # Process authors\n",
    "                for pos, author in enumerate(paper['authors'], 1):\n",
    "                    # Clean and normalize author fields\n",
    "                    keyname = author['keyname'].strip()\n",
    "                    forenames = author.get('forenames', '').strip() if author.get('forenames') else ''\n",
    "                    suffix = normalize_suffix(author.get('suffix'))\n",
    "                    \n",
    "                    # First try to find existing author with normalized fields\n",
    "                    c.execute('''\n",
    "                        SELECT id FROM authors \n",
    "                        WHERE keyname = ? AND \n",
    "                              COALESCE(forenames, '') = ? AND\n",
    "                              COALESCE(suffix, '') = COALESCE(?, '')\n",
    "                    ''', (\n",
    "                        keyname,\n",
    "                        forenames,\n",
    "                        suffix\n",
    "                    ))\n",
    "                    result = c.fetchone()\n",
    "                    \n",
    "                    if result:\n",
    "                        author_id = result[0]\n",
    "                    else:\n",
    "                        # Insert new author with normalized fields\n",
    "                        c.execute('''\n",
    "                            INSERT INTO authors (keyname, forenames, suffix)\n",
    "                            VALUES (?, ?, ?)\n",
    "                        ''', (\n",
    "                            keyname,\n",
    "                            forenames if forenames else None,  # Store NULL if empty\n",
    "                            suffix  # Already normalized\n",
    "                        ))\n",
    "                        author_id = c.lastrowid\n",
    "                    \n",
    "                    # Create paper-author relationship\n",
    "                    c.execute('''\n",
    "                        INSERT OR IGNORE INTO paper_authors (paper_id, author_id, author_position)\n",
    "                        VALUES (?, ?, ?)\n",
    "                    ''', (paper['id'], author_id, pos))\n",
    "                \n",
    "                pbar.update(1)\n",
    "                    \n",
    "            except sqlite3.IntegrityError as e:\n",
    "                print(f\"Error saving paper {paper['id']}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    conn.commit()\n",
    "\n",
    "def save_versions(paper_versions, conn):\n",
    "    \"\"\"Save paper version information to database efficiently\"\"\"\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Calculate batch sizes\n",
    "    VERSION_BATCH_SIZE = get_safe_batch_size(conn, vars_per_item=5)  # 5 vars per version\n",
    "    PAPER_UPDATE_BATCH_SIZE = get_safe_batch_size(conn, vars_per_item=4)  # 4 vars per paper update\n",
    "    \n",
    "    print(f\"Using batch sizes:\")\n",
    "    print(f\"- Versions: {VERSION_BATCH_SIZE}\")\n",
    "    print(f\"- Paper Updates: {PAPER_UPDATE_BATCH_SIZE}\")\n",
    "    \n",
    "    # Prepare batch data\n",
    "    paper_updates = []\n",
    "    version_inserts = []\n",
    "    \n",
    "    # Group versions by paper ID\n",
    "    paper_version_map = {}\n",
    "    for paper in paper_versions:\n",
    "        paper_id = paper['id']\n",
    "        if 'versions' in paper:\n",
    "            paper_version_map[paper_id] = paper['versions']\n",
    "    \n",
    "    # Collect all data first\n",
    "    for paper_id, versions in paper_version_map.items():\n",
    "        # Sort versions by version number\n",
    "        versions = sorted(versions, key=lambda x: x['version'])\n",
    "        \n",
    "        if versions:\n",
    "            # Get first and latest version dates\n",
    "            created = versions[0]['date']\n",
    "            updated = versions[-1]['date']\n",
    "            \n",
    "            # Check if paper is withdrawn based on latest version\n",
    "            latest_version = versions[-1]\n",
    "            withdrawn = (\n",
    "                latest_version['source_type'] == 'I' or\n",
    "                latest_version['size'] == '0kb'\n",
    "            )\n",
    "            \n",
    "            # Add to paper updates\n",
    "            paper_updates.append((created, updated, withdrawn, paper_id))\n",
    "            \n",
    "            # Add all versions\n",
    "            for version in versions:\n",
    "                version_inserts.append((\n",
    "                    paper_id,\n",
    "                    version['version'],\n",
    "                    version['source_type'],\n",
    "                    version['size'],\n",
    "                    version['date']\n",
    "                ))\n",
    "    \n",
    "    try:\n",
    "        # Batch update papers\n",
    "        for i in range(0, len(paper_updates), PAPER_UPDATE_BATCH_SIZE):\n",
    "            batch = paper_updates[i:i + PAPER_UPDATE_BATCH_SIZE]\n",
    "            c.executemany('''\n",
    "                UPDATE papers \n",
    "                SET created = ?, updated = ?, withdrawn = ?\n",
    "                WHERE id = ?\n",
    "            ''', batch)\n",
    "        \n",
    "        # Batch insert versions\n",
    "        for i in range(0, len(version_inserts), VERSION_BATCH_SIZE):\n",
    "            batch = version_inserts[i:i + VERSION_BATCH_SIZE]\n",
    "            c.executemany('''\n",
    "                INSERT OR IGNORE INTO paper_versions (\n",
    "                    paper_id, version, source_type, size, date\n",
    "                ) VALUES (?, ?, ?, ?, ?)\n",
    "            ''', batch)\n",
    "        \n",
    "        conn.commit()\n",
    "        \n",
    "    except sqlite3.IntegrityError as e:\n",
    "        print(f\"Error during batch operations: {str(e)}\")\n",
    "        conn.rollback()\n",
    "\n",
    "def fetch_arxiv_records(metadata_prefix, record_class, max_results=None, resumption_token=None):\n",
    "    \"\"\"Base function for fetching records from arXiv OAI-PMH API\"\"\"\n",
    "    sickle = Sickle('http://export.arxiv.org/oai2',\n",
    "                    max_retries=5,\n",
    "                    retry_status_codes=[503],\n",
    "                    default_retry_after=20)\n",
    "    \n",
    "    # Register our custom record class\n",
    "    sickle.class_mapping['ListRecords'] = record_class\n",
    "    \n",
    "    try:\n",
    "        # Use ListRecords with specified format\n",
    "        if resumption_token:\n",
    "            # Extract token from URL if full URL was pasted\n",
    "            if 'resumptionToken=' in resumption_token:\n",
    "                resumption_token = re.search(r'resumptionToken=([^&]+)', resumption_token).group(1)\n",
    "            # Unescape the token\n",
    "            resumption_token = urllib.parse.unquote(resumption_token)\n",
    "            records = sickle.ListRecords(resumptionToken=resumption_token)\n",
    "        else:\n",
    "            records = sickle.ListRecords(\n",
    "                metadataPrefix=metadata_prefix,\n",
    "                set='cs',\n",
    "                ignore_deleted=True\n",
    "            )\n",
    "        \n",
    "        # Process records with progress bar\n",
    "        results = []\n",
    "        with tqdm(desc=f\"Fetching {metadata_prefix}\", unit=\" papers\",\n",
    "                 miniters=25,\n",
    "                 smoothing=0.8\n",
    "                 ) as pbar:\n",
    "            while True:\n",
    "                try:\n",
    "                    record = next(records)\n",
    "                    try:\n",
    "                        metadata = record.get_metadata()\n",
    "                        if metadata and metadata.get('id'):  # Only add if we have valid metadata\n",
    "                            results.append(metadata)\n",
    "                            pbar.update(1)\n",
    "                            \n",
    "                            # Respect max_results limit if set\n",
    "                            if max_results and len(results) >= max_results:\n",
    "                                break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing record: {str(e)}\")\n",
    "                        continue\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    # Get the current resumption token from the Sickle iterator\n",
    "                    if hasattr(records, 'resumption_token'):\n",
    "                        print(f\"\\nError occurred. Current resumption token: {records.resumption_token}\")\n",
    "                    raise  # Re-raise the exception to let Sickle's retry mechanism handle it\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during harvesting: {str(e)}\")\n",
    "        # Print final resumption token if available\n",
    "        if hasattr(records, 'resumption_token'):\n",
    "            print(f\"Final resumption token: {records.resumption_token}\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "def fetch_cs_papers(max_results=None, resumption_token=None):\n",
    "    \"\"\"Fetch CS papers from arXiv using OAI-PMH with arXiv metadata format\"\"\"\n",
    "    return fetch_arxiv_records('arXiv', ArxivRecord, max_results, resumption_token)\n",
    "\n",
    "def fetch_raw_metadata(max_results=None, resumption_token=None):\n",
    "    \"\"\"Fetch paper version information from arXiv using OAI-PMH with arXivRaw metadata format\"\"\"\n",
    "    return fetch_arxiv_records('arXivRaw', ArxivRawRecord, max_results, resumption_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Harvest arXiv Metadata\n",
    "This cell fetches the main metadata (titles, abstracts, authors, etc.) from arXiv's OAI-PMH API.\n",
    "You can run this cell independently and save the database before proceeding to raw metadata harvesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch papers using the slider value (None for all papers)\n",
    "# First check current database count\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT COUNT(*) FROM papers')\n",
    "current_count = c.fetchone()[0]\n",
    "print(f\"Current papers in database before harvesting: {current_count}\")\n",
    "\n",
    "# Fetch papers with arXiv metadata\n",
    "initial_papers = fetch_cs_papers(\n",
    "    max_results=num_papers if num_papers > 0 else None,\n",
    "    resumption_token=resumption_token if resumption_token else None\n",
    ")\n",
    "save_papers(initial_papers, conn)\n",
    "\n",
    "# Print final count\n",
    "c.execute('SELECT COUNT(*) FROM papers')\n",
    "final_count = c.fetchone()[0]\n",
    "print(f\"\\nMetadata harvesting complete:\")\n",
    "print(f\"- Papers added this session: {final_count - current_count}\")\n",
    "print(f\"- Total papers in database: {final_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Harvest arXiv Raw Metadata\n",
    "This cell fetches additional metadata about paper versions from arXiv's OAI-PMH API using the arXivRaw format.\n",
    "You can run this cell separately after running the main metadata harvesting above.\n",
    "\n",
    "**Note**: Make sure to run the database loading cell first if you're running this in a new session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Fetch version information with arXivRaw\n",
    "paper_versions = fetch_raw_metadata(\n",
    "    max_results=num_papers if num_papers > 0 else None,\n",
    "    resumption_token=resumption_token if resumption_token else None\n",
    ")\n",
    "save_versions(paper_versions, conn)\n",
    "\n",
    "# Print final stats about versions\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT COUNT(*) FROM paper_versions')\n",
    "version_count = c.fetchone()[0]\n",
    "c.execute('SELECT COUNT(DISTINCT paper_id) FROM paper_versions')\n",
    "papers_with_versions = c.fetchone()[0]\n",
    "print(f\"\\nRaw metadata harvesting complete:\")\n",
    "print(f\"- Total versions stored: {version_count}\")\n",
    "print(f\"- Papers with version info: {papers_with_versions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_papers(conn, limit=5):\n",
    "    \"\"\"Print detailed information for a sample of papers\"\"\"\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Get sample papers with their authors\n",
    "    c.execute('''\n",
    "        SELECT p.id, p.title, p.abstract, p.categories, p.created, p.updated,\n",
    "               GROUP_CONCAT(a.keyname || COALESCE(', ' || a.forenames, '') || COALESCE(' ' || a.suffix, ''))\n",
    "        FROM papers p\n",
    "        LEFT JOIN paper_authors pa ON p.id = pa.paper_id\n",
    "        LEFT JOIN authors a ON pa.author_id = a.id\n",
    "        GROUP BY p.id\n",
    "        LIMIT ?\n",
    "    ''', (limit,))\n",
    "    \n",
    "    papers = c.fetchall()\n",
    "    \n",
    "    print(f\"Inspecting {len(papers)} sample papers:\\n\")\n",
    "    for paper in papers:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ID: {paper[0]}\")\n",
    "        print(f\"Title: {paper[1]}\")\n",
    "        print(f\"Abstract: {paper[2][:200]}...\" if paper[2] else \"Abstract: None\")\n",
    "        print(f\"Categories: {paper[3]}\")\n",
    "        print(f\"Created: {paper[4]}\")\n",
    "        print(f\"Updated: {paper[5]}\")\n",
    "        print(f\"Authors: {paper[6]}\")\n",
    "        print()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\nDatabase Statistics:\")\n",
    "    \n",
    "    # Paper statistics\n",
    "    c.execute('SELECT COUNT(*) FROM papers')\n",
    "    total_papers = c.fetchone()[0]\n",
    "    print(f\"\\nPapers:\")\n",
    "    print(f\"Total Papers: {total_papers}\")\n",
    "    \n",
    "    # Withdrawn paper statistics\n",
    "    c.execute('SELECT COUNT(*) FROM papers WHERE withdrawn = 1')\n",
    "    withdrawn_count = c.fetchone()[0]\n",
    "    c.execute('SELECT COUNT(*) FROM papers WHERE withdrawn = 0')\n",
    "    active_count = c.fetchone()[0]\n",
    "    print(f\"Active Papers: {active_count} ({(active_count/total_papers*100):.1f}% of total)\")\n",
    "    print(f\"Withdrawn Papers: {withdrawn_count} ({(withdrawn_count/total_papers*100):.1f}% of total)\")\n",
    "    \n",
    "    # CS Primary papers\n",
    "    c.execute('''\n",
    "        WITH split_categories AS (\n",
    "            SELECT id, withdrawn,\n",
    "                   TRIM(SUBSTR(categories, 1, INSTR(categories || ' ', ' ') - 1)) as primary_category\n",
    "            FROM papers\n",
    "            WHERE categories IS NOT NULL\n",
    "        )\n",
    "        SELECT \n",
    "            COUNT(*) as total,\n",
    "            SUM(CASE WHEN withdrawn = 0 THEN 1 ELSE 0 END) as active\n",
    "        FROM split_categories \n",
    "        WHERE primary_category LIKE 'cs.%'\n",
    "    ''')\n",
    "    cs_stats = c.fetchone()\n",
    "    cs_total, cs_active = cs_stats\n",
    "    print(f\"\\nCS primary papers: {cs_total} ({(cs_total/total_papers*100):.1f}% of total)\")\n",
    "    print(f\"Active CS primary papers: {cs_active} ({(cs_active/cs_total*100):.1f}% of CS papers)\")\n",
    "    \n",
    "    # Category statistics\n",
    "    print(\"\\nTop Categories:\")\n",
    "    c.execute('''\n",
    "        SELECT category, COUNT(*) as count\n",
    "        FROM (\n",
    "            SELECT TRIM(value) as category\n",
    "            FROM papers, json_each('[\"' || REPLACE(categories, ' ', '\",\"') || '\"]')\n",
    "            WHERE categories IS NOT NULL\n",
    "        )\n",
    "        WHERE category != ''\n",
    "        GROUP BY category\n",
    "        ORDER BY count DESC\n",
    "        LIMIT 10\n",
    "    ''')\n",
    "    for category, count in c.fetchall():\n",
    "        if category:  # Skip empty category\n",
    "            percentage = (count / total_papers) * 100\n",
    "            print(f\"{category}: {count} papers ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Author statistics\n",
    "    print(f\"\\nAuthors:\")\n",
    "    c.execute('SELECT COUNT(*) FROM authors')\n",
    "    total_authors = c.fetchone()[0]\n",
    "    print(f\"Total Authors: {total_authors}\")\n",
    "    \n",
    "    c.execute('SELECT COUNT(*) FROM paper_authors')\n",
    "    total_author_links = c.fetchone()[0]\n",
    "    print(f\"Total Author-Paper Links: {total_author_links}\")\n",
    "    \n",
    "    # Papers per author distribution\n",
    "    print(\"\\nPapers per author distribution:\")\n",
    "    c.execute('''\n",
    "        SELECT papers_count, COUNT(*) as authors_with_this_many_papers\n",
    "        FROM (\n",
    "            SELECT author_id, COUNT(*) as papers_count\n",
    "            FROM paper_authors\n",
    "            GROUP BY author_id\n",
    "        )\n",
    "        GROUP BY papers_count\n",
    "        ORDER BY papers_count\n",
    "        LIMIT 10\n",
    "    ''')\n",
    "    for paper_count, author_count in c.fetchall():\n",
    "        print(f\"{author_count} authors have {paper_count} paper(s)\")\n",
    "    \n",
    "    c.execute('''\n",
    "        SELECT COUNT(*) FROM \n",
    "        (SELECT author_id FROM paper_authors GROUP BY author_id HAVING COUNT(*) > 1)\n",
    "    ''')\n",
    "    authors_multiple_papers = c.fetchone()[0]\n",
    "    print(f\"\\nAuthors with Multiple Papers: {authors_multiple_papers}\")\n",
    "    \n",
    "    c.execute('SELECT AVG(author_count) FROM (SELECT paper_id, COUNT(*) as author_count FROM paper_authors GROUP BY paper_id)')\n",
    "    avg_authors_per_paper = c.fetchone()[0]\n",
    "    print(f\"\\nAverage Authors per Paper: {avg_authors_per_paper:.2f}\")\n",
    "    \n",
    "    # Metadata field statistics\n",
    "    print(\"\\nMetadata Field Coverage:\")\n",
    "    fields = [\n",
    "        'title', 'abstract', 'categories', 'created', 'updated',\n",
    "        'msc_class', 'acm_class', 'doi', 'license'\n",
    "    ]\n",
    "    \n",
    "    for field in fields:\n",
    "        c.execute(f'SELECT COUNT(*) FROM papers WHERE {field} IS NOT NULL')\n",
    "        present = c.fetchone()[0]\n",
    "        percentage = (present / total_papers) * 100 if total_papers > 0 else 0\n",
    "        print(f\"{field}: {present}/{total_papers} ({percentage:.1f}%)\")\n",
    "\n",
    "    # Duplicate title analysis\n",
    "    def normalize_title(title):\n",
    "        \"\"\"Normalize title for comparison\"\"\"\n",
    "        if not title:\n",
    "            return \"\"\n",
    "        # Convert to lowercase\n",
    "        title = title.lower()\n",
    "        # Remove version numbers\n",
    "        title = re.sub(r'\\bv\\d+\\b', '', title)\n",
    "        # Remove arXiv identifiers\n",
    "        title = re.sub(r'arxiv:\\d+\\.\\d+', '', title, flags=re.IGNORECASE)\n",
    "        # Remove punctuation and normalize whitespace\n",
    "        title = re.sub(r'[^\\w\\s]', '', title)\n",
    "        return ' '.join(title.split())\n",
    "    \n",
    "    print(\"\\nAnalyzing duplicate titles...\")\n",
    "    c.execute('''\n",
    "        SELECT \n",
    "            p.id,\n",
    "            p.title,\n",
    "            p.categories,\n",
    "            GROUP_CONCAT(a.keyname || COALESCE(', ' || a.forenames, '') || COALESCE(' ' || a.suffix, '')) as authors\n",
    "        FROM papers p\n",
    "        LEFT JOIN paper_authors pa ON p.id = pa.paper_id\n",
    "        LEFT JOIN authors a ON pa.author_id = a.id\n",
    "        WHERE p.title IS NOT NULL AND p.withdrawn = 0  -- Only analyze active papers\n",
    "        GROUP BY p.id\n",
    "    ''')\n",
    "    papers = c.fetchall()\n",
    "    \n",
    "    # Group by normalized title\n",
    "    title_groups = {}\n",
    "    for paper in papers:\n",
    "        norm_title = normalize_title(paper[1])\n",
    "        if norm_title in title_groups:\n",
    "            title_groups[norm_title].append(paper)\n",
    "        else:\n",
    "            title_groups[norm_title] = [paper]\n",
    "    \n",
    "    # Find groups with multiple papers\n",
    "    duplicates = {title: papers for title, papers in title_groups.items() if len(papers) > 1}\n",
    "    \n",
    "    print(f\"\\nDuplicate Title Analysis (Active Papers Only):\")\n",
    "    print(f\"Found {len(duplicates)} groups of papers with identical normalized titles\")\n",
    "    total_dupes = sum(len(papers) for papers in duplicates.values())\n",
    "    print(f\"Total papers involved in duplicates: {total_dupes}\")\n",
    "    \n",
    "    # Print some examples\n",
    "    if duplicates:\n",
    "        print(\"\\nExample duplicate groups:\")\n",
    "        for title, group in list(duplicates.items())[:3]:  # Show first 3 groups\n",
    "            print(f\"\\nNormalized Title: {title}\")\n",
    "            for paper in group:\n",
    "                print(f\"- ID: {paper[0]}\")\n",
    "                print(f\"  Original Title: {paper[1]}\")\n",
    "                print(f\"  Categories: {paper[2]}\")\n",
    "                print(f\"  Authors: {paper[3]}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "# Inspect sample papers\n",
    "inspect_papers(conn) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Save Database to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory if it doesn't exist\n",
    "!mkdir -p \"/content/drive/MyDrive/ai-safety-papers\"\n",
    "\n",
    "# Copy database to Drive\n",
    "!cp papers.db \"/content/drive/MyDrive/ai-safety-papers/papers.db\"\n",
    "print(\"Database saved to Google Drive at: /ai-safety-papers/papers.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Database Cleanup\n",
    "Use this cell if harvesting was interrupted and you need to clean up and retry with the data in memory.\n",
    "Only run this if you still have the `initial_papers` variable in memory from a previous interrupted run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all tables in correct order (respecting foreign key constraints)\n",
    "c = conn.cursor()\n",
    "c.execute(\"DROP TABLE IF EXISTS paper_versions\")\n",
    "c.execute(\"DROP TABLE IF EXISTS paper_authors\")\n",
    "c.execute(\"DROP TABLE IF EXISTS authors\")\n",
    "c.execute(\"DROP TABLE IF EXISTS papers\")\n",
    "conn.commit()\n",
    "\n",
    "# Recreate tables\n",
    "create_database()\n",
    "\n",
    "# Resave papers from memory\n",
    "if 'initial_papers' in locals():\n",
    "    print(\"Found papers in memory, saving them...\")\n",
    "    save_papers(initial_papers, conn)\n",
    "    \n",
    "    # Print count of saved papers\n",
    "    c.execute('SELECT COUNT(*) FROM papers')\n",
    "    final_count = c.fetchone()[0]\n",
    "    print(f\"\\nPapers saved after cleanup: {final_count}\")\n",
    "else:\n",
    "    print(\"No papers found in memory. You'll need to run the harvesting cell again.\") "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
