{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Papers Visualization - Phase 1\n",
    "\n",
    "This notebook implements Phase 1 of the AI Safety Papers visualization project:\n",
    "1. Metadata Collection using arXiv OAI-PMH API\n",
    "2. Abstract Embedding Generation using ModernBERT-large\n",
    "3. Initial Clustering using UMAP and HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Data Collection Parameters\n",
    "# Set to 0 to fetch all CS papers (warning: this will take a long time)\n",
    "# Set to a positive number to fetch only that many papers\n",
    "num_papers = 100 # @param {type:\"slider\", min:0, max:10000, step:100}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "def create_database():\n",
    "    \"\"\"Create SQLite database with necessary tables\"\"\"\n",
    "    conn = sqlite3.connect('papers.db')\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # # Drop existing tables if they exist to ensure clean schema\n",
    "    # c.execute('DROP TABLE IF EXISTS paper_authors')\n",
    "    # c.execute('DROP TABLE IF EXISTS author_affiliations')\n",
    "    # c.execute('DROP TABLE IF EXISTS authors')\n",
    "    # c.execute('DROP TABLE IF EXISTS papers')\n",
    "    # c.execute('DROP TABLE IF EXISTS pipeline_progress')\n",
    "    # c.execute('DROP TABLE IF EXISTS cluster_descriptions')\n",
    "    \n",
    "    # Create papers table with all arXiv metadata fields\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS papers (\n",
    "            id TEXT PRIMARY KEY,           -- Required, maxOccurs=1\n",
    "            created TEXT,                  -- Optional, maxOccurs=1\n",
    "            updated TEXT,                  -- Optional, maxOccurs=1\n",
    "            title TEXT,                    -- Optional, maxOccurs=1\n",
    "            abstract TEXT,                 -- Optional, maxOccurs=1\n",
    "            categories TEXT,               -- Optional, maxOccurs=1\n",
    "            msc_class TEXT,                -- Optional, maxOccurs=1\n",
    "            acm_class TEXT,                -- Optional, maxOccurs=1\n",
    "            doi TEXT,                      -- Optional, maxOccurs=1\n",
    "            license TEXT,                  -- Optional, maxOccurs=1\n",
    "            -- Fields we add for our analysis\n",
    "            full_text TEXT,                -- Only for relevant papers\n",
    "            abstract_embedding BLOB,        -- For all CS papers\n",
    "            full_text_embedding BLOB,       -- Only for relevant papers\n",
    "            initial_cluster_id INTEGER,     -- From abstract clustering\n",
    "            final_cluster_id INTEGER,       -- From final clustering\n",
    "            is_relevant BOOLEAN,            -- Identified as AI Safety paper\n",
    "            relevance_score FLOAT,          -- Confidence in relevance\n",
    "            status TEXT,                    -- Track paper processing status\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create authors table matching arXiv schema\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS authors (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            keyname TEXT NOT NULL,          -- Required, maxOccurs=1\n",
    "            forenames TEXT,                 -- Optional, maxOccurs=1\n",
    "            suffix TEXT,                    -- Optional, maxOccurs=1\n",
    "            UNIQUE(keyname, forenames, suffix)  -- Avoid duplicate authors\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create author affiliations table (since it's maxOccurs=unbounded in schema)\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS author_affiliations (\n",
    "            author_id INTEGER,\n",
    "            affiliation TEXT NOT NULL,\n",
    "            PRIMARY KEY (author_id, affiliation),\n",
    "            FOREIGN KEY (author_id) REFERENCES authors(id)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create paper_authors junction table\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS paper_authors (\n",
    "            paper_id TEXT,\n",
    "            author_id INTEGER,\n",
    "            author_position INTEGER,        -- Track author order in paper\n",
    "            PRIMARY KEY (paper_id, author_id),\n",
    "            FOREIGN KEY (paper_id) REFERENCES papers(id),\n",
    "            FOREIGN KEY (author_id) REFERENCES authors(id)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create progress tracking table\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS pipeline_progress (\n",
    "            phase TEXT PRIMARY KEY,\n",
    "            current_count INTEGER,\n",
    "            total_count INTEGER,\n",
    "            status TEXT,\n",
    "            last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create cluster descriptions table\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS cluster_descriptions (\n",
    "            cluster_id INTEGER PRIMARY KEY,\n",
    "            is_initial BOOLEAN,            -- True for abstract clusters, False for final\n",
    "            description TEXT,              -- Generated by Gemini\n",
    "            representative_papers TEXT,     -- JSON array of paper IDs\n",
    "            paper_count INTEGER,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create indices for common queries after all tables are created\n",
    "    c.execute(\"CREATE INDEX IF NOT EXISTS idx_status ON papers(status)\")\n",
    "    c.execute(\"CREATE INDEX IF NOT EXISTS idx_initial_cluster_id ON papers(initial_cluster_id)\")\n",
    "    c.execute(\"CREATE INDEX IF NOT EXISTS idx_final_cluster_id ON papers(final_cluster_id)\")\n",
    "    c.execute(\"CREATE INDEX IF NOT EXISTS idx_is_relevant ON papers(is_relevant)\")\n",
    "    c.execute(\"CREATE INDEX IF NOT EXISTS idx_categories ON papers(categories)\")\n",
    "    c.execute(\"CREATE INDEX IF NOT EXISTS idx_authors_names ON authors(keyname, forenames)\")\n",
    "    \n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "# Initialize database\n",
    "conn = create_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Load Existing Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check if database exists in Drive\n",
    "db_path = \"/content/drive/MyDrive/ai-safety-papers/papers.db\"\n",
    "if os.path.exists(db_path):\n",
    "    print(f\"Found existing database at {db_path}\")\n",
    "    !cp \"{db_path}\" papers.db\n",
    "    \n",
    "    # Print existing data summary\n",
    "    conn = sqlite3.connect('papers.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT COUNT(*) FROM papers')\n",
    "    print(f\"Database contains {c.fetchone()[0]} papers\")\n",
    "else:\n",
    "    print(\"No existing database found in Drive. Will create new one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2. Metadata Harvesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from sickle import Sickle\n",
    "from sickle.models import Record\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ArxivRecord(Record):\n",
    "    \"\"\"Custom record class for arXiv metadata format\"\"\"\n",
    "    def get_metadata(self):\n",
    "        # Get the arXiv metadata namespace\n",
    "        ns = {'arxiv': 'http://arxiv.org/OAI/arXiv/'}\n",
    "        \n",
    "        # Get the arXiv element which contains all metadata\n",
    "        arxiv = self.xml.find('.//{http://arxiv.org/OAI/arXiv/}arXiv')\n",
    "        if arxiv is None:\n",
    "            raise ValueError(\"Could not find arXiv metadata element\")\n",
    "            \n",
    "        metadata = {}\n",
    "        \n",
    "        # Required field - use identifier from header\n",
    "        metadata['id'] = self.header.identifier\n",
    "        if metadata['id'].startswith('oai:arXiv.org:'):\n",
    "            metadata['id'] = metadata['id'].replace('oai:arXiv.org:', '')\n",
    "        \n",
    "        # Map arXiv metadata fields according to schema\n",
    "        field_mapping = {\n",
    "            'created': 'created',\n",
    "            'updated': 'updated',\n",
    "            'title': 'title',\n",
    "            'abstract': 'abstract',\n",
    "            'categories': 'categories',\n",
    "            'msc_class': 'msc-class',\n",
    "            'acm_class': 'acm-class',\n",
    "            'report_no': 'report-no',\n",
    "            'journal_ref': 'journal-ref',\n",
    "            'doi': 'doi',\n",
    "            'comments': 'comments',\n",
    "            'license': 'license'\n",
    "        }\n",
    "        \n",
    "        for field, xml_field in field_mapping.items():\n",
    "            elem = arxiv.find(f'arxiv:{xml_field}', namespaces=ns)\n",
    "            metadata[field] = elem.text if elem is not None and elem.text else None\n",
    "        \n",
    "        # Extract authors according to schema\n",
    "        authors = []\n",
    "        authors_elem = arxiv.find('arxiv:authors', namespaces=ns)\n",
    "        if authors_elem is not None:\n",
    "            for author_elem in authors_elem.findall('arxiv:author', namespaces=ns):\n",
    "                if author_elem is None:\n",
    "                    continue\n",
    "                    \n",
    "                author = {}\n",
    "                \n",
    "                # Required keyname field\n",
    "                keyname_elem = author_elem.find('arxiv:keyname', namespaces=ns)\n",
    "                if keyname_elem is None or not keyname_elem.text:\n",
    "                    continue  # Skip authors without keyname\n",
    "                author['keyname'] = keyname_elem.text\n",
    "                \n",
    "                # Optional author fields\n",
    "                forenames_elem = author_elem.find('arxiv:forenames', namespaces=ns)\n",
    "                author['forenames'] = forenames_elem.text if forenames_elem is not None and forenames_elem.text else None\n",
    "                \n",
    "                suffix_elem = author_elem.find('arxiv:suffix', namespaces=ns)\n",
    "                author['suffix'] = suffix_elem.text if suffix_elem is not None and suffix_elem.text else None\n",
    "                \n",
    "                # Multiple affiliations possible\n",
    "                author['affiliations'] = []\n",
    "                for affiliation_elem in author_elem.findall('arxiv:affiliation', namespaces=ns):\n",
    "                    if affiliation_elem is not None and affiliation_elem.text:\n",
    "                        author['affiliations'].append(affiliation_elem.text)\n",
    "                \n",
    "                authors.append(author)\n",
    "        \n",
    "        metadata['authors'] = authors\n",
    "        return metadata\n",
    "\n",
    "def save_papers(papers, conn):\n",
    "    \"\"\"Save papers and authors to SQLite database\"\"\"\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    for paper in papers:\n",
    "        try:\n",
    "            # Insert paper with all fields\n",
    "            c.execute('''\n",
    "                INSERT OR IGNORE INTO papers (\n",
    "                    id, created, updated, title, abstract, categories,\n",
    "                    msc_class, acm_class, doi, license, status\n",
    "                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                paper['id'], \n",
    "                paper.get('created'),\n",
    "                paper.get('updated'),\n",
    "                paper.get('title'),\n",
    "                paper.get('abstract'),\n",
    "                paper.get('categories'),\n",
    "                paper.get('msc_class'),\n",
    "                paper.get('acm_class'),\n",
    "                paper.get('doi'),\n",
    "                paper.get('license'),\n",
    "                'fetched'\n",
    "            ))\n",
    "            \n",
    "            # Process authors\n",
    "            for pos, author in enumerate(paper['authors'], 1):\n",
    "                # Insert author if not exists\n",
    "                c.execute('''\n",
    "                    INSERT OR IGNORE INTO authors (keyname, forenames, suffix)\n",
    "                    VALUES (?, ?, ?)\n",
    "                ''', (\n",
    "                    author['keyname'],\n",
    "                    author.get('forenames'),\n",
    "                    author.get('suffix')\n",
    "                ))\n",
    "                \n",
    "                # Get author ID\n",
    "                c.execute('''\n",
    "                    SELECT id FROM authors \n",
    "                    WHERE keyname = ? AND \n",
    "                          COALESCE(forenames, '') = COALESCE(?, '') AND\n",
    "                          COALESCE(suffix, '') = COALESCE(?, '')\n",
    "                ''', (\n",
    "                    author['keyname'],\n",
    "                    author.get('forenames'),\n",
    "                    author.get('suffix')\n",
    "                ))\n",
    "                author_id = c.fetchone()[0]\n",
    "                \n",
    "                # Create paper-author relationship\n",
    "                c.execute('''\n",
    "                    INSERT OR IGNORE INTO paper_authors (paper_id, author_id, author_position)\n",
    "                    VALUES (?, ?, ?)\n",
    "                ''', (paper['id'], author_id, pos))\n",
    "                \n",
    "                # Add any affiliations\n",
    "                for affiliation in author.get('affiliations', []):\n",
    "                    c.execute('''\n",
    "                        INSERT OR IGNORE INTO author_affiliations (author_id, affiliation)\n",
    "                        VALUES (?, ?)\n",
    "                    ''', (author_id, affiliation))\n",
    "                \n",
    "        except sqlite3.IntegrityError as e:\n",
    "            print(f\"Error saving paper {paper['id']}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    conn.commit()\n",
    "\n",
    "def fetch_cs_papers(start_index=0, max_results=None):\n",
    "    \"\"\"Fetch CS papers from arXiv using OAI-PMH with arXiv metadata format\"\"\"\n",
    "    # Create Sickle client with retry configuration\n",
    "    sickle = Sickle('http://export.arxiv.org/oai2',\n",
    "                    max_retries=5,\n",
    "                    retry_status_codes=[503],\n",
    "                    default_retry_after=20)\n",
    "    \n",
    "    # Register our custom record class for arXiv metadata format\n",
    "    sickle.class_mapping['ListRecords'] = ArxivRecord\n",
    "    \n",
    "    papers = []\n",
    "    try:\n",
    "        # Use ListRecords with arXiv metadata format\n",
    "        records = sickle.ListRecords(\n",
    "            metadataPrefix='arXiv',  # Use arXiv's native format\n",
    "            set='cs',\n",
    "            ignore_deleted=True\n",
    "        )\n",
    "        \n",
    "        # Process records with progress bar\n",
    "        with tqdm(desc=\"Fetching papers\", unit=\" papers\") as pbar:\n",
    "            for record in records:\n",
    "                try:\n",
    "                    metadata = record.get_metadata()\n",
    "                    if metadata and metadata.get('id'):  # Only add if we have valid metadata\n",
    "                        papers.append(metadata)\n",
    "                        pbar.update(1)\n",
    "                        \n",
    "                        # Respect max_results limit if set\n",
    "                        if max_results and len(papers) >= max_results:\n",
    "                            break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing record: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during harvesting: {str(e)}\")\n",
    "        \n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Fetch papers using the slider value (None for all papers)\n",
    "initial_papers = fetch_cs_papers(max_results=num_papers if num_papers > 0 else None)\n",
    "save_papers(initial_papers, conn)\n",
    "\n",
    "# Print summary\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT COUNT(*) FROM papers')\n",
    "print(f\"Total papers in database: {c.fetchone()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_papers(conn, limit=5):\n",
    "    \"\"\"Print detailed information for a sample of papers\"\"\"\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Get sample papers with their authors\n",
    "    c.execute('''\n",
    "        SELECT p.id, p.title, p.abstract, p.categories, p.created, p.updated,\n",
    "               GROUP_CONCAT(a.keyname || COALESCE(', ' || a.forenames, '') || COALESCE(' ' || a.suffix, ''))\n",
    "        FROM papers p\n",
    "        LEFT JOIN paper_authors pa ON p.id = pa.paper_id\n",
    "        LEFT JOIN authors a ON pa.author_id = a.id\n",
    "        GROUP BY p.id\n",
    "        LIMIT ?\n",
    "    ''', (limit,))\n",
    "    \n",
    "    papers = c.fetchall()\n",
    "    \n",
    "    print(f\"Inspecting {len(papers)} sample papers:\\n\")\n",
    "    for paper in papers:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ID: {paper[0]}\")\n",
    "        print(f\"Title: {paper[1]}\")\n",
    "        print(f\"Abstract: {paper[2][:200]}...\" if paper[2] else \"Abstract: None\")\n",
    "        print(f\"Categories: {paper[3]}\")\n",
    "        print(f\"Created: {paper[4]}\")\n",
    "        print(f\"Updated: {paper[5]}\")\n",
    "        print(f\"Authors: {paper[6]}\")\n",
    "        print()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\nDatabase Statistics:\")\n",
    "    \n",
    "    # Paper statistics\n",
    "    c.execute('SELECT COUNT(*) FROM papers')\n",
    "    total_papers = c.fetchone()[0]\n",
    "    print(f\"\\nPapers:\")\n",
    "    print(f\"Total Papers: {total_papers}\")\n",
    "    \n",
    "    # Author statistics\n",
    "    c.execute('SELECT COUNT(*) FROM authors')\n",
    "    total_authors = c.fetchone()[0]\n",
    "    \n",
    "    c.execute('SELECT COUNT(*) FROM paper_authors')\n",
    "    total_author_links = c.fetchone()[0]\n",
    "    \n",
    "    c.execute('''\n",
    "        SELECT COUNT(*) FROM \n",
    "        (SELECT author_id FROM paper_authors GROUP BY author_id HAVING COUNT(*) > 1)\n",
    "    ''')\n",
    "    authors_multiple_papers = c.fetchone()[0]\n",
    "    \n",
    "    c.execute('SELECT AVG(author_count) FROM (SELECT paper_id, COUNT(*) as author_count FROM paper_authors GROUP BY paper_id)')\n",
    "    avg_authors_per_paper = c.fetchone()[0]\n",
    "    \n",
    "    print(f\"\\nAuthors:\")\n",
    "    print(f\"Total Authors: {total_authors}\")\n",
    "    print(f\"Total Author-Paper Links: {total_author_links}\")\n",
    "    print(f\"Authors with Multiple Papers: {authors_multiple_papers}\")\n",
    "    print(f\"Average Authors per Paper: {avg_authors_per_paper:.2f}\")\n",
    "    \n",
    "    # Metadata field statistics\n",
    "    print(\"\\nMetadata Field Coverage:\")\n",
    "    fields = [\n",
    "        'title', 'abstract', 'categories', 'created', 'updated',\n",
    "        'msc_class', 'acm_class', 'doi', 'license'\n",
    "    ]\n",
    "    \n",
    "    for field in fields:\n",
    "        # Count non-null values\n",
    "        c.execute(f'SELECT COUNT(*) FROM papers WHERE {field} IS NOT NULL')\n",
    "        present = c.fetchone()[0]\n",
    "        percentage = (present / total_papers) * 100 if total_papers > 0 else 0\n",
    "        print(f\"{field}: {present}/{total_papers} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Category statistics\n",
    "    print(\"\\nTop Categories:\")\n",
    "    c.execute('''\n",
    "        SELECT categories, COUNT(*) as count \n",
    "        FROM papers \n",
    "        WHERE categories IS NOT NULL \n",
    "        GROUP BY categories \n",
    "        ORDER BY count DESC \n",
    "        LIMIT 5\n",
    "    ''')\n",
    "    for category, count in c.fetchall():\n",
    "        percentage = (count / total_papers) * 100\n",
    "        print(f\"{category}: {count} papers ({percentage:.1f}%)\")\n",
    "\n",
    "# Inspect sample papers\n",
    "inspect_papers(conn) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Save Database to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory if it doesn't exist\n",
    "!mkdir -p \"/content/drive/MyDrive/ai-safety-papers\"\n",
    "\n",
    "# Copy database to Drive\n",
    "!cp papers.db \"/content/drive/MyDrive/ai-safety-papers/papers.db\"\n",
    "print(\"Database saved to Google Drive at: /ai-safety-papers/papers.db\") "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
