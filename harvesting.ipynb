{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# arXiv Computer Science Papers Database\n",
    "\n",
    "This notebook creates and maintains a SQLite database of Computer Science papers from arXiv. It:\n",
    "1. Harvests metadata using arXiv's OAI-PMH API\n",
    "2. Stores paper metadata, versions, and author information\n",
    "3. Provides data quality analysis and management tools\n",
    "\n",
    "The database schema includes:\n",
    "- Papers: Core metadata (title, abstract, categories, etc.)\n",
    "- Paper Versions: Version history and submission dates\n",
    "- Authors: Normalized author information\n",
    "- Paper-Author relationships: Author order in papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# 1. Database Setup and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1.1 Environment Setup\n",
    "Run this cell to set up the environment if using Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Install and configure PostgreSQL\n",
    "    !sudo apt-get -qq update && sudo apt-get -qq install postgresql postgresql-contrib # pyright: ignore\n",
    "    !sudo service postgresql start # pyright: ignore\n",
    "    !sudo sed -i 's/local\\s*all\\s*postgres\\s*peer/local all postgres trust/' /etc/postgresql/14/main/pg_hba.conf # pyright: ignore\n",
    "    !sudo service postgresql restart # pyright: ignore\n",
    "    \n",
    "    # Install Python client\n",
    "    %pip install psycopg2-binary # pyright: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1.2 Database Initialization\n",
    "Choose whether to load an existing database from Google Drive or create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Database Initialization Choice\n",
    "db_choice = \"create_new\" # @param [\"create_new\", \"load_existing\"] {type:\"string\", label:\"Database Choice\"}\n",
    "\n",
    "import sqlite3\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "def load_existing_database():\n",
    "    \"\"\"Load existing PostgreSQL database from backup\"\"\"\n",
    "    print(\"Loading PostgreSQL backup...\")\n",
    "    backup_path = \"/content/drive/MyDrive/ai-safety-papers/papers.sql\"\n",
    "    !createdb -U postgres papers # pyright: ignore\n",
    "    !pg_restore -U postgres --jobs=8 -d papers \"{backup_path}\" # pyright: ignore\n",
    "    return psycopg2.connect(\n",
    "        host='',\n",
    "        database=\"papers\",\n",
    "        user=\"postgres\"\n",
    "    )\n",
    "\n",
    "def create_new_database():\n",
    "    \"\"\"Create a new empty database with schema\"\"\"\n",
    "    if os.path.exists('papers.sql'):\n",
    "        print(\"Warning: Overwriting existing local database\")\n",
    "    \n",
    "    print(\"Creating new database...\")\n",
    "    conn = create_database()\n",
    "    print(\"Database created successfully\")\n",
    "    return conn\n",
    "\n",
    "# Initialize database based on user choice\n",
    "if db_choice == \"create_new\":\n",
    "    print(\"Creating new database...\")\n",
    "    conn = create_new_database()\n",
    "else:\n",
    "    print(\"Loading existing database...\")\n",
    "    conn = load_existing_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 1.2.1 Database Schema\n",
    "This shows the schema used for both new and existing databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "def create_database():\n",
    "    \"\"\"Create PostgreSQL database matching migration script\"\"\"\n",
    "    # Create/maintain database connection\n",
    "    conn = psycopg2.connect(\n",
    "        host='',\n",
    "        database=\"papers\",\n",
    "        user=\"postgres\"\n",
    "    )\n",
    "    conn.autocommit = True  # Needed for database creation\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            # Create database if not exists\n",
    "            cursor.execute(\"CREATE DATABASE papers\")\n",
    "    except psycopg2.errors.DuplicateDatabase:\n",
    "        pass\n",
    "    \n",
    "    # Connect to the new database\n",
    "    conn = psycopg2.connect(\n",
    "        host='',\n",
    "        database=\"papers\",\n",
    "        user=\"postgres\"\n",
    "    )\n",
    "    \n",
    "    with conn.cursor() as cursor:\n",
    "        # Create tables with PostgreSQL data types\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS papers (\n",
    "                id TEXT PRIMARY KEY,\n",
    "                title TEXT,\n",
    "                abstract TEXT,\n",
    "                categories TEXT,\n",
    "                msc_class TEXT,\n",
    "                acm_class TEXT,\n",
    "                doi TEXT,\n",
    "                license TEXT,\n",
    "                comments TEXT,\n",
    "                created TIMESTAMP,\n",
    "                updated TIMESTAMP,\n",
    "                withdrawn BOOLEAN DEFAULT FALSE,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                abstract_embedding BYTEA\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS paper_versions (\n",
    "                paper_id TEXT,\n",
    "                version INTEGER,\n",
    "                source_type TEXT,\n",
    "                size TEXT,\n",
    "                date TIMESTAMP,\n",
    "                PRIMARY KEY (paper_id, version),\n",
    "                FOREIGN KEY (paper_id) REFERENCES papers(id)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS authors (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                keyname TEXT NOT NULL,\n",
    "                forenames TEXT,\n",
    "                suffix TEXT,\n",
    "                CONSTRAINT unique_author UNIQUE (keyname, forenames, suffix)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS paper_authors (\n",
    "                paper_id TEXT,\n",
    "                author_id INTEGER,\n",
    "                author_position INTEGER,\n",
    "                PRIMARY KEY (paper_id, author_id),\n",
    "                FOREIGN KEY (paper_id) REFERENCES papers(id),\n",
    "                FOREIGN KEY (author_id) REFERENCES authors(id)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Create indexes\n",
    "        cursor.execute('''\n",
    "            CREATE INDEX IF NOT EXISTS idx_categories \n",
    "            ON papers(categories)\n",
    "        ''')\n",
    "        cursor.execute('''\n",
    "            CREATE INDEX IF NOT EXISTS idx_withdrawn \n",
    "            ON papers(withdrawn)\n",
    "        ''')\n",
    "        cursor.execute('''\n",
    "            CREATE INDEX IF NOT EXISTS idx_created \n",
    "            ON papers(created)\n",
    "        ''')\n",
    "        cursor.execute('''\n",
    "            CREATE INDEX IF NOT EXISTS idx_updated \n",
    "            ON papers(updated)\n",
    "        ''')\n",
    "        \n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "# Initialize database\n",
    "conn = create_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# 2. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 2.1 Harvesting Configuration\n",
    "Configure the number of papers to fetch and provide a resumption token if continuing an interrupted harvest.\n",
    "\n",
    "### Number of Papers\n",
    "- Set to 0 to fetch all CS papers (warning: this will take several hours)\n",
    "- Set to a specific number (e.g., 100) to test the harvesting process\n",
    "\n",
    "### Resumption Token\n",
    "If harvesting was interrupted (e.g., due to timeout or error), you can continue from where you left off:\n",
    "1. Copy the resumption token from the error message or last output\n",
    "2. Paste it below to resume harvesting from that point\n",
    "3. You can paste either:\n",
    "   - The full URL (e.g., `http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=foo%7Cbar`)\n",
    "   - Just the token part (e.g., `foo|bar`)\n",
    "   - URL-encoded characters (like %7C) will be automatically converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Harvesting Configuration  {\"run\":\"auto\"}\n",
    "num_papers = 0 # @param {type:\"slider\", min:0, max:10000, step:100}\n",
    "resumption_token = \"\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from sickle import Sickle\n",
    "from sickle.models import Record\n",
    "from tqdm import tqdm\n",
    "import urllib.parse\n",
    "import re\n",
    "\n",
    "def get_safe_batch_size(conn):\n",
    "    \"\"\"PostgreSQL can handle large batches\"\"\"\n",
    "    return 1000  # Conservative estimate\n",
    "\n",
    "class ArxivRecord(Record):\n",
    "    \"\"\"Custom record class for arXiv metadata format\"\"\"\n",
    "    def get_metadata(self):\n",
    "        # Get the arXiv metadata namespace\n",
    "        ns = {'arxiv': 'http://arxiv.org/OAI/arXiv/'}\n",
    "        \n",
    "        # Get the arXiv element which contains all metadata\n",
    "        arxiv = self.xml.find('.//{http://arxiv.org/OAI/arXiv/}arXiv')\n",
    "        if arxiv is None:\n",
    "            raise ValueError(\"Could not find arXiv metadata element\")\n",
    "            \n",
    "        metadata = {}\n",
    "        \n",
    "        # Required field - use identifier from header\n",
    "        metadata['id'] = self.header.identifier\n",
    "        if metadata['id'].startswith('oai:arXiv.org:'):\n",
    "            metadata['id'] = metadata['id'].replace('oai:arXiv.org:', '')\n",
    "        \n",
    "        # Map arXiv metadata fields according to schema\n",
    "        field_mapping = {\n",
    "            'created': 'created',\n",
    "            'updated': 'updated',\n",
    "            'title': 'title',\n",
    "            'abstract': 'abstract',\n",
    "            'categories': 'categories',\n",
    "            'msc_class': 'msc-class',\n",
    "            'acm_class': 'acm-class',\n",
    "            'report_no': 'report-no',\n",
    "            'journal_ref': 'journal-ref',\n",
    "            'doi': 'doi',\n",
    "            'comments': 'comments',\n",
    "            'license': 'license'\n",
    "        }\n",
    "        \n",
    "        for field, xml_field in field_mapping.items():\n",
    "            elem = arxiv.find(f'arxiv:{xml_field}', namespaces=ns)\n",
    "            metadata[field] = elem.text if elem is not None and elem.text else None\n",
    "        \n",
    "        # Extract authors according to schema\n",
    "        authors = []\n",
    "        authors_elem = arxiv.find('arxiv:authors', namespaces=ns)\n",
    "        if authors_elem is not None:\n",
    "            for author_elem in authors_elem.findall('arxiv:author', namespaces=ns):\n",
    "                if author_elem is None:\n",
    "                    continue\n",
    "                    \n",
    "                author = {}\n",
    "                \n",
    "                # Required keyname field\n",
    "                keyname_elem = author_elem.find('arxiv:keyname', namespaces=ns)\n",
    "                if keyname_elem is None or not keyname_elem.text:\n",
    "                    continue  # Skip authors without keyname\n",
    "                author['keyname'] = keyname_elem.text.rstrip(',')  # Remove trailing comma if present\n",
    "                \n",
    "                # Optional author fields\n",
    "                forenames_elem = author_elem.find('arxiv:forenames', namespaces=ns)\n",
    "                author['forenames'] = forenames_elem.text if forenames_elem is not None and forenames_elem.text else None\n",
    "                \n",
    "                suffix_elem = author_elem.find('arxiv:suffix', namespaces=ns)\n",
    "                author['suffix'] = suffix_elem.text if suffix_elem is not None and suffix_elem.text else None\n",
    "                \n",
    "                authors.append(author)\n",
    "        \n",
    "        metadata['authors'] = authors\n",
    "        return metadata\n",
    "\n",
    "class ArxivRawRecord(Record):\n",
    "    \"\"\"Custom record class for arXivRaw metadata format\"\"\"\n",
    "    def get_metadata(self):\n",
    "        # Get the arXivRaw metadata namespace\n",
    "        ns = {'arxiv': 'http://arxiv.org/OAI/arXivRaw/'}\n",
    "        \n",
    "        # Get the arXivRaw element which contains all metadata\n",
    "        arxiv = self.xml.find('.//{http://arxiv.org/OAI/arXivRaw/}arXivRaw')\n",
    "        if arxiv is None:\n",
    "            raise ValueError(\"Could not find arXivRaw metadata element\")\n",
    "            \n",
    "        metadata = {}\n",
    "        \n",
    "        # Required field - use identifier from header\n",
    "        metadata['id'] = self.header.identifier\n",
    "        if metadata['id'].startswith('oai:arXiv.org:'):\n",
    "            metadata['id'] = metadata['id'].replace('oai:arXiv.org:', '')\n",
    "        \n",
    "        # Get all versions and sort by version number\n",
    "        versions = []\n",
    "        for version_elem in arxiv.findall('.//arxiv:version', namespaces=ns):\n",
    "            version_num = version_elem.get('version', 'v1').lstrip('v')\n",
    "            try:\n",
    "                version_num = int(version_num)\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            version_info = {\n",
    "                'version': version_num,\n",
    "                'date': version_elem.find('arxiv:date', namespaces=ns).text if version_elem.find('arxiv:date', namespaces=ns) is not None else None,\n",
    "                'size': version_elem.find('arxiv:size', namespaces=ns).text if version_elem.find('arxiv:size', namespaces=ns) is not None else None,\n",
    "                'source_type': version_elem.find('arxiv:source_type', namespaces=ns).text if version_elem.find('arxiv:source_type', namespaces=ns) is not None else 'D'\n",
    "            }\n",
    "            versions.append(version_info)\n",
    "        \n",
    "        # Sort versions by version number\n",
    "        versions.sort(key=lambda x: x['version'])\n",
    "        metadata['versions'] = versions\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "def save_papers(papers, conn):\n",
    "    \"\"\"Save papers and authors to SQLite database\"\"\"\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    def normalize_suffix(suffix):\n",
    "        \"\"\"Normalize author suffixes to a standard format\"\"\"\n",
    "        if not suffix:\n",
    "            return None\n",
    "        suffix = suffix.strip()\n",
    "        # Normalize Jr variations\n",
    "        if suffix.upper() in ['JR', 'JR.', 'JR ', 'JUNIOR']:\n",
    "            return 'Jr.'\n",
    "        # Normalize Sr variations\n",
    "        if suffix.upper() in ['SR', 'SR.', 'SR ', 'SENIOR']:\n",
    "            return 'Sr.'\n",
    "        # Normalize roman numerals\n",
    "        if suffix.upper() in ['I', 'II', 'III', 'IV', 'V']:\n",
    "            return suffix.upper()\n",
    "        return suffix\n",
    "    \n",
    "    print(f\"\\nSaving {len(papers)} papers and their authors...\")\n",
    "    with tqdm(total=len(papers), desc=\"Saving papers\", unit=\" papers\",\n",
    "              miniters=500,\n",
    "              smoothing=0.8\n",
    "              ) as pbar:\n",
    "        for paper in papers:\n",
    "            try:\n",
    "                # Insert paper with all fields\n",
    "                c.execute('''\n",
    "                    INSERT INTO papers (\n",
    "                        id, title, abstract, categories,\n",
    "                        msc_class, acm_class, doi, license, comments\n",
    "                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                    ON CONFLICT (id) DO NOTHING\n",
    "                ''', (\n",
    "                    paper['id'], \n",
    "                    paper.get('title'),\n",
    "                    paper.get('abstract'),\n",
    "                    paper.get('categories'),\n",
    "                    paper.get('msc_class'),\n",
    "                    paper.get('acm_class'),\n",
    "                    paper.get('doi'),\n",
    "                    paper.get('license'),\n",
    "                    paper.get('comments')\n",
    "                ))\n",
    "                \n",
    "                # Process authors\n",
    "                for pos, author in enumerate(paper['authors'], 1):\n",
    "                    # Clean and normalize author fields\n",
    "                    keyname = author['keyname'].strip()\n",
    "                    forenames = author.get('forenames', '').strip() if author.get('forenames') else ''\n",
    "                    suffix = normalize_suffix(author.get('suffix'))\n",
    "                    \n",
    "                    # First try to find existing author with normalized fields\n",
    "                    c.execute('''\n",
    "                        SELECT id FROM authors \n",
    "                        WHERE keyname = %s AND \n",
    "                              COALESCE(forenames, '') = %s AND\n",
    "                              COALESCE(suffix, '') = COALESCE(%s, '')\n",
    "                    ''', (\n",
    "                        keyname,\n",
    "                        forenames,\n",
    "                        suffix\n",
    "                    ))\n",
    "                    result = c.fetchone()\n",
    "                    \n",
    "                    if result:\n",
    "                        author_id = result[0]\n",
    "                    else:\n",
    "                        # Insert new author with normalized fields\n",
    "                        c.execute('''\n",
    "                            INSERT INTO authors (keyname, forenames, suffix)\n",
    "                            VALUES (%s, %s, %s)\n",
    "                        ''', (\n",
    "                            keyname,\n",
    "                            forenames if forenames else None,  # Store NULL if empty\n",
    "                            suffix  # Already normalized\n",
    "                        ))\n",
    "                        author_id = c.lastrowid\n",
    "                    \n",
    "                    # Create paper-author relationship\n",
    "                    c.execute('''\n",
    "                        INSERT INTO paper_authors (paper_id, author_id, author_position)\n",
    "                        VALUES (%s, %s, %s)\n",
    "                    ''', (paper['id'], author_id, pos))\n",
    "                \n",
    "                pbar.update(1)\n",
    "                    \n",
    "            except sqlite3.IntegrityError as e:\n",
    "                print(f\"Error saving paper {paper['id']}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    conn.commit()\n",
    "\n",
    "def save_versions(paper_versions, conn):\n",
    "    \"\"\"Save paper version information to database efficiently\"\"\"\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Calculate batch sizes\n",
    "\n",
    "    batch_size = 1000\n",
    "\n",
    "    \n",
    "    # Prepare batch data\n",
    "    paper_updates = []\n",
    "    version_inserts = []\n",
    "    \n",
    "    # Group versions by paper ID\n",
    "    paper_version_map = {}\n",
    "    for paper in paper_versions:\n",
    "        paper_id = paper['id']\n",
    "        if 'versions' in paper:\n",
    "            # Only keep versions for papers that exist\n",
    "            c.execute('SELECT 1 FROM papers WHERE id = %s', (paper_id,))\n",
    "            if c.fetchone():\n",
    "                paper_version_map[paper_id] = paper['versions']\n",
    "            else:\n",
    "                print(f\"Warning: Skipping versions for non-existent paper {paper_id}\")\n",
    "    \n",
    "    # Collect all data first\n",
    "    for paper_id, versions in paper_version_map.items():\n",
    "        # Sort versions by version number\n",
    "        versions = sorted(versions, key=lambda x: x['version'])\n",
    "        \n",
    "        if versions:\n",
    "            # Get first and latest version dates\n",
    "            created = versions[0]['date']\n",
    "            updated = versions[-1]['date']\n",
    "            \n",
    "            # Check if paper is withdrawn based on latest version\n",
    "            latest_version = versions[-1]\n",
    "            withdrawn = (\n",
    "                latest_version['source_type'] == 'I' or\n",
    "                latest_version['size'] == '0kb'\n",
    "            )\n",
    "            \n",
    "            # Add to paper updates\n",
    "            paper_updates.append((created, updated, withdrawn, paper_id))\n",
    "            \n",
    "            # Add all versions\n",
    "            for version in versions:\n",
    "                version_inserts.append((\n",
    "                    paper_id,\n",
    "                    version['version'],\n",
    "                    version['source_type'],\n",
    "                    version['size'],\n",
    "                    version['date']\n",
    "                ))\n",
    "    \n",
    "    try:\n",
    "        # Batch update papers\n",
    "        for i in range(0, len(paper_updates), batch_size):\n",
    "            batch = paper_updates[i:i + batch_size]\n",
    "            c.executemany('''\n",
    "                UPDATE papers \n",
    "                SET created = %s, updated = %s, withdrawn = %s\n",
    "                WHERE id = %s\n",
    "            ''', batch)\n",
    "        \n",
    "        # Batch insert versions\n",
    "        for i in range(0, len(version_inserts), batch_size):\n",
    "            batch = version_inserts[i:i + batch_size]\n",
    "            c.executemany('''\n",
    "                INSERT INTO paper_versions (\n",
    "                    paper_id, version, source_type, size, date\n",
    "                ) VALUES (%s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (paper_id, version) DO NOTHING\n",
    "            ''', batch)\n",
    "        \n",
    "        conn.commit()\n",
    "        \n",
    "    except sqlite3.IntegrityError as e:\n",
    "        print(f\"Error during batch operations: {str(e)}\")\n",
    "        conn.rollback()\n",
    "\n",
    "def fetch_arxiv_records(metadata_prefix, record_class, max_results=None, resumption_token=None):\n",
    "    \"\"\"Base function for fetching records from arXiv OAI-PMH API\"\"\"\n",
    "    sickle = Sickle('http://export.arxiv.org/oai2',\n",
    "                    max_retries=5,\n",
    "                    retry_status_codes=[503],\n",
    "                    default_retry_after=20)\n",
    "    \n",
    "    # Register our custom record class\n",
    "    sickle.class_mapping['ListRecords'] = record_class\n",
    "    \n",
    "    try:\n",
    "        # Use ListRecords with specified format\n",
    "        if resumption_token:\n",
    "            # Extract token from URL if full URL was pasted\n",
    "            if 'resumptionToken=' in resumption_token:\n",
    "                resumption_token = re.search(r'resumptionToken=([^&]+)', resumption_token).group(1)\n",
    "            # Unescape the token\n",
    "            resumption_token = urllib.parse.unquote(resumption_token)\n",
    "            records = sickle.ListRecords(resumptionToken=resumption_token)\n",
    "        else:\n",
    "            records = sickle.ListRecords(\n",
    "                metadataPrefix=metadata_prefix,\n",
    "                set='cs',\n",
    "                ignore_deleted=True\n",
    "            )\n",
    "        \n",
    "        # Process records with progress bar\n",
    "        results = []\n",
    "        with tqdm(desc=f\"Fetching {metadata_prefix}\", unit=\" papers\",\n",
    "                 miniters=25,\n",
    "                 smoothing=0.8\n",
    "                 ) as pbar:\n",
    "            while True:\n",
    "                try:\n",
    "                    record = next(records)\n",
    "                    try:\n",
    "                        metadata = record.get_metadata()\n",
    "                        if metadata and metadata.get('id'):  # Only add if we have valid metadata\n",
    "                            results.append(metadata)\n",
    "                            pbar.update(1)\n",
    "                            \n",
    "                            # Respect max_results limit if set\n",
    "                            if max_results and len(results) >= max_results:\n",
    "                                break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing record: {str(e)}\")\n",
    "                        continue\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    # Get the current resumption token from the Sickle iterator\n",
    "                    if hasattr(records, 'resumption_token'):\n",
    "                        print(f\"\\nError occurred. Current resumption token: {records.resumption_token}\")\n",
    "                    raise  # Re-raise the exception to let Sickle's retry mechanism handle it\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during harvesting: {str(e)}\")\n",
    "        # Print final resumption token if available\n",
    "        if hasattr(records, 'resumption_token'):\n",
    "            print(f\"Final resumption token: {records.resumption_token}\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "def fetch_cs_papers(max_results=None, resumption_token=None):\n",
    "    \"\"\"Fetch CS papers from arXiv using OAI-PMH with arXiv metadata format\"\"\"\n",
    "    return fetch_arxiv_records('arXiv', ArxivRecord, max_results, resumption_token)\n",
    "\n",
    "def fetch_raw_metadata(max_results=None, resumption_token=None):\n",
    "    \"\"\"Fetch paper version information from arXiv using OAI-PMH with arXivRaw metadata format\"\"\"\n",
    "    return fetch_arxiv_records('arXivRaw', ArxivRawRecord, max_results, resumption_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 2.2 Core Metadata Collection\n",
    "Fetches primary metadata (titles, abstracts, authors) using arXiv's OAI-PMH API.\n",
    "You can run this section independently and save before proceeding to version metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch papers using the slider value (None for all papers)\n",
    "# First check current database count\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT COUNT(*) FROM papers')\n",
    "current_count = c.fetchone()[0]\n",
    "print(f\"Current papers in database before harvesting: {current_count}\")\n",
    "\n",
    "# Fetch papers with arXiv metadata\n",
    "initial_papers = fetch_cs_papers(\n",
    "    max_results=num_papers if num_papers > 0 else None,\n",
    "    resumption_token=resumption_token if resumption_token else None\n",
    ")\n",
    "save_papers(initial_papers, conn)\n",
    "\n",
    "# Print final count\n",
    "c.execute('SELECT COUNT(*) FROM papers')\n",
    "final_count = c.fetchone()[0]\n",
    "print(f\"\\nMetadata harvesting complete:\")\n",
    "print(f\"- Papers added this session: {final_count - current_count}\")\n",
    "print(f\"- Total papers in database: {final_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 2.3 Version Metadata Collection\n",
    "Fetches additional metadata about paper versions using arXiv's OAI-PMH API with arXivRaw format.\n",
    "**Note**: Run this after collecting core metadata, and ensure database is loaded if in a new session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Fetch version information with arXivRaw\n",
    "paper_versions = fetch_raw_metadata(\n",
    "    max_results=num_papers if num_papers > 0 else None,\n",
    "    resumption_token=resumption_token if resumption_token else None\n",
    ")\n",
    "save_versions(paper_versions, conn)\n",
    "\n",
    "# Print final stats about versions\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT COUNT(*) FROM paper_versions')\n",
    "version_count = c.fetchone()[0]\n",
    "c.execute('SELECT COUNT(DISTINCT paper_id) FROM paper_versions')\n",
    "papers_with_versions = c.fetchone()[0]\n",
    "print(f\"\\nRaw metadata harvesting complete:\")\n",
    "print(f\"- Total versions stored: {version_count}\")\n",
    "print(f\"- Papers with version info: {papers_with_versions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# 3. Data Quality and Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 3.1 Quality Check\n",
    "Performs comprehensive analysis of the collected data, including:\n",
    "- Sample paper inspection\n",
    "- Coverage statistics\n",
    "- Author statistics\n",
    "- Category distribution\n",
    "- Duplicate detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def inspect_papers(conn, limit=5):\n",
    "    \"\"\"Print detailed information for a sample of papers\"\"\"\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Get sample papers with their authors\n",
    "    c.execute('''\n",
    "        SELECT p.id, p.title, p.abstract, p.categories, p.created, p.updated,\n",
    "               STRING_AGG(\n",
    "                   a.keyname || \n",
    "                   COALESCE(', ' || a.forenames, '') || \n",
    "                   COALESCE(' ' || a.suffix, ''),\n",
    "                   ', '  # This is the separator BETWEEN authors\n",
    "               ) as authors\n",
    "        FROM papers p\n",
    "        LEFT JOIN paper_authors pa ON p.id = pa.paper_id\n",
    "        LEFT JOIN authors a ON pa.author_id = a.id\n",
    "        GROUP BY p.id\n",
    "        LIMIT %s\n",
    "    ''', (limit,))\n",
    "    \n",
    "    papers = c.fetchall()\n",
    "    \n",
    "    print(f\"Inspecting {len(papers)} sample papers:\\n\")\n",
    "    for paper in papers:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ID: {paper[0]}\")\n",
    "        print(f\"Title: {paper[1]}\")\n",
    "        print(f\"Abstract: {paper[2][:200]}...\" if paper[2] else \"Abstract: None\")\n",
    "        print(f\"Categories: {paper[3]}\")\n",
    "        print(f\"Created: {paper[4]}\")\n",
    "        print(f\"Updated: {paper[5]}\")\n",
    "        print(f\"Authors: {paper[6]}\")\n",
    "        print()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\nDatabase Statistics:\")\n",
    "    \n",
    "    # Paper statistics\n",
    "    c.execute('SELECT COUNT(*) FROM papers')\n",
    "    total_papers = c.fetchone()[0]\n",
    "    print(f\"\\nPapers:\")\n",
    "    print(f\"Total Papers: {total_papers}\")\n",
    "    \n",
    "    # Withdrawn paper statistics\n",
    "    c.execute('SELECT COUNT(*) FROM papers WHERE withdrawn = 1')\n",
    "    withdrawn_count = c.fetchone()[0]\n",
    "    c.execute('SELECT COUNT(*) FROM papers WHERE withdrawn = 0')\n",
    "    active_count = c.fetchone()[0]\n",
    "    print(f\"Active Papers: {active_count} ({(active_count/total_papers*100):.1f}% of total)\")\n",
    "    print(f\"Withdrawn Papers: {withdrawn_count} ({(withdrawn_count/total_papers*100):.1f}% of total)\")\n",
    "    \n",
    "    # CS Primary papers\n",
    "    c.execute('''\n",
    "        WITH split_categories AS (\n",
    "            SELECT id, withdrawn,\n",
    "                   TRIM(SUBSTR(categories, 1, INSTR(categories || ' ', ' ') - 1)) as primary_category\n",
    "            FROM papers\n",
    "            WHERE categories IS NOT NULL\n",
    "        )\n",
    "        SELECT \n",
    "            COUNT(*) as total,\n",
    "            SUM(CASE WHEN withdrawn = 0 THEN 1 ELSE 0 END) as active\n",
    "        FROM split_categories \n",
    "        WHERE primary_category LIKE 'cs.%'\n",
    "    ''')\n",
    "    cs_stats = c.fetchone()\n",
    "    cs_total, cs_active = cs_stats\n",
    "    print(f\"\\nCS primary papers: {cs_total} ({(cs_total/total_papers*100):.1f}% of total)\")\n",
    "    print(f\"Active CS primary papers: {cs_active} ({(cs_active/cs_total*100):.1f}% of CS papers)\")\n",
    "    \n",
    "    # Category statistics\n",
    "    print(\"\\nTop Categories:\")\n",
    "    c.execute('''\n",
    "        SELECT category, COUNT(*) as count\n",
    "        FROM (\n",
    "            SELECT UNNEST(string_to_array(categories, ' ')) as category\n",
    "            FROM papers\n",
    "            WHERE categories IS NOT NULL\n",
    "        ) \n",
    "        WHERE category != ''\n",
    "        GROUP BY category\n",
    "        ORDER BY count DESC\n",
    "        LIMIT 10\n",
    "    ''')\n",
    "    for category, count in c.fetchall():\n",
    "        if category:  # Skip empty category\n",
    "            percentage = (count / total_papers) * 100\n",
    "            print(f\"{category}: {count} papers ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Author statistics\n",
    "    print(f\"\\nAuthors:\")\n",
    "    c.execute('SELECT COUNT(*) FROM authors')\n",
    "    total_authors = c.fetchone()[0]\n",
    "    print(f\"Total Authors: {total_authors}\")\n",
    "    \n",
    "    c.execute('SELECT COUNT(*) FROM paper_authors')\n",
    "    total_author_links = c.fetchone()[0]\n",
    "    print(f\"Total Author-Paper Links: {total_author_links}\")\n",
    "    \n",
    "    # Papers per author distribution\n",
    "    print(\"\\nPapers per author distribution:\")\n",
    "    c.execute('''\n",
    "        SELECT papers_count, COUNT(*) as authors_with_this_many_papers\n",
    "        FROM (\n",
    "            SELECT author_id, COUNT(*) as papers_count\n",
    "            FROM paper_authors\n",
    "            GROUP BY author_id\n",
    "        )\n",
    "        GROUP BY papers_count\n",
    "        ORDER BY papers_count\n",
    "        LIMIT 10\n",
    "    ''')\n",
    "    for paper_count, author_count in c.fetchall():\n",
    "        print(f\"{author_count} authors have {paper_count} paper(s)\")\n",
    "    \n",
    "    c.execute('''\n",
    "        SELECT COUNT(*) FROM \n",
    "        (SELECT author_id FROM paper_authors GROUP BY author_id HAVING COUNT(*) > 1)\n",
    "    ''')\n",
    "    authors_multiple_papers = c.fetchone()[0]\n",
    "    print(f\"\\nAuthors with Multiple Papers: {authors_multiple_papers}\")\n",
    "    \n",
    "    c.execute('SELECT AVG(author_count) FROM (SELECT paper_id, COUNT(*) as author_count FROM paper_authors GROUP BY paper_id)')\n",
    "    avg_authors_per_paper = c.fetchone()[0]\n",
    "    print(f\"\\nAverage Authors per Paper: {avg_authors_per_paper:.2f}\")\n",
    "    \n",
    "    # Metadata field statistics\n",
    "    print(\"\\nMetadata Field Coverage:\")\n",
    "    fields = [\n",
    "        'title', 'abstract', 'categories', 'created', 'updated',\n",
    "        'msc_class', 'acm_class', 'doi', 'license'\n",
    "    ]\n",
    "    \n",
    "    for field in fields:\n",
    "        c.execute(f'SELECT COUNT(*) FROM papers WHERE {field} IS NOT NULL')\n",
    "        present = c.fetchone()[0]\n",
    "        percentage = (present / total_papers) * 100 if total_papers > 0 else 0\n",
    "        print(f\"{field}: {present}/{total_papers} ({percentage:.1f}%)\")\n",
    "\n",
    "    # Duplicate title analysis\n",
    "    def normalize_title(title):\n",
    "        \"\"\"Normalize title for comparison\"\"\"\n",
    "        if not title:\n",
    "            return \"\"\n",
    "        # Convert to lowercase\n",
    "        title = title.lower()\n",
    "        # Remove version numbers\n",
    "        title = re.sub(r'\\bv\\d+\\b', '', title)\n",
    "        # Remove arXiv identifiers\n",
    "        title = re.sub(r'arxiv:\\d+\\.\\d+', '', title, flags=re.IGNORECASE)\n",
    "        # Remove punctuation and normalize whitespace\n",
    "        title = re.sub(r'[^\\w\\s]', '', title)\n",
    "        return ' '.join(title.split())\n",
    "    \n",
    "    print(\"\\nAnalyzing duplicate titles...\")\n",
    "    c.execute('''\n",
    "        SELECT \n",
    "            p.id,\n",
    "            p.title,\n",
    "            p.categories,\n",
    "            STRING_AGG(\n",
    "                a.keyname || \n",
    "                COALESCE(', ' || a.forenames, '') || \n",
    "                COALESCE(' ' || a.suffix, ''),\n",
    "                ', '  # This is the separator BETWEEN authors\n",
    "            ) as authors\n",
    "        FROM papers p\n",
    "        LEFT JOIN paper_authors pa ON p.id = pa.paper_id\n",
    "        LEFT JOIN authors a ON pa.author_id = a.id\n",
    "        WHERE p.title IS NOT NULL AND p.withdrawn = 0  -- Only analyze active papers\n",
    "        GROUP BY p.id\n",
    "    ''')\n",
    "    papers = c.fetchall()\n",
    "    \n",
    "    # Group by normalized title\n",
    "    title_groups = {}\n",
    "    for paper in papers:\n",
    "        norm_title = normalize_title(paper[1])\n",
    "        if norm_title in title_groups:\n",
    "            title_groups[norm_title].append(paper)\n",
    "        else:\n",
    "            title_groups[norm_title] = [paper]\n",
    "    \n",
    "    # Find groups with multiple papers\n",
    "    duplicates = {title: papers for title, papers in title_groups.items() if len(papers) > 1}\n",
    "    \n",
    "    print(f\"\\nDuplicate Title Analysis (Active Papers Only):\")\n",
    "    print(f\"Found {len(duplicates)} groups of papers with identical normalized titles\")\n",
    "    total_dupes = sum(len(papers) for papers in duplicates.values())\n",
    "    print(f\"Total papers involved in duplicates: {total_dupes}\")\n",
    "    \n",
    "    # Print some examples\n",
    "    if duplicates:\n",
    "        print(\"\\nExample duplicate groups:\")\n",
    "        for title, group in list(duplicates.items())[:3]:  # Show first 3 groups\n",
    "            print(f\"\\nNormalized Title: {title}\")\n",
    "            for paper in group:\n",
    "                print(f\"- ID: {paper[0]}\")\n",
    "                print(f\"  Original Title: {paper[1]}\")\n",
    "                print(f\"  Categories: {paper[2]}\")\n",
    "                print(f\"  Authors: {paper[3]}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "# Inspect sample papers\n",
    "inspect_papers(conn) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 3.2 Database Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup_database():\n",
    "    \"\"\"Backup PostgreSQL database to Google Drive\"\"\"\n",
    "    backup_path = \"/content/drive/MyDrive/ai-safety-papers/papers.sql\"\n",
    "    print(f\"Creating PostgreSQL backup at {backup_path}\")\n",
    "    !pg_dump -U postgres -F c -f \"{backup_path}\" papers  # pyright: ignore\n",
    "    print(\"Backup completed successfully\")\n",
    "\n",
    "# Run backup after saving data\n",
    "backup_database()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
