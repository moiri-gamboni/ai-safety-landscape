{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Papers Visualization - Phase 1\n",
    "\n",
    "This notebook implements Phase 1 of the AI Safety Papers visualization project:\n",
    "1. Metadata Collection using arXiv OAI-PMH API\n",
    "2. Abstract Embedding Generation using ModernBERT-large\n",
    "3. Initial Clustering using UMAP and HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Clone the repository\n",
    "    !git clone https://github.com/moiri-gamboni/ai-safety-landscape.git\n",
    "    %cd ai-safety-landscape\n",
    "\n",
    "# Install required packages\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Load Existing Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check if database exists in Drive\n",
    "db_path = \"/content/drive/MyDrive/ai-safety-papers/papers.db\"\n",
    "if os.path.exists(db_path):\n",
    "    print(f\"Found existing database at {db_path}\")\n",
    "    !cp \"{db_path}\" papers.db\n",
    "    \n",
    "    # Print existing data summary\n",
    "    conn = sqlite3.connect('papers.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT COUNT(*) FROM papers')\n",
    "    print(f\"Database contains {c.fetchone()[0]} papers\")\n",
    "else:\n",
    "    print(\"No existing database found in Drive. Will create new one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Create Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "def create_database():\n",
    "    \"\"\"Create SQLite database with necessary tables\"\"\"\n",
    "    conn = sqlite3.connect('papers.db')\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Create papers table with all arXiv metadata fields\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS papers (\n",
    "            id TEXT PRIMARY KEY,           -- Required, maxOccurs=1\n",
    "            title TEXT,                    -- Optional, maxOccurs=1\n",
    "            abstract TEXT,                 -- Optional, maxOccurs=1\n",
    "            categories TEXT,               -- Optional, maxOccurs=1\n",
    "            msc_class TEXT,                -- Optional, maxOccurs=1\n",
    "            acm_class TEXT,                -- Optional, maxOccurs=1\n",
    "            doi TEXT,                      -- Optional, maxOccurs=1\n",
    "            license TEXT,                  -- Optional, maxOccurs=1\n",
    "            comments TEXT,                 -- Optional, maxOccurs=1\n",
    "            created TEXT,                  -- Date of first version\n",
    "            updated TEXT,                  -- Date of latest version\n",
    "            withdrawn BOOLEAN DEFAULT 0,    -- Indicates if paper is withdrawn\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create paper versions table to track version history\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS paper_versions (\n",
    "            paper_id TEXT,\n",
    "            version INTEGER,\n",
    "            source_type TEXT,              -- D for Document, I for Inactive\n",
    "            size TEXT,                     -- Size in kb\n",
    "            date TEXT,                     -- Submission date of this version\n",
    "            PRIMARY KEY (paper_id, version),\n",
    "            FOREIGN KEY (paper_id) REFERENCES papers(id)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create authors table matching arXiv schema\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS authors (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            keyname TEXT NOT NULL,          -- Required, maxOccurs=1\n",
    "            forenames TEXT,                 -- Optional, maxOccurs=1\n",
    "            suffix TEXT,                    -- Optional, maxOccurs=1\n",
    "            UNIQUE(keyname, forenames, suffix)  -- Avoid duplicate authors\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create author affiliations table (since it's maxOccurs=unbounded in schema)\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS author_affiliations (\n",
    "            author_id INTEGER,\n",
    "            affiliation TEXT NOT NULL,\n",
    "            PRIMARY KEY (author_id, affiliation),\n",
    "            FOREIGN KEY (author_id) REFERENCES authors(id)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create paper_authors junction table\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS paper_authors (\n",
    "            paper_id TEXT,\n",
    "            author_id INTEGER,\n",
    "            author_position INTEGER,        -- Track author order in paper\n",
    "            PRIMARY KEY (paper_id, author_id),\n",
    "            FOREIGN KEY (paper_id) REFERENCES papers(id),\n",
    "            FOREIGN KEY (author_id) REFERENCES authors(id)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create indices for common queries after all tables are created\n",
    "    c.execute(\"CREATE INDEX IF NOT EXISTS idx_categories ON papers(categories)\")\n",
    "    c.execute(\"CREATE INDEX IF NOT EXISTS idx_withdrawn ON papers(withdrawn)\")\n",
    "    c.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS idx_authors_unique ON authors(keyname, forenames, suffix)\")\n",
    "    \n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "# Initialize database\n",
    "conn = create_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Harvest Metadata\n",
    "\n",
    "### Configuration\n",
    "- **Number of Papers**: Control how many papers to fetch. Set to 0 to fetch all CS papers (warning: this will take a long time).\n",
    "- **Resumption Token**: If harvesting was interrupted, paste the resumption token here to continue from where you left off.\n",
    "  You can paste the full URL or just the token - URL-encoded characters (like %7C) will be automatically converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Harvesting Configuration  {\"run\":\"auto\"}\n",
    "num_papers = 100 # @param {type:\"slider\", min:0, max:10000, step:100}\n",
    "resumption_token = \"\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from sickle import Sickle\n",
    "from sickle.models import Record\n",
    "from tqdm import tqdm\n",
    "import urllib.parse\n",
    "import re\n",
    "\n",
    "class ArxivRecord(Record):\n",
    "    \"\"\"Custom record class for arXiv metadata format\"\"\"\n",
    "    def get_metadata(self):\n",
    "        # Get the arXiv metadata namespace\n",
    "        ns = {'arxiv': 'http://arxiv.org/OAI/arXiv/'}\n",
    "        \n",
    "        # Get the arXiv element which contains all metadata\n",
    "        arxiv = self.xml.find('.//{http://arxiv.org/OAI/arXiv/}arXiv')\n",
    "        if arxiv is None:\n",
    "            raise ValueError(\"Could not find arXiv metadata element\")\n",
    "            \n",
    "        metadata = {}\n",
    "        \n",
    "        # Required field - use identifier from header\n",
    "        metadata['id'] = self.header.identifier\n",
    "        if metadata['id'].startswith('oai:arXiv.org:'):\n",
    "            metadata['id'] = metadata['id'].replace('oai:arXiv.org:', '')\n",
    "        \n",
    "        # Map arXiv metadata fields according to schema\n",
    "        field_mapping = {\n",
    "            'created': 'created',\n",
    "            'updated': 'updated',\n",
    "            'title': 'title',\n",
    "            'abstract': 'abstract',\n",
    "            'categories': 'categories',\n",
    "            'msc_class': 'msc-class',\n",
    "            'acm_class': 'acm-class',\n",
    "            'report_no': 'report-no',\n",
    "            'journal_ref': 'journal-ref',\n",
    "            'doi': 'doi',\n",
    "            'comments': 'comments',\n",
    "            'license': 'license'\n",
    "        }\n",
    "        \n",
    "        for field, xml_field in field_mapping.items():\n",
    "            elem = arxiv.find(f'arxiv:{xml_field}', namespaces=ns)\n",
    "            metadata[field] = elem.text if elem is not None and elem.text else None\n",
    "        \n",
    "        # Extract authors according to schema\n",
    "        authors = []\n",
    "        authors_elem = arxiv.find('arxiv:authors', namespaces=ns)\n",
    "        if authors_elem is not None:\n",
    "            for author_elem in authors_elem.findall('arxiv:author', namespaces=ns):\n",
    "                if author_elem is None:\n",
    "                    continue\n",
    "                    \n",
    "                author = {}\n",
    "                \n",
    "                # Required keyname field\n",
    "                keyname_elem = author_elem.find('arxiv:keyname', namespaces=ns)\n",
    "                if keyname_elem is None or not keyname_elem.text:\n",
    "                    continue  # Skip authors without keyname\n",
    "                author['keyname'] = keyname_elem.text.rstrip(',')  # Remove trailing comma if present\n",
    "                \n",
    "                # Optional author fields\n",
    "                forenames_elem = author_elem.find('arxiv:forenames', namespaces=ns)\n",
    "                author['forenames'] = forenames_elem.text if forenames_elem is not None and forenames_elem.text else None\n",
    "                \n",
    "                suffix_elem = author_elem.find('arxiv:suffix', namespaces=ns)\n",
    "                author['suffix'] = suffix_elem.text if suffix_elem is not None and suffix_elem.text else None\n",
    "                \n",
    "                # Multiple affiliations possible\n",
    "                author['affiliations'] = []\n",
    "                for affiliation_elem in author_elem.findall('arxiv:affiliation', namespaces=ns):\n",
    "                    if affiliation_elem is not None and affiliation_elem.text:\n",
    "                        author['affiliations'].append(affiliation_elem.text)\n",
    "                \n",
    "                authors.append(author)\n",
    "        \n",
    "        metadata['authors'] = authors\n",
    "        return metadata\n",
    "\n",
    "class ArxivRawRecord(Record):\n",
    "    \"\"\"Custom record class for arXivRaw metadata format\"\"\"\n",
    "    def get_metadata(self):\n",
    "        # Get the arXivRaw metadata namespace\n",
    "        ns = {'arxiv': 'http://arxiv.org/OAI/arXivRaw/'}\n",
    "        \n",
    "        # Get the arXivRaw element which contains all metadata\n",
    "        arxiv = self.xml.find('.//{http://arxiv.org/OAI/arXivRaw/}arXivRaw')\n",
    "        if arxiv is None:\n",
    "            raise ValueError(\"Could not find arXivRaw metadata element\")\n",
    "            \n",
    "        metadata = {}\n",
    "        \n",
    "        # Required field - use identifier from header\n",
    "        metadata['id'] = self.header.identifier\n",
    "        if metadata['id'].startswith('oai:arXiv.org:'):\n",
    "            metadata['id'] = metadata['id'].replace('oai:arXiv.org:', '')\n",
    "        \n",
    "        # Get all versions and sort by version number\n",
    "        versions = []\n",
    "        for version_elem in arxiv.findall('.//arxiv:version', namespaces=ns):\n",
    "            version_num = version_elem.get('version', 'v1').lstrip('v')\n",
    "            try:\n",
    "                version_num = int(version_num)\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            version_info = {\n",
    "                'version': version_num,\n",
    "                'date': version_elem.find('arxiv:date', namespaces=ns).text if version_elem.find('arxiv:date', namespaces=ns) is not None else None,\n",
    "                'size': version_elem.find('arxiv:size', namespaces=ns).text if version_elem.find('arxiv:size', namespaces=ns) is not None else None,\n",
    "                'source_type': version_elem.find('arxiv:source_type', namespaces=ns).text if version_elem.find('arxiv:source_type', namespaces=ns) is not None else 'D'\n",
    "            }\n",
    "            versions.append(version_info)\n",
    "        \n",
    "        # Sort versions by version number\n",
    "        versions.sort(key=lambda x: x['version'])\n",
    "        metadata['versions'] = versions\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "def save_papers(papers, conn):\n",
    "    \"\"\"Save papers and authors to SQLite database\"\"\"\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    def normalize_suffix(suffix):\n",
    "        \"\"\"Normalize author suffixes to a standard format\"\"\"\n",
    "        if not suffix:\n",
    "            return None\n",
    "        suffix = suffix.strip()\n",
    "        # Normalize Jr variations\n",
    "        if suffix.upper() in ['JR', 'JR.', 'JR ', 'JUNIOR']:\n",
    "            return 'Jr.'\n",
    "        # Normalize Sr variations\n",
    "        if suffix.upper() in ['SR', 'SR.', 'SR ', 'SENIOR']:\n",
    "            return 'Sr.'\n",
    "        # Normalize roman numerals\n",
    "        if suffix.upper() in ['I', 'II', 'III', 'IV', 'V']:\n",
    "            return suffix.upper()\n",
    "        return suffix\n",
    "    \n",
    "    # Prepare batch data\n",
    "    paper_data = []\n",
    "    author_data = set()  # Use set to deduplicate authors\n",
    "    paper_author_data = []\n",
    "    affiliation_data = set()  # Use set to deduplicate affiliations\n",
    "    \n",
    "    # Collect all data first\n",
    "    for paper in papers:\n",
    "        # Collect paper data\n",
    "        paper_data.append((\n",
    "            paper['id'],\n",
    "            paper.get('title'),\n",
    "            paper.get('abstract'),\n",
    "            paper.get('categories'),\n",
    "            paper.get('msc_class'),\n",
    "            paper.get('acm_class'),\n",
    "            paper.get('doi'),\n",
    "            paper.get('license'),\n",
    "            paper.get('comments')\n",
    "        ))\n",
    "        \n",
    "        # Collect author and affiliation data\n",
    "        for pos, author in enumerate(paper['authors'], 1):\n",
    "            # Normalize suffix\n",
    "            suffix = normalize_suffix(author.get('suffix'))\n",
    "            \n",
    "            # Add author to set\n",
    "            author_tuple = (\n",
    "                author['keyname'],\n",
    "                author.get('forenames'),\n",
    "                suffix\n",
    "            )\n",
    "            author_data.add(author_tuple)\n",
    "            \n",
    "            # Store paper-author relationship with position\n",
    "            paper_author_data.append((\n",
    "                paper['id'],\n",
    "                author_tuple,  # We'll replace this with ID after inserting authors\n",
    "                pos\n",
    "            ))\n",
    "            \n",
    "            # Add affiliations to set\n",
    "            for affiliation in author.get('affiliations', []):\n",
    "                affiliation_data.add((author_tuple, affiliation))\n",
    "    \n",
    "    try:\n",
    "        # Batch insert papers\n",
    "        c.executemany('''\n",
    "            INSERT OR IGNORE INTO papers (\n",
    "                id, title, abstract, categories,\n",
    "                msc_class, acm_class, doi, license, comments\n",
    "            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', paper_data)\n",
    "        \n",
    "        # Batch insert authors and get their IDs\n",
    "        author_id_map = {}  # Map author tuple to ID\n",
    "        c.executemany('''\n",
    "            INSERT OR IGNORE INTO authors (keyname, forenames, suffix)\n",
    "            VALUES (?, ?, ?)\n",
    "        ''', author_data)\n",
    "        \n",
    "        # Get all author IDs in one query\n",
    "        placeholders = ','.join(['(?, ?, ?)'] * len(author_data))\n",
    "        c.execute(f'''\n",
    "            SELECT id, keyname, forenames, suffix \n",
    "            FROM authors \n",
    "            WHERE (keyname, forenames, suffix) IN ({placeholders})\n",
    "        ''', [val for author in author_data for val in author])\n",
    "        \n",
    "        for row in c.fetchall():\n",
    "            author_id_map[author_tuple] = row[0]\n",
    "        \n",
    "        # Update paper-author data with real IDs\n",
    "        paper_author_final = []\n",
    "        for paper_id, author_tuple, pos in paper_author_data:\n",
    "            try:\n",
    "                author_id = author_id_map[author_tuple]\n",
    "                paper_author_final.append((paper_id, author_id, pos))\n",
    "            except KeyError:\n",
    "                print(f\"Warning: Could not find ID for author {author_tuple}\")\n",
    "        \n",
    "        # Batch insert paper-author relationships\n",
    "        c.executemany('''\n",
    "            INSERT OR IGNORE INTO paper_authors (paper_id, author_id, author_position)\n",
    "            VALUES (?, ?, ?)\n",
    "        ''', paper_author_final)\n",
    "        \n",
    "        # Batch insert affiliations\n",
    "        affiliation_final = [\n",
    "            (author_id_map[author_tuple], affiliation)\n",
    "            for author_tuple, affiliation in affiliation_data\n",
    "        ]\n",
    "        c.executemany('''\n",
    "            INSERT OR IGNORE INTO author_affiliations (author_id, affiliation)\n",
    "            VALUES (?, ?)\n",
    "        ''', affiliation_final)\n",
    "        \n",
    "        conn.commit()\n",
    "        \n",
    "    except sqlite3.IntegrityError as e:\n",
    "        print(f\"Error during batch operations: {str(e)}\")\n",
    "        conn.rollback()\n",
    "\n",
    "def save_versions(paper_versions, conn):\n",
    "    \"\"\"Save paper version information to database efficiently\"\"\"\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Prepare batch data\n",
    "    paper_updates = []\n",
    "    version_inserts = []\n",
    "    \n",
    "    # Collect all data first\n",
    "    for paper_id, versions in paper_versions.items():\n",
    "        # Sort versions by version number\n",
    "        versions = sorted(versions, key=lambda x: x['version'])\n",
    "        \n",
    "        if versions:\n",
    "            # Get first and latest version dates\n",
    "            created = versions[0]['date']\n",
    "            updated = versions[-1]['date']\n",
    "            \n",
    "            # Check if paper is withdrawn based on latest version\n",
    "            latest_version = versions[-1]\n",
    "            withdrawn = (\n",
    "                latest_version['source_type'] == 'I' or\n",
    "                latest_version['size'] == '0kb'\n",
    "            )\n",
    "            \n",
    "            # Add to paper updates\n",
    "            paper_updates.append((created, updated, withdrawn, paper_id))\n",
    "            \n",
    "            # Add all versions\n",
    "            for version in versions:\n",
    "                version_inserts.append((\n",
    "                    paper_id,\n",
    "                    version['version'],\n",
    "                    version['source_type'],\n",
    "                    version['size'],\n",
    "                    version['date']\n",
    "                ))\n",
    "    \n",
    "    try:\n",
    "        # Batch update papers\n",
    "        c.executemany('''\n",
    "            UPDATE papers \n",
    "            SET created = ?, updated = ?, withdrawn = ?\n",
    "            WHERE id = ?\n",
    "        ''', paper_updates)\n",
    "        \n",
    "        # Batch insert versions\n",
    "        c.executemany('''\n",
    "            INSERT OR IGNORE INTO paper_versions (\n",
    "                paper_id, version, source_type, size, date\n",
    "            ) VALUES (?, ?, ?, ?, ?)\n",
    "        ''', version_inserts)\n",
    "        \n",
    "        conn.commit()\n",
    "        \n",
    "    except sqlite3.IntegrityError as e:\n",
    "        print(f\"Error during batch operations: {str(e)}\")\n",
    "        conn.rollback()\n",
    "\n",
    "def fetch_arxiv_records(metadata_prefix, record_class, max_results=None, resumption_token=None):\n",
    "    \"\"\"Base function for fetching records from arXiv OAI-PMH API\"\"\"\n",
    "    sickle = Sickle('http://export.arxiv.org/oai2',\n",
    "                    max_retries=5,\n",
    "                    retry_status_codes=[503],\n",
    "                    default_retry_after=20)\n",
    "    \n",
    "    # Register our custom record class\n",
    "    sickle.class_mapping['ListRecords'] = record_class\n",
    "    \n",
    "    try:\n",
    "        # Use ListRecords with specified format\n",
    "        if resumption_token:\n",
    "            # Extract token from URL if full URL was pasted\n",
    "            if 'resumptionToken=' in resumption_token:\n",
    "                resumption_token = re.search(r'resumptionToken=([^&]+)', resumption_token).group(1)\n",
    "            # Unescape the token\n",
    "            resumption_token = urllib.parse.unquote(resumption_token)\n",
    "            records = sickle.ListRecords(resumptionToken=resumption_token)\n",
    "        else:\n",
    "            records = sickle.ListRecords(\n",
    "                metadataPrefix=metadata_prefix,\n",
    "                set='cs',\n",
    "                ignore_deleted=True\n",
    "            )\n",
    "        \n",
    "        # Process records with progress bar\n",
    "        results = []\n",
    "        with tqdm(desc=f\"Fetching {metadata_prefix}\", unit=\" papers\") as pbar:\n",
    "            while True:\n",
    "                try:\n",
    "                    record = next(records)\n",
    "                    try:\n",
    "                        metadata = record.get_metadata()\n",
    "                        if metadata and metadata.get('id'):  # Only add if we have valid metadata\n",
    "                            results.append(metadata)\n",
    "                            pbar.update(1)\n",
    "                            \n",
    "                            # Respect max_results limit if set\n",
    "                            if max_results and len(results) >= max_results:\n",
    "                                break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing record: {str(e)}\")\n",
    "                        continue\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    # Get the current resumption token from the Sickle iterator\n",
    "                    if hasattr(records, 'resumption_token'):\n",
    "                        print(f\"\\nError occurred. Current resumption token: {records.resumption_token}\")\n",
    "                    raise  # Re-raise the exception to let Sickle's retry mechanism handle it\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during harvesting: {str(e)}\")\n",
    "        # Print final resumption token if available\n",
    "        if hasattr(records, 'resumption_token'):\n",
    "            print(f\"Final resumption token: {records.resumption_token}\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "def fetch_cs_papers(max_results=None, resumption_token=None):\n",
    "    \"\"\"Fetch CS papers from arXiv using OAI-PMH with arXiv metadata format\"\"\"\n",
    "    return fetch_arxiv_records('arXiv', ArxivRecord, max_results, resumption_token)\n",
    "\n",
    "def fetch_raw_metadata(max_results=None, resumption_token=None):\n",
    "    \"\"\"Fetch paper version information from arXiv using OAI-PMH with arXivRaw metadata format\"\"\"\n",
    "    return fetch_arxiv_records('arXivRaw', ArxivRawRecord, max_results, resumption_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Fetch papers using the slider value (None for all papers)\n",
    "# First check current database count\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT COUNT(*) FROM papers')\n",
    "current_count = c.fetchone()[0]\n",
    "print(f\"Current papers in database before harvesting: {current_count}\")\n",
    "\n",
    "# First fetch papers with arXiv metadata\n",
    "initial_papers = fetch_cs_papers(\n",
    "    max_results=num_papers if num_papers > 0 else None,\n",
    "    resumption_token=resumption_token if resumption_token else None\n",
    ")\n",
    "save_papers(initial_papers, conn)\n",
    "\n",
    "# Then fetch version information with arXivRaw\n",
    "paper_versions = fetch_raw_metadata(\n",
    "    max_results=num_papers if num_papers > 0 else None,\n",
    "    resumption_token=resumption_token if resumption_token else None\n",
    ")\n",
    "save_versions(paper_versions, conn)\n",
    "\n",
    "# Print final count\n",
    "c.execute('SELECT COUNT(*) FROM papers')\n",
    "final_count = c.fetchone()[0]\n",
    "print(f\"\\nHarvesting complete:\")\n",
    "print(f\"- Papers added this session: {final_count - current_count}\")\n",
    "print(f\"- Total papers in database: {final_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_papers(conn, limit=5):\n",
    "    \"\"\"Print detailed information for a sample of papers\"\"\"\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Get sample papers with their authors\n",
    "    c.execute('''\n",
    "        SELECT p.id, p.title, p.abstract, p.categories, p.created, p.updated,\n",
    "               GROUP_CONCAT(a.keyname || COALESCE(', ' || a.forenames, '') || COALESCE(' ' || a.suffix, ''))\n",
    "        FROM papers p\n",
    "        LEFT JOIN paper_authors pa ON p.id = pa.paper_id\n",
    "        LEFT JOIN authors a ON pa.author_id = a.id\n",
    "        GROUP BY p.id\n",
    "        LIMIT ?\n",
    "    ''', (limit,))\n",
    "    \n",
    "    papers = c.fetchall()\n",
    "    \n",
    "    print(f\"Inspecting {len(papers)} sample papers:\\n\")\n",
    "    for paper in papers:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ID: {paper[0]}\")\n",
    "        print(f\"Title: {paper[1]}\")\n",
    "        print(f\"Abstract: {paper[2][:200]}...\" if paper[2] else \"Abstract: None\")\n",
    "        print(f\"Categories: {paper[3]}\")\n",
    "        print(f\"Created: {paper[4]}\")\n",
    "        print(f\"Updated: {paper[5]}\")\n",
    "        print(f\"Authors: {paper[6]}\")\n",
    "        print()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\nDatabase Statistics:\")\n",
    "    \n",
    "    # Paper statistics\n",
    "    c.execute('SELECT COUNT(*) FROM papers')\n",
    "    total_papers = c.fetchone()[0]\n",
    "    print(f\"\\nPapers:\")\n",
    "    print(f\"Total Papers: {total_papers}\")\n",
    "    \n",
    "    # Count cs.AI papers\n",
    "    c.execute(\"SELECT COUNT(*) FROM papers WHERE categories LIKE '%cs.AI%'\")\n",
    "    ai_papers = c.fetchone()[0]\n",
    "    print(f\"CS.AI Papers: {ai_papers} ({(ai_papers/total_papers*100):.1f}% of total)\")\n",
    "    \n",
    "    # Category statistics\n",
    "    print(\"\\nTop Categories:\")\n",
    "    c.execute('''\n",
    "        SELECT category, COUNT(*) as count\n",
    "        FROM (\n",
    "            SELECT TRIM(value) as category\n",
    "            FROM papers, json_each('[\"' || REPLACE(categories, ' ', '\",\"') || '\"]')\n",
    "            WHERE categories IS NOT NULL\n",
    "        )\n",
    "        WHERE category != ''\n",
    "        GROUP BY category\n",
    "        ORDER BY count DESC\n",
    "        LIMIT 10\n",
    "    ''')\n",
    "    for category, count in c.fetchall():\n",
    "        if category:  # Skip empty category\n",
    "            percentage = (count / total_papers) * 100\n",
    "            print(f\"{category}: {count} papers ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Author statistics\n",
    "    print(f\"\\nAuthors:\")\n",
    "    c.execute('SELECT COUNT(*) FROM authors')\n",
    "    total_authors = c.fetchone()[0]\n",
    "    print(f\"Total Authors: {total_authors}\")\n",
    "    \n",
    "    c.execute('SELECT COUNT(*) FROM paper_authors')\n",
    "    total_author_links = c.fetchone()[0]\n",
    "    print(f\"Total Author-Paper Links: {total_author_links}\")\n",
    "    \n",
    "    # Papers per author distribution\n",
    "    print(\"\\nPapers per author distribution:\")\n",
    "    c.execute('''\n",
    "        SELECT papers_count, COUNT(*) as authors_with_this_many_papers\n",
    "        FROM (\n",
    "            SELECT author_id, COUNT(*) as papers_count\n",
    "            FROM paper_authors\n",
    "            GROUP BY author_id\n",
    "        )\n",
    "        GROUP BY papers_count\n",
    "        ORDER BY papers_count\n",
    "        LIMIT 10\n",
    "    ''')\n",
    "    for paper_count, author_count in c.fetchall():\n",
    "        print(f\"{author_count} authors have {paper_count} paper(s)\")\n",
    "    \n",
    "    c.execute('''\n",
    "        SELECT COUNT(*) FROM \n",
    "        (SELECT author_id FROM paper_authors GROUP BY author_id HAVING COUNT(*) > 1)\n",
    "    ''')\n",
    "    authors_multiple_papers = c.fetchone()[0]\n",
    "    print(f\"\\nAuthors with Multiple Papers: {authors_multiple_papers}\")\n",
    "    \n",
    "    c.execute('SELECT AVG(author_count) FROM (SELECT paper_id, COUNT(*) as author_count FROM paper_authors GROUP BY paper_id)')\n",
    "    avg_authors_per_paper = c.fetchone()[0]\n",
    "    print(f\"\\nAverage Authors per Paper: {avg_authors_per_paper:.2f}\")\n",
    "    \n",
    "    # Affiliation statistics\n",
    "    print(\"\\nAffiliations:\")\n",
    "    c.execute('SELECT COUNT(DISTINCT affiliation) FROM author_affiliations')\n",
    "    total_unique_affiliations = c.fetchone()[0]\n",
    "    print(f\"Unique Affiliations: {total_unique_affiliations}\")\n",
    "    \n",
    "    c.execute('SELECT COUNT(*) FROM author_affiliations')\n",
    "    total_affiliation_links = c.fetchone()[0]\n",
    "    print(f\"Total Author-Affiliation Links: {total_affiliation_links}\")\n",
    "    \n",
    "    c.execute('''\n",
    "        SELECT COUNT(*) FROM \n",
    "        (SELECT author_id FROM author_affiliations GROUP BY author_id)\n",
    "    ''')\n",
    "    authors_with_affiliations = c.fetchone()[0]\n",
    "    print(f\"Authors with Affiliations: {authors_with_affiliations} ({(authors_with_affiliations/total_authors*100):.1f}% of authors)\")\n",
    "    \n",
    "    print(\"\\nTop 10 Affiliations:\")\n",
    "    c.execute('''\n",
    "        SELECT affiliation, COUNT(*) as count\n",
    "        FROM author_affiliations\n",
    "        GROUP BY affiliation\n",
    "        ORDER BY count DESC\n",
    "        LIMIT 10\n",
    "    ''')\n",
    "    for affiliation, count in c.fetchall():\n",
    "        print(f\"- {affiliation}: {count} authors\")\n",
    "    \n",
    "    # Metadata field statistics\n",
    "    print(\"\\nMetadata Field Coverage:\")\n",
    "    fields = [\n",
    "        'title', 'abstract', 'categories', 'created', 'updated',\n",
    "        'msc_class', 'acm_class', 'doi', 'license'\n",
    "    ]\n",
    "    \n",
    "    for field in fields:\n",
    "        c.execute(f'SELECT COUNT(*) FROM papers WHERE {field} IS NOT NULL')\n",
    "        present = c.fetchone()[0]\n",
    "        percentage = (present / total_papers) * 100 if total_papers > 0 else 0\n",
    "        print(f\"{field}: {present}/{total_papers} ({percentage:.1f}%)\")\n",
    "\n",
    "    # Add withdrawn paper statistics after paper statistics\n",
    "    print(\"\\nWithdrawn Paper Statistics:\")\n",
    "    c.execute('SELECT COUNT(*) FROM papers WHERE withdrawn = 1')\n",
    "    withdrawn_count = c.fetchone()[0]\n",
    "    c.execute('SELECT COUNT(*) FROM papers WHERE withdrawn = 0')\n",
    "    active_count = c.fetchone()[0]\n",
    "    print(f\"Active Papers: {active_count} ({(active_count/total_papers*100):.1f}% of total)\")\n",
    "    print(f\"Withdrawn Papers: {withdrawn_count} ({(withdrawn_count/total_papers*100):.1f}% of total)\")\n",
    "    \n",
    "    # Check for papers with \"withdrawn\" in comments but not marked withdrawn\n",
    "    c.execute('''\n",
    "        SELECT id, title, comments \n",
    "        FROM papers \n",
    "        WHERE comments LIKE '%withdrawn%' AND withdrawn = 0\n",
    "    ''')\n",
    "    potential_withdrawn = c.fetchall()\n",
    "    if potential_withdrawn:\n",
    "        print(f\"\\nFound {len(potential_withdrawn)} papers with 'withdrawn' in comments but not marked withdrawn:\")\n",
    "        for i, paper in enumerate(potential_withdrawn[:5], 1):  # Show up to 5 examples\n",
    "            print(f\"\\n{i}. Paper ID: {paper[0]}\")\n",
    "            print(f\"   Title: {paper[1]}\")\n",
    "            print(f\"   Comments: {paper[2]}\")\n",
    "    else:\n",
    "        print(\"\\nNo papers found with 'withdrawn' in comments but not marked withdrawn\")\n",
    "\n",
    "# Inspect sample papers\n",
    "inspect_papers(conn) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Save Database to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory if it doesn't exist\n",
    "!mkdir -p \"/content/drive/MyDrive/ai-safety-papers\"\n",
    "\n",
    "# Copy database to Drive\n",
    "!cp papers.db \"/content/drive/MyDrive/ai-safety-papers/papers.db\"\n",
    "print(\"Database saved to Google Drive at: /ai-safety-papers/papers.db\") "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
