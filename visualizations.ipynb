{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Papers - Clustering Visualizations\n",
    "\n",
    "This notebook provides visualizations and analysis tools for exploring the clustering results:\n",
    "1. Basic cluster visualization\n",
    "2. Cluster metrics and statistics\n",
    "3. Hierarchical structure exploration\n",
    "4. Cluster content analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive # pyright: ignore [reportMissingImports]\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Install and configure PostgreSQL\n",
    "    !sudo apt-get -qq update && sudo apt-get -qq install postgresql postgresql-contrib # pyright: ignore\n",
    "    !sudo service postgresql start # pyright: ignore\n",
    "    !sudo sed -i 's/local\\s*all\\s*postgres\\s*peer/local all postgres trust/' /etc/postgresql/14/main/pg_hba.conf # pyright: ignore\n",
    "    !sudo service postgresql restart # pyright: ignore\n",
    "    \n",
    "    # Install Python client\n",
    "    %pip install psycopg2-binary umap-learn # pyright: ignore\n",
    "    %pip install matplotlib seaborn scipy scikit-learn networkx umap-learn hdbscan # pyright: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "import numpy as np\n",
    "from psycopg2.extras import DictCursor\n",
    "import json\n",
    "\n",
    "# Path to database\n",
    "db_path = \"/content/drive/MyDrive/ai-safety-papers/papers.sql\"\n",
    "\n",
    "def load_database():\n",
    "    \"\"\"Load PostgreSQL backup using psql\"\"\"\n",
    "    print(\"Loading PostgreSQL backup...\")\n",
    "    !pg_restore -U postgres --jobs=8 -f \"{db_path}\" # pyright: ignore\n",
    "\n",
    "def connect_db():\n",
    "    \"\"\"Connect to PostgreSQL database with schema validation\"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        host='',\n",
    "        database=\"papers\",\n",
    "        user=\"postgres\",\n",
    "        cursor_factory=DictCursor\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "load_database()\n",
    "conn = connect_db()\n",
    "\n",
    "# Load best trial immediately after connection\n",
    "def get_best_trial():\n",
    "    \"\"\"Load best trial ID and metrics from JSON\"\"\"\n",
    "    drive_path = \"/content/drive/MyDrive/ai-safety-papers/best_trial.json\"\n",
    "    with open(drive_path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "best_trial_data = get_best_trial()\n",
    "best_trial = best_trial_data['trial_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.widgets import Slider\n",
    "import networkx as nx\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.spatial.distance import cdist, squareform\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_cluster_keywords(papers, n_keywords=10):\n",
    "    \"\"\"Extract keywords using TF-IDF\"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "    # Combine title and abstract\n",
    "    texts = [f\"{p['title']} {p['abstract']}\" for p in papers]\n",
    "    \n",
    "    # TF-IDF with custom parameters for scientific text\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_df=0.7,                      # Ignore terms that appear in >70% of docs\n",
    "        min_df=3,                        # Ignore terms that appear in <3 docs\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),             # Include bigrams\n",
    "        token_pattern=r'(?u)\\b[A-Za-z][A-Za-z-]+\\b'  # Words starting with letter\n",
    "    )\n",
    "    \n",
    "    # Get TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Sum TF-IDF scores for each term\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = tfidf_matrix.sum(axis=0).A1\n",
    "    \n",
    "    # Get top keywords\n",
    "    top_indices = scores.argsort()[-n_keywords:][::-1]\n",
    "    keywords = [(feature_names[i], scores[i]) for i in top_indices]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def get_clusters_at_lambda(lambda_val):\n",
    "    \"\"\"Get cluster assignments at a specific lambda level\"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            WITH RECURSIVE cluster_tree AS (\n",
    "                SELECT ct.parent_cluster_id as cluster_id, ct.lambda_val\n",
    "                FROM cluster_trees ct\n",
    "                WHERE ct.trial_id = %s\n",
    "                AND ct.parent_cluster_id NOT IN (\n",
    "                    SELECT child_cluster_id \n",
    "                    FROM cluster_trees \n",
    "                    WHERE trial_id = %s\n",
    "                )\n",
    "                UNION ALL\n",
    "                SELECT ct.child_cluster_id, ct.lambda_val\n",
    "                FROM cluster_trees ct\n",
    "                JOIN cluster_tree t ON ct.parent_cluster_id = t.cluster_id\n",
    "                WHERE ct.trial_id = %s\n",
    "                AND ct.lambda_val >= %s\n",
    "            )\n",
    "            SELECT DISTINCT cluster_id, lambda_val\n",
    "            FROM cluster_tree\n",
    "            WHERE lambda_val >= %s\n",
    "            ORDER BY cluster_id\n",
    "        ''', (best_trial, best_trial, best_trial, lambda_val, lambda_val))\n",
    "        return cursor.fetchall()\n",
    "\n",
    "def get_papers_in_cluster(cluster_id):\n",
    "    \"\"\"Get all papers belonging to a specific cluster\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "        SELECT a.paper_id, p.title, p.abstract, a.cluster_prob\n",
    "        FROM artifacts a\n",
    "        JOIN papers p ON a.paper_id = p.id\n",
    "        WHERE a.trial_id = %s AND a.cluster_id = %s\n",
    "        ORDER BY a.cluster_prob DESC\n",
    "    ''', (best_trial, cluster_id))\n",
    "    \n",
    "    return cursor.fetchall()\n",
    "\n",
    "def get_cluster_hierarchy_levels():\n",
    "    \"\"\"Get all available lambda levels in the hierarchy\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "        SELECT DISTINCT lambda_val\n",
    "        FROM cluster_trees\n",
    "        WHERE trial_id = %s\n",
    "        ORDER BY lambda_val DESC\n",
    "    ''', (best_trial,))\n",
    "    \n",
    "    return [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "def analyze_cluster_at_level(cluster_id, lambda_val):\n",
    "    \"\"\"Analyze a specific cluster at a given level\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get papers in this cluster\n",
    "    papers = get_papers_in_cluster(cluster_id)\n",
    "    \n",
    "    # Get immediate child clusters\n",
    "    cursor.execute('''\n",
    "        SELECT child_cluster_id, child_size\n",
    "        FROM cluster_trees\n",
    "        WHERE trial_id = %s \n",
    "          AND parent_cluster_id = %s\n",
    "          AND lambda_val >= %s\n",
    "        ORDER BY child_size DESC\n",
    "    ''', (best_trial, cluster_id, lambda_val))\n",
    "    \n",
    "    children = cursor.fetchall()\n",
    "    \n",
    "    # Extract keywords for the cluster\n",
    "    paper_dicts = [{'title': p[1], 'abstract': p[2]} for p in papers]\n",
    "    keywords = extract_cluster_keywords(paper_dicts) if paper_dicts else []\n",
    "    \n",
    "    return {\n",
    "        'cluster_id': cluster_id,\n",
    "        'size': len(papers),\n",
    "        'keywords': keywords,\n",
    "        'children': children,\n",
    "        'papers': papers\n",
    "    }\n",
    "\n",
    "def calculate_validity_metrics():\n",
    "    \"\"\"Calculate HDBSCAN validity metrics for the best trial\"\"\"\n",
    "    import hdbscan\n",
    "    \n",
    "    # Get all cluster assignments and embeddings\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT a.cluster_id, p.embedding \n",
    "        FROM artifacts a\n",
    "        JOIN papers p ON a.paper_id = p.id\n",
    "        WHERE a.trial_id = %s AND a.cluster_id != -1\n",
    "    ''', (best_trial,))\n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    labels = np.array([r[0] for r in results])\n",
    "    embeddings = np.vstack([np.frombuffer(r[1], dtype=np.float32) for r in results])\n",
    "    \n",
    "    # Calculate validity metrics\n",
    "    validity = hdbscan.validity.validity_index(\n",
    "        X=embeddings,\n",
    "        labels=labels,\n",
    "        metric='euclidean',\n",
    "        d=embeddings.shape[1],\n",
    "        per_cluster_scores=True\n",
    "    )\n",
    "    \n",
    "    # Get density separation between clusters\n",
    "    unique_labels = np.unique(labels)\n",
    "    density_separations = []\n",
    "    for i in range(len(unique_labels)):\n",
    "        for j in range(i+1, len(unique_labels)):\n",
    "            sep = hdbscan.validity.density_separation(\n",
    "                X=embeddings,\n",
    "                labels=labels,\n",
    "                cluster_id1=unique_labels[i],\n",
    "                cluster_id2=unique_labels[j],\n",
    "                internal_nodes1=None,\n",
    "                internal_nodes2=None,\n",
    "                core_distances1=None,\n",
    "                core_distances2=None\n",
    "            )\n",
    "            density_separations.append(sep)\n",
    "    \n",
    "    # Store per-cluster metrics in database\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS cluster_metrics (\n",
    "                trial_id INTEGER NOT NULL,\n",
    "                cluster_id INTEGER NOT NULL,\n",
    "                validity_index REAL,\n",
    "                density_separation REAL,\n",
    "                PRIMARY KEY (trial_id, cluster_id)\n",
    "            )\n",
    "        ''')\n",
    "        for cluster_id, validity_score in validity[1].items():\n",
    "            cursor.execute('''\n",
    "                INSERT INTO cluster_metrics \n",
    "                (trial_id, cluster_id, validity_index)\n",
    "                VALUES (%s, %s, %s)\n",
    "                ON CONFLICT (trial_id, cluster_id) DO UPDATE SET\n",
    "                    validity_index = EXCLUDED.validity_index\n",
    "            ''', (best_trial, cluster_id, validity_score))\n",
    "        conn.commit()\n",
    "    \n",
    "    return {\n",
    "        'overall_validity': validity[0],\n",
    "        'mean_density_separation': np.mean(density_separations),\n",
    "        'min_density_separation': np.min(density_separations)\n",
    "    }\n",
    "\n",
    "def calculate_and_store_validity_metrics():\n",
    "    \"\"\"Calculate and store validity metrics if missing\"\"\"\n",
    "    if 'validity_index' not in best_trial_data['metrics']:\n",
    "        # Calculate metrics\n",
    "        validity_metrics = calculate_validity_metrics()\n",
    "        \n",
    "        # Update JSON data\n",
    "        best_trial_data['metrics'].update({\n",
    "            'validity_index': validity_metrics['overall_validity'],\n",
    "            'mean_density_sep': validity_metrics['mean_density_separation'],\n",
    "            'min_density_sep': validity_metrics['min_density_separation']\n",
    "        })\n",
    "        \n",
    "        # Save to drive\n",
    "        drive_path = \"/content/drive/MyDrive/ai-safety-papers/best_trial.json\"\n",
    "        with open(drive_path, 'w') as f:\n",
    "            json.dump(best_trial_data, f)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Validity Index: {validity_metrics['overall_validity']:.3f}\")\n",
    "        print(f\"Mean Density Separation: {validity_metrics['mean_density_separation']:.3f}\")\n",
    "        print(f\"Min Density Separation: {validity_metrics['min_density_separation']:.3f}\")\n",
    "    else:\n",
    "        print(\"Validity metrics already calculated\")\n",
    "\n",
    "def perform_2d_umap():\n",
    "    \"\"\"Generate and store 2D UMAP embeddings\"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            ALTER TABLE artifacts \n",
    "            ADD COLUMN IF NOT EXISTS viz_embedding BYTEA\n",
    "        ''')\n",
    "        conn.commit()\n",
    "\n",
    "        # Check if embeddings already exist\n",
    "        cursor.execute('''\n",
    "            SELECT COUNT(*) \n",
    "            FROM artifacts \n",
    "            WHERE trial_id = %s \n",
    "              AND viz_embedding IS NOT NULL\n",
    "        ''', (best_trial,))\n",
    "        if cursor.fetchone()[0] > 0:\n",
    "            print(\"2D UMAP embeddings already exist\")\n",
    "            return\n",
    "        \n",
    "        # Load embeddings from papers table\n",
    "        cursor.execute('''\n",
    "            SELECT p.id, p.embedding \n",
    "            FROM papers p\n",
    "            JOIN artifacts a ON p.id = a.paper_id\n",
    "            WHERE a.trial_id = %s\n",
    "        ''', (best_trial,))\n",
    "        results = cursor.fetchall()\n",
    "        \n",
    "        # Convert to numpy array instead of cupy\n",
    "        embeddings = np.vstack([np.frombuffer(r[1], dtype=np.float32) for r in results])\n",
    "        \n",
    "        # Use CPU-based UMAP implementation with trial parameters\n",
    "        reducer = UMAP(\n",
    "            n_components=2,  # Fixed for visualization\n",
    "            n_neighbors=best_trial_data['params']['n_neighbors'],\n",
    "        )\n",
    "        viz_embeddings = reducer.fit_transform(embeddings)\n",
    "        \n",
    "        # Store results\n",
    "        for i, (paper_id, _) in enumerate(results):\n",
    "            cursor.execute('''\n",
    "                UPDATE artifacts\n",
    "                SET viz_embedding = %s\n",
    "                WHERE trial_id = %s AND paper_id = %s\n",
    "            ''', (viz_embeddings[i].tobytes(), best_trial, paper_id))\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"2D UMAP embeddings generated and stored\")\n",
    "        \n",
    "        # Create compressed backup\n",
    "        backup_database()\n",
    "\n",
    "def backup_database():\n",
    "    \"\"\"Create compressed database backup\"\"\"\n",
    "    backup_path = \"/content/drive/MyDrive/ai-safety-papers/papers.sql\"\n",
    "    print(f\"Creating compressed backup at {backup_path}\")\n",
    "    !pg_dump -U postgres -F c -f \"{backup_path}\" papers # pyright: ignore\n",
    "    print(\"Backup completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_clusters(figsize=(15, 15)):\n",
    "    \"\"\"Plot clusters with probability-based transparency\"\"\"\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT a.paper_id, a.cluster_id, a.cluster_prob, a.viz_embedding\n",
    "        FROM artifacts a\n",
    "        WHERE a.trial_id = %s\n",
    "    ''', (best_trial,))\n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    # Convert embeddings directly to numpy\n",
    "    coords = np.vstack([np.frombuffer(r[3], dtype=np.float32) for r in results])\n",
    "    labels = np.array([r[1] for r in results])\n",
    "    probabilities = np.array([r[2] for r in results])\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Set up colors for clusters\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, n_clusters))\n",
    "    \n",
    "    # Plot noise points first\n",
    "    noise_mask = labels == -1\n",
    "    if np.any(noise_mask):\n",
    "        plt.scatter(\n",
    "            coords[noise_mask, 0],\n",
    "            coords[noise_mask, 1],\n",
    "            c='lightgray',\n",
    "            marker='.',\n",
    "            alpha=0.1,\n",
    "            label='Noise'\n",
    "        )\n",
    "    \n",
    "    # Plot clusters\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        if label == -1:\n",
    "            continue\n",
    "            \n",
    "        mask = labels == label\n",
    "        plt.scatter(\n",
    "            coords[mask, 0],\n",
    "            coords[mask, 1],\n",
    "            c=[colors[i]],\n",
    "            marker='.',\n",
    "            alpha=probabilities[mask],\n",
    "            label=f'Cluster {label}'\n",
    "        )\n",
    "    \n",
    "    plt.title('UMAP Visualization of Paper Clusters')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_cluster_metrics():\n",
    "    \"\"\"Plot metrics for the best clustering run\"\"\"\n",
    "    # Get metrics from loaded JSON data\n",
    "    metrics = best_trial_data['metrics']\n",
    "    params = best_trial_data['params']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    fig.suptitle(f'Clustering Trial {best_trial} Analysis')\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        # Cluster sizes plot\n",
    "        cursor.execute('''\n",
    "            SELECT cluster_id, COUNT(*) as size\n",
    "            FROM artifacts\n",
    "            WHERE trial_id = %s AND cluster_id >= 0\n",
    "            GROUP BY cluster_id\n",
    "        ''', (best_trial,))\n",
    "        sizes = [row[1] for row in cursor.fetchall()]\n",
    "        \n",
    "    ax = axes[0, 0]\n",
    "    ax.bar(range(len(sizes)), sizes)\n",
    "    ax.set_title('Cluster Sizes')\n",
    "\n",
    "    # Probability distribution plot\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            SELECT cluster_prob FROM artifacts\n",
    "            WHERE trial_id = %s AND cluster_id >= 0\n",
    "        ''', (best_trial,))\n",
    "        probs = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    sns.histplot(probs, bins=50, ax=ax)\n",
    "    ax.set_title('Probability Distribution')\n",
    "\n",
    "    # Metrics summary\n",
    "    ax = axes[1, 0]\n",
    "    metric_keys = ['noise_ratio', 'n_clusters', 'mean_persistence']\n",
    "    metric_values = [metrics[k] for k in metric_keys]\n",
    "    ax.bar([k.replace('_', ' ').title() for k in metric_keys], metric_values)\n",
    "    ax.set_title('Quality Metrics')\n",
    "\n",
    "    # Parameters summary\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "    params_text = '\\n'.join(f\"{k}: {v}\" for k,v in params.items())\n",
    "    ax.text(0.1, 0.5, params_text, fontfamily='monospace')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_cluster_dendrogram(max_clusters=30):\n",
    "    \"\"\"Visualize cluster hierarchy using dendrogram\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "        SELECT parent_cluster_id, child_cluster_id, lambda_val, child_size\n",
    "        FROM cluster_trees\n",
    "        WHERE trial_id = %s\n",
    "        ORDER BY lambda_val DESC\n",
    "    ''', (best_trial,))\n",
    "    \n",
    "    tree_data = cursor.fetchall()\n",
    "    \n",
    "    if not tree_data:\n",
    "        print(\"No hierarchy data found for this run\")\n",
    "        return\n",
    "    \n",
    "    # Create distance matrix from tree\n",
    "    all_nodes = set()\n",
    "    for row in tree_data:\n",
    "        all_nodes.add(row[0])\n",
    "        all_nodes.add(row[1])\n",
    "    \n",
    "    n_points = len(all_nodes)\n",
    "    distances = np.zeros((n_points, n_points))\n",
    "    \n",
    "    # Fill distance matrix based on lambda values\n",
    "    for parent, child, lambda_val, _ in tree_data:\n",
    "        distances[parent, child] = lambda_val\n",
    "        distances[child, parent] = lambda_val\n",
    "    \n",
    "    # Convert to condensed form\n",
    "    condensed_dist = squareform(distances)\n",
    "    \n",
    "    # Plot dendrogram\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    dendrogram(\n",
    "        condensed_dist,\n",
    "        p=min(max_clusters, n_points),\n",
    "        truncate_mode='lastp',\n",
    "        show_leaf_counts=True\n",
    "    )\n",
    "    plt.title('Hierarchical Clustering Dendrogram')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Distance (λ)')\n",
    "    plt.show()\n",
    "\n",
    "def plot_cluster_network(min_cluster_size=5):\n",
    "    \"\"\"Visualize cluster relationships as a network\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "        SELECT parent_cluster_id, child_cluster_id, lambda_val, child_size\n",
    "        FROM cluster_trees\n",
    "        WHERE trial_id = %s\n",
    "        ORDER BY lambda_val DESC\n",
    "    ''', (best_trial,))\n",
    "    tree_data = cursor.fetchall()\n",
    "    \n",
    "    if not tree_data:\n",
    "        print(\"No hierarchy data found for this run\")\n",
    "        return\n",
    "    \n",
    "    # Create networkx graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Get lambda range for normalization\n",
    "    lambda_vals = [row[2] for row in tree_data]\n",
    "    lambda_range = max(lambda_vals) - min(lambda_vals)\n",
    "    \n",
    "    # Track clusters and their papers\n",
    "    cluster_papers = {}\n",
    "    \n",
    "    # Add nodes and edges\n",
    "    for parent, child, lambda_val, child_size in tree_data:\n",
    "        if child_size > 1:  # Skip individual points\n",
    "            # Normalize lambda value to [0,1] for visualization\n",
    "            lambda_norm = (lambda_val - min(lambda_vals)) / lambda_range\n",
    "            \n",
    "            # Add nodes if they don't exist\n",
    "            if parent not in G:\n",
    "                G.add_node(parent, level=lambda_norm)\n",
    "            if child not in G:\n",
    "                G.add_node(child, level=lambda_norm)\n",
    "            \n",
    "            # Add edge\n",
    "            G.add_edge(parent, child, weight=child_size)\n",
    "            \n",
    "            # Get papers for this cluster\n",
    "            papers = get_papers_in_cluster(child)\n",
    "            if papers:\n",
    "                cluster_papers[child] = [\n",
    "                    {'title': p[1], 'abstract': p[2]} \n",
    "                    for p in papers\n",
    "                ]\n",
    "    \n",
    "    # Calculate layout\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Draw edges with varying thickness based on cluster size\n",
    "    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "    max_weight = max(edge_weights)\n",
    "    edge_widths = [2 * w / max_weight for w in edge_weights]\n",
    "    \n",
    "    # Draw the graph\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.5)\n",
    "    \n",
    "    # Draw nodes with size based on number of papers\n",
    "    node_sizes = []\n",
    "    node_colors = []\n",
    "    labels = {}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        if node in cluster_papers:\n",
    "            size = len(cluster_papers[node])\n",
    "            node_sizes.append(1000 * size / len(cluster_papers))\n",
    "            node_colors.append('lightblue')\n",
    "            \n",
    "            # Extract keywords for cluster label\n",
    "            if size > min_cluster_size:\n",
    "                keywords = extract_cluster_keywords(cluster_papers[node], n_keywords=3)\n",
    "                labels[node] = '\\n'.join([k[0] for k in keywords])\n",
    "        else:\n",
    "            node_sizes.append(100)\n",
    "            node_colors.append('gray')\n",
    "            labels[node] = ''\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, \n",
    "                          node_size=node_sizes,\n",
    "                          node_color=node_colors,\n",
    "                          alpha=0.6)\n",
    "    \n",
    "    # Add labels with smaller font\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "    \n",
    "    plt.title('Cluster Hierarchy Network')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return G, cluster_papers\n",
    "\n",
    "def plot_hierarchical_clusters(figsize=(20, 10)):\n",
    "    \"\"\"Plot clusters at different hierarchical levels with interactive controls\"\"\"\n",
    "    # Get all available lambda levels\n",
    "    lambda_levels = get_cluster_hierarchy_levels()\n",
    "    if not lambda_levels:\n",
    "        print(\"No hierarchy levels found for this run\")\n",
    "        return\n",
    "    \n",
    "    # Get the 2D embeddings and original cluster assignments\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT a.paper_id, a.cluster_id, a.cluster_prob, a.viz_embedding\n",
    "        FROM artifacts a\n",
    "        WHERE a.trial_id = %s\n",
    "    ''', (best_trial,))\n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    # Convert embeddings to numpy array\n",
    "    coords = np.vstack([np.frombuffer(r[3], dtype=np.float32) for r in results])\n",
    "    original_labels = np.array([r[1] for r in results])\n",
    "    probabilities = np.array([r[2] for r in results])\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = plt.GridSpec(2, 2, height_ratios=[1, 0.1])\n",
    "    \n",
    "    # Main scatter plot\n",
    "    ax_scatter = fig.add_subplot(gs[0, :])\n",
    "    \n",
    "    # Slider axes\n",
    "    ax_slider = fig.add_subplot(gs[1, :])\n",
    "    \n",
    "    # Initialize scatter plot\n",
    "    scatter = None\n",
    "    \n",
    "    def update_plot(lambda_val):\n",
    "        nonlocal scatter\n",
    "        \n",
    "        # Clear previous scatter plot\n",
    "        if scatter is not None:\n",
    "            scatter.remove()\n",
    "        ax_scatter.clear()\n",
    "        \n",
    "        # Get clusters at this level\n",
    "        clusters = get_clusters_at_lambda(lambda_val)\n",
    "        cluster_ids = [c[0] for c in clusters]\n",
    "        \n",
    "        # Set up colors for clusters\n",
    "        unique_labels = np.unique(cluster_ids)\n",
    "        n_clusters = len(unique_labels)\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, max(n_clusters, 1)))\n",
    "        \n",
    "        # Plot noise points first\n",
    "        noise_mask = ~np.isin(original_labels, cluster_ids)\n",
    "        if np.any(noise_mask):\n",
    "            ax_scatter.scatter(\n",
    "                coords[noise_mask, 0],\n",
    "                coords[noise_mask, 1],\n",
    "                c='lightgray',\n",
    "                marker='.',\n",
    "                alpha=0.1,\n",
    "                label='Noise'\n",
    "            )\n",
    "        \n",
    "        # Plot clusters\n",
    "        for i, label in enumerate(unique_labels):\n",
    "            mask = original_labels == label\n",
    "            scatter = ax_scatter.scatter(\n",
    "                coords[mask, 0],\n",
    "                coords[mask, 1],\n",
    "                c=[colors[i]],\n",
    "                marker='.',\n",
    "                alpha=probabilities[mask],\n",
    "                label=f'Cluster {label}'\n",
    "            )\n",
    "        \n",
    "        # Get cluster info\n",
    "        cluster_info = []\n",
    "        for cluster_id in unique_labels:\n",
    "            analysis = analyze_cluster_at_level(cluster_id, lambda_val)\n",
    "            if analysis['keywords']:\n",
    "                keywords = \", \".join(k[0] for k in analysis['keywords'][:3])\n",
    "                cluster_info.append(f\"Cluster {cluster_id}: {keywords}\")\n",
    "        \n",
    "        # Update title with cluster information\n",
    "        title = f'Clusters at λ = {lambda_val:.3f}\\n'\n",
    "        title += \"\\n\".join(cluster_info)\n",
    "        ax_scatter.set_title(title)\n",
    "        \n",
    "        # Add legend\n",
    "        ax_scatter.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    # Create slider\n",
    "    lambda_min, lambda_max = min(lambda_levels), max(lambda_levels)\n",
    "    slider = Slider(\n",
    "        ax=ax_slider,\n",
    "        label='Lambda Level',\n",
    "        valmin=lambda_min,\n",
    "        valmax=lambda_max,\n",
    "        valinit=lambda_max,\n",
    "        valstep=sorted(lambda_levels)\n",
    "    )\n",
    "    \n",
    "    # Update function for slider\n",
    "    def update(val):\n",
    "        update_plot(slider.val)\n",
    "    \n",
    "    slider.on_changed(update)\n",
    "    \n",
    "    # Initial plot\n",
    "    update_plot(lambda_max)\n",
    "    \n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def print_cluster_hierarchy(max_depth=None):\n",
    "    \"\"\"Print the cluster hierarchy in a tree-like format\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    def print_cluster(cluster_id, depth=0, min_lambda=None):\n",
    "        if max_depth is not None and depth >= max_depth:\n",
    "            return\n",
    "            \n",
    "        # Get cluster info\n",
    "        analysis = analyze_cluster_at_level(cluster_id, min_lambda or 0)\n",
    "        \n",
    "        # Print cluster details with indentation\n",
    "        indent = \"  \" * depth\n",
    "        print(f\"{indent}Cluster {cluster_id} ({analysis['size']} papers)\")\n",
    "        \n",
    "        if analysis['keywords']:\n",
    "            print(f\"{indent}Keywords:\", \", \".join(k[0] for k in analysis['keywords'][:5]))\n",
    "        \n",
    "        # Get and sort children\n",
    "        cursor.execute('''\n",
    "            SELECT child_cluster_id, lambda_val\n",
    "            FROM cluster_trees\n",
    "            WHERE trial_id = %s AND parent_cluster_id = %s\n",
    "            ORDER BY child_size DESC\n",
    "        ''', (best_trial, cluster_id))\n",
    "        \n",
    "        children = cursor.fetchall()\n",
    "        \n",
    "        # Recursively print children\n",
    "        for child_id, lambda_val in children:\n",
    "            print_cluster(child_id, depth + 1, lambda_val)\n",
    "    \n",
    "    # Get root clusters\n",
    "    cursor.execute('''\n",
    "        SELECT DISTINCT parent_cluster_id\n",
    "        FROM cluster_trees\n",
    "        WHERE trial_id = %s\n",
    "          AND parent_cluster_id NOT IN (\n",
    "              SELECT child_cluster_id \n",
    "              FROM cluster_trees \n",
    "              WHERE trial_id = %s\n",
    "          )\n",
    "    ''', (best_trial, best_trial))\n",
    "    \n",
    "    root_clusters = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    # Print hierarchy starting from each root\n",
    "    for root_id in root_clusters:\n",
    "        print_cluster(root_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 2D embeddings exist\n",
    "perform_2d_umap()\n",
    "\n",
    "# Calculate and store validity metrics\n",
    "calculate_and_store_validity_metrics()\n",
    "\n",
    "# Basic cluster visualization\n",
    "plot_clusters()\n",
    "\n",
    "# Cluster metrics and statistics\n",
    "plot_cluster_metrics()\n",
    "\n",
    "# Hierarchical structure\n",
    "plot_cluster_dendrogram()\n",
    "plot_cluster_network()\n",
    "plot_hierarchical_clusters()\n",
    "print_cluster_hierarchy(max_depth=2)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
