{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive # pyright: ignore [reportMissingImports]\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Install and configure PostgreSQL\n",
    "    !sudo apt-get -qq update && sudo apt-get -qq install postgresql postgresql-contrib # pyright: ignore\n",
    "    !sudo service postgresql start # pyright: ignore\n",
    "    !sudo sed -i 's/local\\s*all\\s*postgres\\s*peer/local all postgres trust/' /etc/postgresql/14/main/pg_hba.conf # pyright: ignore\n",
    "    !sudo service postgresql restart # pyright: ignore\n",
    "    \n",
    "    %pip install joblib psycopg2-binary optuna hdbscan umap-learn numpy cupy-cuda12x # pyright: ignore\n",
    "    !git clone https://github.com/rapidsai/rapidsai-csp-utils.git # pyright: ignore\n",
    "    !python rapidsai-csp-utils/colab/pip-install.py # pyright: ignore\n",
    "\n",
    "# Core imports\n",
    "import sqlite3\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "# ML imports\n",
    "from cuml import UMAP\n",
    "from cuml.preprocessing import StandardScaler\n",
    "from cuml.cluster.hdbscan import HDBSCAN\n",
    "from cuml.metrics.trustworthiness import trustworthiness\n",
    "import cuml\n",
    "cuml.set_global_output_type('cupy')\n",
    "\n",
    "# Optimization imports\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "# Locale fix after install https://github.com/googlecolab/colabtools/issues/3409\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "# Add to Core imports\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "\n",
    "# Additional imports\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import time\n",
    "import asyncio\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "# Add after imports but before database setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Gemini API Key\n",
    "gemini_api_key = \"\" # @param {type:\"string\"}\n",
    "MODEL_ID = \"gemini-1.5-flash\"\n",
    "\n",
    "# Initialize Gemini client\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "# Add rate limiter from labeling.py\n",
    "class GeminiRateLimiter:\n",
    "    def __init__(self):\n",
    "        self.rpm_limit = 2000\n",
    "        self.tpm_limit = 4_000_000\n",
    "        self.requests = []\n",
    "        self.tokens = []\n",
    "        \n",
    "    def can_make_request(self, input_tokens, output_tokens):\n",
    "        current_time = time.time()\n",
    "        cutoff_time = current_time - 60\n",
    "        \n",
    "        recent_requests = [t for t in self.requests if t > cutoff_time]\n",
    "        if len(recent_requests) >= self.rpm_limit:\n",
    "            delay = 60 - (current_time - cutoff_time)\n",
    "            print(f\"RPM limit reached. Retrying after {delay:.1f}s\")\n",
    "            time.sleep(delay)\n",
    "            return False\n",
    "            \n",
    "        recent_tokens = sum(c for t, c in self.tokens if t > cutoff_time)\n",
    "        total_tokens = recent_tokens + input_tokens + output_tokens\n",
    "        if total_tokens > self.tpm_limit:\n",
    "            print(f\"TPM limit exceeded ({total_tokens}/{self.tpm_limit})\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def add_request(self, input_tokens, output_tokens):\n",
    "        current_time = time.time()\n",
    "        self.requests.append(current_time)\n",
    "        self.tokens.append((current_time, input_tokens + output_tokens))\n",
    "        time.sleep(0.05)\n",
    "\n",
    "rate_limiter = GeminiRateLimiter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive # pyright: ignore [reportMissingImports]\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Install and configure PostgreSQL\n",
    "    !sudo apt-get -qq update && sudo apt-get -qq install postgresql postgresql-contrib # pyright: ignore\n",
    "    !sudo service postgresql start # pyright: ignore\n",
    "    !sudo sed -i 's/local\\s*all\\s*postgres\\s*peer/local all postgres trust/' /etc/postgresql/14/main/pg_hba.conf # pyright: ignore\n",
    "    !sudo service postgresql restart # pyright: ignore\n",
    "    \n",
    "    %pip install joblib psycopg2-binary optuna hdbscan umap-learn numpy cupy-cuda12x # pyright: ignore\n",
    "    !git clone https://github.com/rapidsai/rapidsai-csp-utils.git # pyright: ignore\n",
    "    !python rapidsai-csp-utils/colab/pip-install.py # pyright: ignore\n",
    "\n",
    "# Core imports\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "# ML imports\n",
    "from cuml import UMAP\n",
    "from cuml.preprocessing import StandardScaler\n",
    "from cuml.cluster.hdbscan import HDBSCAN\n",
    "from cuml.metrics.trustworthiness import trustworthiness\n",
    "import cuml\n",
    "\n",
    "# Optimization imports\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "# Locale fix after install https://github.com/googlecolab/colabtools/issues/3409\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "# Add to Core imports\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "\n",
    "# Additional imports\n",
    "import pickle\n",
    "from itertools import islice\n",
    "import gc\n",
    "from joblib import parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_path = \"/content/drive/MyDrive/ai-safety-papers/filtered_compressed.db\"\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Create PostgreSQL connection with retries\"\"\"\n",
    "    import psycopg2\n",
    "    from psycopg2.extras import DictCursor\n",
    "    \n",
    "    return psycopg2.connect(\n",
    "        host='',  # Empty string for Unix socket connection\n",
    "        database=\"papers\",\n",
    "        user=\"postgres\",\n",
    "        cursor_factory=DictCursor\n",
    "    )\n",
    "\n",
    "# After creating connection but before creating tables:\n",
    "print(\"Loading existing database...\")\n",
    "!createdb -U postgres papers # pyright: ignore\n",
    "!pg_restore -U postgres --jobs=8 -d papers \"{backup_path}\" # pyright: ignore\n",
    "conn = get_db_connection()\n",
    "\n",
    "import json \n",
    "# Load best trial immediately after connection\n",
    "def get_best_trial():\n",
    "    \"\"\"Load best trial ID and metrics from JSON\"\"\"\n",
    "    drive_path = \"/content/drive/MyDrive/ai-safety-papers/best_trial.json\"\n",
    "    with open(drive_path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "best_trial_data = get_best_trial()\n",
    "best_trial = best_trial_data['trial_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_best_clusterer():\n",
    "    \"\"\"Reconstruct clusterer using stored embeddings and params from JSON\"\"\"\n",
    "    # Get parameters and embeddings\n",
    "    cluster_params = best_trial_data['params']\n",
    "    \n",
    "    # Load UMAP reduced embeddings from database\n",
    "    paper_ids = []\n",
    "    umap_embeddings = []\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            SELECT paper_id, umap_embedding\n",
    "            FROM artifacts\n",
    "            WHERE trial_id = %s AND umap_embedding IS NOT NULL\n",
    "            ORDER BY paper_id\n",
    "        ''', (best_trial,))\n",
    "        \n",
    "        for paper_id, emb_bytes in cursor:\n",
    "            paper_ids.append(paper_id)\n",
    "            # Convert bytea back to cupy array\n",
    "            umap_embeddings.append(cp.frombuffer(emb_bytes, dtype=cp.float32))\n",
    "    \n",
    "    # Create embeddings matrix\n",
    "    reduced_embeddings = cp.stack(umap_embeddings)\n",
    "    \n",
    "    # Reconstruct and fit clusterer with original parameters\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=cluster_params['min_cluster_size'],\n",
    "        min_samples=cluster_params['min_samples'],\n",
    "        cluster_selection_epsilon=cluster_params['cluster_selection_epsilon'],\n",
    "        cluster_selection_method='leaf',\n",
    "        gen_min_span_tree=True,\n",
    "        get_condensed_tree=True,\n",
    "        gen_single_linkage=True,\n",
    "        output_type='cupy'\n",
    "    ).fit(reduced_embeddings)\n",
    "    \n",
    "    # Create paper_id to index mapping\n",
    "    paper_id_to_idx = {pid: idx for idx, pid in enumerate(paper_ids)}\n",
    "    \n",
    "    return clusterer, paper_ids, reduced_embeddings, paper_id_to_idx\n",
    "\n",
    "# Load clusterer and results after defining best_trial\n",
    "best_clusterer, paper_ids, reduced_embeddings, paper_id_to_idx = get_best_clusterer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Leaf Cluster Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_members(cluster_id):\n",
    "    \"\"\"Retrieve papers using clusterer labels array\"\"\"\n",
    "    # Get indices where label matches cluster_id\n",
    "    mask = best_clusterer.labels_.get() == cluster_id\n",
    "    member_ids = [paper_ids[i] for i in np.where(mask)[0]]\n",
    "    \n",
    "    # Fetch details from database\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            SELECT id, title, abstract \n",
    "            FROM papers \n",
    "            WHERE id = ANY(%s)\n",
    "        ''', (member_ids,))\n",
    "        return cursor.fetchall()\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=1, max=60), stop=stop_after_attempt(6))\n",
    "async def generate_cluster_label_async(representatives, cluster_id):\n",
    "    \"\"\"Generate label and safety relevance score for a cluster\"\"\"\n",
    "    # Build paper list string\n",
    "    papers_str = \"\\n\\n\".join(\n",
    "        f\"Title: {p['title']}\\nAbstract: {p['abstract']}...\"  # Truncate long abstracts\n",
    "        for p in representatives[:10]  # Use first 10 as most representative\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert in AI safety and machine learning. Your task is to generate precise technical labels for clusters of academic papers related to AI research.\n",
    "\n",
    "I will provide the ten papers most representative of the cluster (closest to the cluster centroid).\n",
    "\n",
    "Review these papers and provide:\n",
    "1. A specific technical category that precisely describes the research area represented by this cluster\n",
    "2. A relevance score (0-1) indicating how relevant this research area is to AI safety\n",
    "\n",
    "Guidelines:\n",
    "- Use precise technical terminology\n",
    "- Categories should be specific enough to differentiate between related research areas yet broad enough to actually group papers (e.g. \"Reward Modeling for RLHF\" rather than \"Reinforcement Learning\" or \"Regularizing Hidden States Enables Learning Generalizable Reward Model for RLHF\")\n",
    "- Consider both direct and indirect relevance to AI safety\n",
    "\n",
    "Papers to analyze:\n",
    "{papers_str}\"\"\"\n",
    "    \n",
    "    # Define response schema\n",
    "    schema = {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"label\": {\"type\": \"STRING\"},\n",
    "            \"safety_relevance\": {\n",
    "                \"type\": \"NUMBER\",\n",
    "                \"minimum\": 0,\n",
    "                \"maximum\": 1\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"label\", \"safety_relevance\"]\n",
    "    }\n",
    "    \n",
    "    response = await client.aio.models.generate_content(\n",
    "        model=MODEL_ID,\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=schema\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update rate limiter with actual usage\n",
    "    input_tokens = response.usage_metadata.prompt_token_count\n",
    "    output_tokens = response.usage_metadata.candidates_token_count\n",
    "    rate_limiter.add_request(input_tokens, output_tokens)\n",
    "    \n",
    "    return json.loads(response.text)\n",
    "\n",
    "async def process_leaf_clusters_async():\n",
    "    \"\"\"Async version of leaf cluster processing\"\"\"\n",
    "    semaphore = asyncio.Semaphore(20)\n",
    "    labels = best_clusterer.labels_.get()\n",
    "    leaf_clusters = [cid for cid in np.unique(labels) if cid != -1]\n",
    "    \n",
    "    # Precompute centroids\n",
    "    centroids = get_cluster_centroids()\n",
    "    \n",
    "    async def process_cluster(cluster_id):\n",
    "        async with semaphore:\n",
    "            members = await asyncio.to_thread(get_cluster_members, cluster_id)\n",
    "            if len(members) < 5:\n",
    "                return\n",
    "                \n",
    "            # Get correct embeddings using the mapping\n",
    "            member_ids = [m['id'] for m in members]\n",
    "            indices = [paper_id_to_idx[pid] for pid in member_ids]\n",
    "            cluster_embeddings = reduced_embeddings[indices]\n",
    "            \n",
    "            # Find papers closest to centroid\n",
    "            centroid = centroids[cluster_id]\n",
    "            distances = cp.linalg.norm(cluster_embeddings - centroid, axis=1)\n",
    "            \n",
    "            # Sort members by distance\n",
    "            sorted_indices = cp.argsort(distances).get().tolist()\n",
    "            sorted_members = [members[i] for i in sorted_indices]\n",
    "            \n",
    "            # Select representatives\n",
    "            representatives = [\n",
    "                *sorted_members[:10]  # Top 10 closest to centroid\n",
    "            ]\n",
    "            \n",
    "            # Get label and safety relevance\n",
    "            label_data = await generate_cluster_label_async(representatives, cluster_id)\n",
    "            await asyncio.to_thread(update_cluster_label, cluster_id, label_data, representatives)\n",
    "\n",
    "    tasks = [process_cluster(cid) for cid in leaf_clusters]\n",
    "    for f in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Labeling clusters\"):\n",
    "        await f\n",
    "\n",
    "def update_cluster_label(cluster_id, label_data, representatives):\n",
    "    \"\"\"Store label and relevance score in database\"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        # Convert numpy types to native Python types\n",
    "        cluster_id = int(cluster_id)\n",
    "        safety_relevance = float(label_data['safety_relevance'])\n",
    "        \n",
    "        cursor.execute('''\n",
    "            INSERT INTO cluster_labels\n",
    "            (cluster_id, label, safety_relevance, representative_ids)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "            ON CONFLICT (cluster_id) DO UPDATE SET\n",
    "                label = EXCLUDED.label,\n",
    "                safety_relevance = EXCLUDED.safety_relevance,\n",
    "                representative_ids = EXCLUDED.representative_ids\n",
    "        ''', (\n",
    "            cluster_id,\n",
    "            label_data['label'],\n",
    "            safety_relevance,\n",
    "            [r['id'] for r in representatives]\n",
    "        ))\n",
    "        conn.commit()\n",
    "\n",
    "# Add this before process_leaf_clusters()\n",
    "def create_label_columns():\n",
    "    \"\"\"Create columns for cluster labels\"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS cluster_labels (\n",
    "                cluster_id INTEGER PRIMARY KEY,\n",
    "                label TEXT,\n",
    "                safety_relevance REAL,\n",
    "                representative_ids TEXT[],\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        ''')\n",
    "        conn.commit()\n",
    "\n",
    "def get_cluster_centroids():\n",
    "    \"\"\"Calculate centroids for each cluster using reduced embeddings\"\"\"\n",
    "    centroids = {}\n",
    "    labels = best_clusterer.labels_.get()\n",
    "    unique_clusters = np.unique(labels[labels != -1])\n",
    "    \n",
    "    for cid in unique_clusters:\n",
    "        cluster_mask = labels == cid\n",
    "        cluster_embeddings = reduced_embeddings[cluster_mask]\n",
    "        centroids[cid] = cluster_embeddings.mean(axis=0)\n",
    "    \n",
    "    return centroids\n",
    "\n",
    "create_label_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_leaf_clusters_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Test LLM Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_single_cluster_labeling():\n",
    "    \"\"\"Test label generation for a single cluster\"\"\"\n",
    "    try:\n",
    "        # Get first non-noise cluster\n",
    "        labels = best_clusterer.labels_.get()\n",
    "        valid_clusters = [cid for cid in np.unique(labels) if cid != -1]\n",
    "        if not valid_clusters:\n",
    "            print(\"No clusters available for testing\")\n",
    "            return\n",
    "            \n",
    "        test_cluster = valid_clusters[0]\n",
    "        print(f\"Testing label generation for cluster {test_cluster}\")\n",
    "        \n",
    "        # Get members\n",
    "        members = await asyncio.to_thread(get_cluster_members, test_cluster)\n",
    "        if len(members) < 5:\n",
    "            print(\"Cluster too small for testing\")\n",
    "            return\n",
    "            \n",
    "        # Get representatives\n",
    "        member_ids = [m['id'] for m in members]\n",
    "        indices = [paper_id_to_idx[pid] for pid in member_ids]\n",
    "        cluster_embeddings = reduced_embeddings[indices]\n",
    "        centroid = reduced_embeddings[best_clusterer.labels_.get() == test_cluster].mean(axis=0)\n",
    "        distances = cp.linalg.norm(cluster_embeddings - centroid, axis=1)\n",
    "        sorted_indices = cp.argsort(distances).get().tolist()\n",
    "        representatives = [members[i] for i in sorted_indices[:10]]\n",
    "        \n",
    "        # Generate label\n",
    "        print(\"\\nSample papers:\")\n",
    "        for p in representatives[:2]:  # Show first 2 for verification\n",
    "            print(f\"\\nTitle: {p['title']}\")\n",
    "            print(f\"Abstract: {p['abstract'][:200]}...\")\n",
    "            \n",
    "        print(\"\\nGenerated label:\")\n",
    "        label_data = await generate_cluster_label_async(representatives, test_cluster)\n",
    "        print(json.dumps(label_data, indent=2))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Test failed: {str(e)}\")\n",
    "\n",
    "# Run test before full processing\n",
    "await test_single_cluster_labeling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Cluster Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def get_condensed_tree():\n",
    "    \"\"\"Extract HDBSCAN condensed tree for hierarchy visualization\"\"\"\n",
    "    return best_clusterer.condensed_tree_\n",
    "\n",
    "def plot_hdbscan_dendrogram(condensed_tree, size=10):\n",
    "    \"\"\"Visualize HDBSCAN hierarchy using built-in condensed tree\"\"\"\n",
    "    plt.figure(figsize=(size, size))\n",
    "    condensed_tree.plot(select_clusters=True, label_clusters=True)\n",
    "    plt.title(\"HDBSCAN Condensed Tree Hierarchy\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_cluster_persistence(clusterer):\n",
    "    \"\"\"Plot cluster persistence metrics\"\"\"\n",
    "    # Get cluster persistence from database instead of clusterer\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            SELECT cluster_id, validity_index as persistence\n",
    "            FROM cluster_metrics\n",
    "            WHERE trial_id = %s\n",
    "        ''', (best_trial,))\n",
    "        persistence_data = cursor.fetchall()\n",
    "    \n",
    "    persistence_df = pd.DataFrame(\n",
    "        persistence_data,\n",
    "        columns=['cluster_id', 'persistence']\n",
    "    ).sort_values('persistence')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=persistence_df, x='cluster_id', y='persistence')\n",
    "    plt.xlabel('Cluster ID')\n",
    "    plt.ylabel('Persistence Score')\n",
    "    plt.title('Cluster Persistence Scores')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_cluster_scatter(figsize=(15, 15)):\n",
    "    \"\"\"Plot clusters using precomputed 2D UMAP embeddings\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT a.cluster_id, a.cluster_prob, a.viz_embedding, cl.label\n",
    "        FROM artifacts a\n",
    "        LEFT JOIN cluster_labels cl ON a.cluster_id = cl.cluster_id\n",
    "        WHERE a.trial_id = %s\n",
    "    ''', (best_trial,))\n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    # Convert database results to arrays\n",
    "    cluster_ids = np.array([r[0] for r in results])\n",
    "    probs = np.array([r[1] for r in results])\n",
    "    embeddings = np.vstack([np.frombuffer(r[2], dtype=np.float32) for r in results])\n",
    "    labels = [r[3] or f'Cluster {r[0]}' for r in results]  # Use label if available\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Plot noise points first\n",
    "    noise_mask = cluster_ids == -1\n",
    "    if np.any(noise_mask):\n",
    "        plt.scatter(\n",
    "            embeddings[noise_mask, 0],\n",
    "            embeddings[noise_mask, 1],\n",
    "            c='lightgray',\n",
    "            marker='.',\n",
    "            alpha=0.1,\n",
    "            label='Noise'\n",
    "        )\n",
    "    \n",
    "    # Get unique clusters with labels\n",
    "    unique_clusters = np.unique(cluster_ids[cluster_ids != -1])\n",
    "    cluster_labels = {cid: labels[i] for i, cid in enumerate(cluster_ids) if cid != -1}\n",
    "    \n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_clusters)))\n",
    "    \n",
    "    for i, cid in enumerate(unique_clusters):\n",
    "        mask = cluster_ids == cid\n",
    "        plt.scatter(\n",
    "            embeddings[mask, 0],\n",
    "            embeddings[mask, 1],\n",
    "            c=[colors[i]],\n",
    "            marker='.',\n",
    "            alpha=probs[mask],\n",
    "            label=cluster_labels[cid]\n",
    "        )\n",
    "    \n",
    "    plt.title('AI Safety Paper Clusters')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_validity_metrics():\n",
    "    \"\"\"Calculate HDBSCAN validity metrics for the best trial\"\"\"\n",
    "    import hdbscan\n",
    "    \n",
    "    # Get min_samples parameter from clusterer\n",
    "    min_samples = best_clusterer.min_samples\n",
    "    \n",
    "    # Convert cuML clusterer results to CPU numpy arrays with float64\n",
    "    labels = cp.asnumpy(best_clusterer.labels_).astype(int)\n",
    "    valid_mask = labels != -1\n",
    "    labels = labels[valid_mask]\n",
    "    embeddings = cp.asnumpy(reduced_embeddings[valid_mask]).astype(np.float64)\n",
    "\n",
    "    # Calculate validity without MST parameter\n",
    "    validity = hdbscan.validity.validity_index(\n",
    "        X=embeddings,\n",
    "        labels=labels,\n",
    "        metric='euclidean',\n",
    "        d=embeddings.shape[1],\n",
    "        per_cluster_scores=True\n",
    "    )\n",
    "\n",
    "    # Get density separation using validity tools\n",
    "    unique_labels = np.unique(labels)\n",
    "    density_separations = []\n",
    "    \n",
    "    for i in range(len(unique_labels)):\n",
    "        for j in range(i+1, len(unique_labels)):\n",
    "            c1, c2 = unique_labels[i], unique_labels[j]\n",
    "            \n",
    "            # Get cluster members\n",
    "            cluster1_mask = labels == c1\n",
    "            cluster2_mask = labels == c2\n",
    "            \n",
    "            # Compute core distances for each cluster\n",
    "            from sklearn.neighbors import NearestNeighbors\n",
    "            \n",
    "            # Cluster 1 core distances\n",
    "            cluster1_emb = embeddings[cluster1_mask]\n",
    "            nbrs1 = NearestNeighbors(n_neighbors=min_samples, metric='euclidean').fit(cluster1_emb)\n",
    "            distances1, _ = nbrs1.kneighbors(cluster1_emb)\n",
    "            core_distances1 = distances1[:, -1]  # Get min_samples-th neighbor distance\n",
    "            \n",
    "            # Cluster 2 core distances\n",
    "            cluster2_emb = embeddings[cluster2_mask]\n",
    "            nbrs2 = NearestNeighbors(n_neighbors=min_samples, metric='euclidean').fit(cluster2_emb)\n",
    "            distances2, _ = nbrs2.kneighbors(cluster2_emb)\n",
    "            core_distances2 = distances2[:, -1]\n",
    "\n",
    "            # Compute internal nodes via validity module\n",
    "            distances = hdbscan.validity.distances_between_points(\n",
    "                X=cluster1_emb,\n",
    "                labels=labels[cluster1_mask],\n",
    "                cluster_id=c1\n",
    "            )[0].astype(np.float64)\n",
    "            \n",
    "            internal_nodes1 = hdbscan.validity.internal_minimum_spanning_tree(distances)[0]\n",
    "            \n",
    "            distances = hdbscan.validity.distances_between_points(\n",
    "                X=cluster2_emb,\n",
    "                labels=labels[cluster2_mask],\n",
    "                cluster_id=c2\n",
    "            )[0].astype(np.float64)\n",
    "            \n",
    "            internal_nodes2 = hdbscan.validity.internal_minimum_spanning_tree(distances)[0]\n",
    "\n",
    "            if len(internal_nodes1) == 0 or len(internal_nodes2) == 0:\n",
    "                continue  # Skip clusters without internal nodes\n",
    "                \n",
    "            sep = hdbscan.validity.density_separation(\n",
    "                X=embeddings,\n",
    "                labels=labels,\n",
    "                cluster_id1=c1,\n",
    "                cluster_id2=c2,\n",
    "                internal_nodes1=internal_nodes1,\n",
    "                internal_nodes2=internal_nodes2,\n",
    "                core_distances1=core_distances1,\n",
    "                core_distances2=core_distances2\n",
    "            )\n",
    "            density_separations.append(sep)\n",
    "\n",
    "    # Store metrics\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS cluster_metrics (\n",
    "                trial_id INTEGER NOT NULL,\n",
    "                cluster_id INTEGER NOT NULL,\n",
    "                validity_index REAL,\n",
    "                density_separation REAL,\n",
    "                PRIMARY KEY (trial_id, cluster_id)\n",
    "            )\n",
    "        ''')\n",
    "        for cluster_id, validity_score in validity[1].items():\n",
    "            cursor.execute('''\n",
    "                INSERT INTO cluster_metrics \n",
    "                (trial_id, cluster_id, validity_index)\n",
    "                VALUES (%s, %s, %s)\n",
    "                ON CONFLICT (trial_id, cluster_id) DO UPDATE SET\n",
    "                    validity_index = EXCLUDED.validity_index\n",
    "            ''', (best_trial, cluster_id, validity_score))\n",
    "        conn.commit()\n",
    "    \n",
    "    return {\n",
    "        'overall_validity': validity[0],\n",
    "        'mean_density_separation': np.mean(density_separations),\n",
    "        'min_density_separation': np.min(density_separations)\n",
    "    }\n",
    "\n",
    "def calculate_and_store_validity_metrics():\n",
    "    \"\"\"Calculate and store validity metrics if missing\"\"\"\n",
    "    validity_metrics = None\n",
    "    with conn.cursor() as cursor:\n",
    "        # Check if metrics exist in database\n",
    "        cursor.execute('''\n",
    "            SELECT EXISTS (\n",
    "                SELECT 1 FROM information_schema.tables \n",
    "                WHERE table_name = 'cluster_metrics'\n",
    "            )\n",
    "        ''', (best_trial,))\n",
    "        exists = cursor.fetchone()[0]\n",
    "        \n",
    "        if not exists:\n",
    "            # Calculate metrics if missing\n",
    "            validity_metrics = calculate_validity_metrics()\n",
    "        else:\n",
    "            # Check if we have metrics for this trial\n",
    "            cursor.execute('''\n",
    "                SELECT COUNT(*) FROM cluster_metrics\n",
    "                WHERE trial_id = %s\n",
    "            ''', (best_trial,))\n",
    "            if cursor.fetchone()[0] == 0:\n",
    "                validity_metrics = calculate_validity_metrics()\n",
    "            else:\n",
    "                print(\"Validity metrics already calculated\")\n",
    "    return validity_metrics\n",
    "\n",
    "# Generate visualizations\n",
    "# First ensure metrics exist\n",
    "validity_metrics = calculate_and_store_validity_metrics()\n",
    "condensed_tree = get_condensed_tree()\n",
    "plot_hdbscan_dendrogram(condensed_tree)\n",
    "plot_cluster_persistence(best_clusterer)\n",
    "plot_cluster_scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup_database():\n",
    "    \"\"\"Backup PostgreSQL database to Google Drive\"\"\"\n",
    "    backup_path = \"/content/drive/MyDrive/ai-safety-papers/papers.sql\"\n",
    "    print(f\"Creating PostgreSQL backup at {backup_path}\")\n",
    "    !pg_dump -U postgres -F c -f \"{backup_path}\" papers  # pyright: ignore\n",
    "    print(\"Backup completed successfully\")\n",
    "backup_database()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
