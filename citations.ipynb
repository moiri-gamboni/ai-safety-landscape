{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Papers - Citation Tracking\n",
    "\n",
    "This notebook fetches citation counts from OpenCitations for embedded papers in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive # pyright: ignore [reportMissingImports]\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    !sudo apt-get -qq update && sudo apt-get -qq install postgresql postgresql-contrib # pyright: ignore\n",
    "    !sudo service postgresql start # pyright: ignore\n",
    "    !sudo sed -i 's/local\\s*all\\s*postgres\\s*peer/local all postgres trust/' /etc/postgresql/14/main/pg_hba.conf # pyright: ignore\n",
    "    !sudo service postgresql restart # pyright: ignore\n",
    "    \n",
    "    %pip install psycopg2-binary requests tenacity tqdm # pyright: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Load Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Create PostgreSQL connection\"\"\"\n",
    "    return psycopg2.connect(\n",
    "        host='',\n",
    "        database=\"postgres\",\n",
    "        user=\"postgres\"\n",
    "    )\n",
    "\n",
    "def load_database():\n",
    "    \"\"\"Load PostgreSQL backup using psql\"\"\"\n",
    "    backup_path = \"/content/drive/MyDrive/ai-safety-papers/papers_postgres.sql\"\n",
    "    print(\"Loading PostgreSQL backup...\")\n",
    "    !psql -U postgres -d postgres -f \"{backup_path}\" # pyright: ignore\n",
    "\n",
    "load_database()\n",
    "conn = get_db_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Setup Citation Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_citation_column():\n",
    "    with conn.cursor() as cursor:\n",
    "        # Create column fresh with NULL default\n",
    "        cursor.execute('''\n",
    "            ALTER TABLE papers \n",
    "            ADD COLUMN IF NOT EXISTS citation_count INTEGER DEFAULT NULL\n",
    "        ''')\n",
    "        conn.commit()\n",
    "\n",
    "setup_citation_column()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Async Citation Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load OpenCitations API key\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # @title OpenCitations API Key\n",
    "    oc_token = \"\" # @param {type:\"string\"}\n",
    "else:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    oc_token = os.getenv('OPENCITATIONS_ACCESS_TOKEN')\n",
    "\n",
    "API_HEADERS = {\"authorization\": oc_token} if oc_token else {}\n",
    "BASE_URL = \"https://opencitations.net/index/api/v2/citation-count/doi:\"\n",
    "\n",
    "def arxiv_id_to_doi(arxiv_id: str) -> str:\n",
    "    \"\"\"Convert arXiv ID to DataCite DOI format\"\"\"\n",
    "    return f\"10.48550/arXiv.{arxiv_id}\"\n",
    "\n",
    "# Configure async parameters\n",
    "CONCURRENCY_LIMIT = 8  # OpenCitations recommends 10 req/s\n",
    "BATCH_SIZE = 1000  # Papers per progress update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. Async Citation Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "async def fetch_all_citations():\n",
    "    \"\"\"Top-level async function for notebook execution\"\"\"\n",
    "    async with aiohttp.ClientSession(\n",
    "        headers=API_HEADERS,\n",
    "        connector=aiohttp.TCPConnector(limit=CONCURRENCY_LIMIT),\n",
    "        timeout=aiohttp.ClientTimeout(total=30)\n",
    "    ) as session:\n",
    "        # Get all paper IDs needing processing\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute('''\n",
    "                SELECT id FROM papers \n",
    "                WHERE embedding IS NOT NULL\n",
    "                  AND withdrawn = FALSE\n",
    "                  AND citation_count IS NULL\n",
    "            ''')\n",
    "            paper_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "        # Process with progress tracking\n",
    "        with tqdm(total=len(paper_ids), desc=\"Fetching citations\") as pbar:\n",
    "            semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
    "            \n",
    "            async def process_with_semaphore(paper_id):\n",
    "                async with semaphore:\n",
    "                    await process_paper(session, paper_id)\n",
    "                    pbar.update(1)\n",
    "            \n",
    "            # Batch processing for memory management\n",
    "            for i in range(0, len(paper_ids), BATCH_SIZE):\n",
    "                batch = paper_ids[i:i+BATCH_SIZE]\n",
    "                await asyncio.gather(*[process_with_semaphore(pid) for pid in batch])\n",
    "                del batch  # Explicit memory cleanup\n",
    "                await asyncio.sleep(1)  # Rate limit between batches\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, max=10))\n",
    "async def fetch_citation_count(session, doi: str) -> int:\n",
    "    \"\"\"Async fetch with retries and proper error handling\"\"\"\n",
    "    try:\n",
    "        async with session.get(f\"{BASE_URL}{doi}\") as response:\n",
    "            response.raise_for_status()\n",
    "            data = await response.json()\n",
    "            return int(data[0]['count'])\n",
    "    except (aiohttp.ClientError, asyncio.TimeoutError) as e:\n",
    "        print(f\"Network error for {doi}: {str(e)}\")\n",
    "        raise\n",
    "    except (IndexError, KeyError, ValueError) as e:\n",
    "        print(f\"Invalid response for {doi}: {str(e)}\")\n",
    "        return 0  # Treat as valid zero-citation response\n",
    "\n",
    "async def process_paper(session, paper_id: str):\n",
    "    \"\"\"Process single paper including DB update\"\"\"\n",
    "    doi = arxiv_id_to_doi(paper_id)\n",
    "    try:\n",
    "        count = await fetch_citation_count(session, doi)\n",
    "        # Run DB update in thread pool\n",
    "        await asyncio.to_thread(\n",
    "            update_citation_in_db,\n",
    "            paper_id,\n",
    "            count\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed processing {paper_id}: {str(e)}\")\n",
    "\n",
    "def update_citation_in_db(paper_id: str, count: int):\n",
    "    \"\"\"Synchronous DB update function\"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            UPDATE papers \n",
    "            SET citation_count = %s \n",
    "            WHERE id = %s\n",
    "        ''', (count, paper_id))\n",
    "        conn.commit()\n",
    "\n",
    "await fetch_all_citations() # pyright: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def validate_citations():\n",
    "    \"\"\"Validate citation data quality\"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        # Track processing status\n",
    "        cursor.execute('''\n",
    "            SELECT \n",
    "                COUNT(*) FILTER (WHERE citation_count IS NOT NULL) AS processed,\n",
    "                COUNT(*) FILTER (WHERE citation_count IS NULL) AS unprocessed\n",
    "            FROM papers \n",
    "            WHERE embedding IS NOT NULL\n",
    "              AND withdrawn = FALSE\n",
    "        ''')\n",
    "        processed, unprocessed = cursor.fetchone()\n",
    "        \n",
    "        print(f\"\\nProcessing Status:\")\n",
    "        print(f\"• Processed papers: {processed}\")\n",
    "        print(f\"• Remaining unprocessed: {unprocessed}\")\n",
    "\n",
    "        # Only show stats for processed papers\n",
    "        if processed > 0:\n",
    "            cursor.execute('''\n",
    "                SELECT \n",
    "                    AVG(citation_count) AS mean,\n",
    "                    STDDEV(citation_count) AS stddev,\n",
    "                    MIN(citation_count) AS min,\n",
    "                    MAX(citation_count) AS max\n",
    "                FROM papers \n",
    "                WHERE embedding IS NOT NULL\n",
    "                  AND withdrawn = FALSE\n",
    "                  AND citation_count IS NOT NULL\n",
    "            ''')\n",
    "            stats = cursor.fetchone()\n",
    "            \n",
    "            # Papers with citations\n",
    "            cursor.execute('''\n",
    "                SELECT COUNT(*) \n",
    "                FROM papers \n",
    "                WHERE citation_count > 0 \n",
    "                  AND embedding IS NOT NULL\n",
    "                  AND withdrawn = FALSE\n",
    "                  AND citation_count IS NOT NULL\n",
    "            ''')\n",
    "            non_zero = cursor.fetchone()[0]\n",
    "\n",
    "            print(f\"\\nCitation Statistics for {processed} processed papers:\")\n",
    "            print(f\"• Average: {stats[0]:.1f} ± {stats[1]:.1f}\")\n",
    "            print(f\"• Range: {stats[2]} - {stats[3]}\")\n",
    "            print(f\"• Papers with citations: {non_zero} ({non_zero/processed:.1%})\")\n",
    "\n",
    "validate_citations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup_citations():\n",
    "    \"\"\"Use pg_dump for PostgreSQL backups\"\"\"\n",
    "    backup_path = \"/content/drive/MyDrive/ai-safety-papers/papers_postgres.sql\"\n",
    "    !pg_dump -U postgres -F p -f \"{backup_path}\" postgres # pyright: ignore\n",
    "\n",
    "# Call backup after processing\n",
    "backup_citations()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
