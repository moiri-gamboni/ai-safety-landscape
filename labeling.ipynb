{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Paper Labeling Pipeline\n",
    "Stage 1: Individual Paper Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive # pyright: ignore [reportMissingImports]\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    !sudo apt-get -qq update && sudo apt-get -qq install postgresql postgresql-contrib # pyright: ignore\n",
    "    !sudo service postgresql start # pyright: ignore\n",
    "    !sudo sed -i 's/local\\s*all\\s*postgres\\s*peer/local all postgres trust/' /etc/postgresql/14/main/pg_hba.conf # pyright: ignore\n",
    "    !sudo service postgresql restart # pyright: ignore\n",
    "    \n",
    "    %pip install psycopg2-binary tqdm tenacity google-genai pydantic # pyright: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Load Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "VALID_CATEGORIES = [\n",
    "    'cs.AI', 'cs.LG', 'cs.GT', 'cs.MA',\n",
    "    'cs.LO', 'cs.CY', 'cs.CR', 'cs.SE', 'cs.NE'\n",
    "]\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Create PostgreSQL connection\"\"\"\n",
    "    return psycopg2.connect(\n",
    "        host='',\n",
    "        database=\"papers\",\n",
    "        user=\"postgres\"\n",
    "    )\n",
    "\n",
    "def get_valid_categories():\n",
    "    \"\"\"Return as PostgreSQL array literal\"\"\"\n",
    "    return \"'{\" + \",\".join(VALID_CATEGORIES) + \"}'\"\n",
    "\n",
    "def cleanup_database():\n",
    "    \"\"\"Permanently remove non-target papers and analysis tables\"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('SET maintenance_work_mem TO \\'5GB\\';')\n",
    "    conn.commit()\n",
    "\n",
    "    # Remove analysis tables\n",
    "    with conn.cursor() as cursor:\n",
    "        print(\"Removing old tables...\")\n",
    "        cursor.execute('DROP TABLE IF EXISTS alembic_version, artifacts, cluster_trees, studies, study_directions, study_system_attributes, study_user_attributes, trial_heartbeats, trial_intermediate_values, trial_params, trial_system_attributes, trial_user_attributes, trial_values, trials, version_info CASCADE')\n",
    "        cursor.execute('DROP INDEX IF EXISTS idx_categories, idx_embedding_not_null, idx_updated, ix_studies_study_name, ix_trials_study_id')\n",
    "    conn.commit()\n",
    "\n",
    "    # Schema modifications\n",
    "    with conn.cursor() as cursor:        \n",
    "        print(\"Optimizing category filtering...\")\n",
    "        cursor.execute(f'''\n",
    "            ALTER TABLE papers \n",
    "            ADD COLUMN IF NOT EXISTS arxiv_categories TEXT[]\n",
    "            GENERATED ALWAYS AS (\n",
    "                string_to_array(categories, ' ')\n",
    "            ) STORED\n",
    "        ''')\n",
    "    conn.commit()\n",
    "\n",
    "    with conn.cursor() as cursor:        \n",
    "        print(\"Creating GIN index...\")\n",
    "        cursor.execute('''\n",
    "            CREATE INDEX IF NOT EXISTS idx_paper_categories_arr \n",
    "            ON papers USING GIN(arxiv_categories)\n",
    "        ''')\n",
    "    conn.commit()\n",
    "\n",
    "    with conn.cursor() as cursor:        \n",
    "        print(\"Creating title index...\")\n",
    "        cursor.execute('''\n",
    "            CREATE INDEX IF NOT EXISTS idx_title \n",
    "            ON papers USING GIN(to_tsvector('english', title))\n",
    "        ''')\n",
    "    conn.commit()\n",
    "\n",
    "    # Delete related records\n",
    "    with conn.cursor() as cursor:\n",
    "        print(\"Deleting paper_versions records...\")\n",
    "        cursor.execute(f'''\n",
    "            DELETE FROM paper_versions\n",
    "            WHERE paper_id IN (\n",
    "                SELECT id \n",
    "                FROM papers \n",
    "                WHERE (NOT arxiv_categories && {get_valid_categories()})\n",
    "                   OR withdrawn = TRUE\n",
    "            )\n",
    "        ''')\n",
    "    conn.commit()\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        print(\"Deleting paper_authors records...\")\n",
    "        cursor.execute(f'''\n",
    "            DELETE FROM paper_authors\n",
    "            WHERE paper_id IN (\n",
    "                SELECT id FROM papers \n",
    "                WHERE (NOT arxiv_categories && {get_valid_categories()})\n",
    "                   OR withdrawn = TRUE\n",
    "            )\n",
    "        ''')\n",
    "    conn.commit()\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        print(\"Deleting non-target papers...\")\n",
    "        cursor.execute(f'''\n",
    "            DELETE FROM papers\n",
    "            WHERE (NOT arxiv_categories && {get_valid_categories()})\n",
    "               OR withdrawn = TRUE\n",
    "        ''')\n",
    "    conn.commit()\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        print(\"Dropping original categories column...\")\n",
    "        cursor.execute('''\n",
    "            ALTER TABLE papers \n",
    "            DROP COLUMN IF EXISTS categories\n",
    "        ''')\n",
    "    conn.commit()\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        print(\"Deleting orphaned authors...\")\n",
    "        cursor.execute('CREATE INDEX tmp_author_idx ON paper_authors (author_id)')\n",
    "        cursor.execute('''\n",
    "            DELETE FROM authors\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT 1 FROM paper_authors \n",
    "                WHERE author_id = authors.id\n",
    "            )\n",
    "        ''')\n",
    "        cursor.execute('DROP INDEX tmp_author_idx')\n",
    "        conn.commit()\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        print(\"Optimizing author query index...\")\n",
    "        cursor.execute('''\n",
    "            CREATE INDEX idx_author_papers \n",
    "            ON paper_authors (author_id) INCLUDE (paper_id)\n",
    "        ''')\n",
    "    conn.commit()\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        print(\"Vacuuming database...\")\n",
    "        # Allow VACUUM outside transaction block\n",
    "        conn.autocommit = True\n",
    "        cursor.execute('VACUUM FULL ANALYZE')\n",
    "        conn.autocommit = False\n",
    "    \n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('RESET maintenance_work_mem;')\n",
    "    conn.commit()\n",
    "    \n",
    "    print(\"Cleanup complete\")\n",
    "\n",
    "def load_database():\n",
    "    \"\"\"Load and filter PostgreSQL backup\"\"\"\n",
    "    backup_path = \"/content/drive/MyDrive/ai-safety-papers/papers.sql\"\n",
    "    print(\"Loading PostgreSQL backup...\")\n",
    "    !createdb -U postgres papers # pyright: ignore\n",
    "    !pg_restore -U postgres --jobs=8 -d papers \"{backup_path}\" # pyright: ignore    \n",
    "\n",
    "load_database()\n",
    "conn = get_db_connection()\n",
    "cleanup_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from psycopg2.extras import DictCursor\n",
    "import time\n",
    "from google.genai import types\n",
    "import asyncio\n",
    "\n",
    "# Configure Gemini API\n",
    "# @title Gemini API Key\n",
    "gemini_api_key = \"\" # @param {type:\"string\"}\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "MODEL_ID = \"gemini-2.0-flash-exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_label_columns():\n",
    "    \"\"\"Create columns for labeling results matching db_schema.md\"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('ALTER TABLE papers ADD COLUMN IF NOT EXISTS llm_category TEXT')\n",
    "        conn.commit()\n",
    "\n",
    "# Create columns before processing\n",
    "create_label_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_batches(batch_size=400):\n",
    "    \"\"\"Generator yielding batches of papers needing labeling\"\"\"\n",
    "    cursor = conn.cursor(cursor_factory=DictCursor)\n",
    "    \n",
    "    # Set random seed FIRST\n",
    "    cursor.execute('SELECT setseed(0.42);')  # Fixed seed\n",
    "    \n",
    "    cursor.execute('''\n",
    "        SELECT COUNT(*) \n",
    "        FROM papers \n",
    "        WHERE llm_category IS NULL\n",
    "    ''')\n",
    "    \n",
    "    total_papers = cursor.fetchone()[0]\n",
    "    \n",
    "    # Random order query\n",
    "    cursor.execute('''\n",
    "        SELECT id, title, abstract \n",
    "        FROM papers\n",
    "        WHERE llm_category IS NULL\n",
    "        ORDER BY random()\n",
    "    ''')\n",
    "    \n",
    "    batch = []\n",
    "    with tqdm(total=total_papers, desc=\"Processing papers\", unit=\" papers\") as pbar:\n",
    "        for row in cursor:\n",
    "            batch.append(row)\n",
    "            if len(batch) >= batch_size:\n",
    "                pbar.update(len(batch))\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if batch:\n",
    "            pbar.update(len(batch))\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Rate limiter class matching Gemini 2.0 Flash limits\n",
    "class GeminiRateLimiter:\n",
    "    def __init__(self):\n",
    "        self.rpm_limit = 10  # Requests per minute\n",
    "        self.tpm_limit = 4_000_000  # Tokens per minute\n",
    "        self.rpd_limit = 1_500  # Requests per day\n",
    "        self.requests = []\n",
    "        self.tokens = []\n",
    "        self.daily_count = 0\n",
    "        \n",
    "    def can_make_request(self, input_tokens, output_tokens):\n",
    "        current_time = time.time()\n",
    "        cutoff_time = current_time - 60\n",
    "        \n",
    "        # Check daily limit\n",
    "        if self.daily_count >= self.rpd_limit:\n",
    "            print(\"Daily request limit reached\")\n",
    "            return False\n",
    "            \n",
    "        # Check RPM\n",
    "        recent_requests = [t for t in self.requests if t > cutoff_time]\n",
    "        if len(recent_requests) >= self.rpm_limit:\n",
    "            print(f\"RPM limit reached ({len(recent_requests)}/{self.rpm_limit})\")\n",
    "            return False\n",
    "            \n",
    "        # Check TPM\n",
    "        recent_tokens = sum(c for t, c in self.tokens if t > cutoff_time)\n",
    "        total_tokens = recent_tokens + input_tokens + output_tokens\n",
    "        if total_tokens > self.tpm_limit:\n",
    "            print(f\"TPM limit exceeded ({total_tokens}/{self.tpm_limit})\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def add_request(self, input_tokens, output_tokens):\n",
    "        current_time = time.time()\n",
    "        self.requests.append(current_time)\n",
    "        self.tokens.append((current_time, input_tokens + output_tokens))\n",
    "        self.daily_count += 1\n",
    "\n",
    "# Initialize rate limiter\n",
    "rate_limiter = GeminiRateLimiter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        self.total_input = 0\n",
    "        self.total_output = 0\n",
    "        self.batches_processed = 0\n",
    "        \n",
    "    def add_batch(self, input_tokens, output_tokens):\n",
    "        self.total_input += input_tokens\n",
    "        self.total_output += output_tokens\n",
    "        self.batches_processed += 1\n",
    "        \n",
    "    def print_summary(self):\n",
    "        print(\"\\n=== Token Usage Summary ===\")\n",
    "        print(f\"Total batches processed: {self.batches_processed}\")\n",
    "        print(f\"Total input tokens: {self.total_input}\")\n",
    "        print(f\"Total output tokens: {self.total_output}\")\n",
    "        print(f\"Total tokens used: {self.total_input + self.total_output}\")\n",
    "\n",
    "token_tracker = TokenTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(multiplier=1, max=60), stop=stop_after_attempt(6))\n",
    "async def generate_labels(batch):\n",
    "    \"\"\"Generate labels for a batch of papers using Gemini\"\"\"\n",
    "    base_prompt = \"\"\"You are an expert in AI and machine learning. Your task is to categorize academic papers.\n",
    "For each paper, provide a specific technical category using precise technical terminology that describes the primary research focus.\n",
    "Categories should be specific enough to differentiate between similar papers yet broad enough to actually group papers (e.g. \"Reward Modeling for RLHF\" rather than \"Reinforcement Learning\" or \"Regularizing Hidden States Enables Learning Generalizable Reward Model for RLHF\")\n",
    "\n",
    "Papers to analyze:\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = base_prompt\n",
    "    batch_size = len(batch)\n",
    "    for paper in batch:\n",
    "        prompt += f\"\\n\\nTitle: {paper['title']}\\nAbstract: {paper['abstract']}\"\n",
    "    # Build schema as dictionary\n",
    "    schema = {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"STRING\",\n",
    "        },\n",
    "        \"minItems\": batch_size,\n",
    "        \"maxItems\": batch_size\n",
    "    }\n",
    "    \n",
    "    response = await client.aio.models.generate_content(\n",
    "        model=MODEL_ID,\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=schema\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if not response.usage_metadata:\n",
    "        raise ValueError(\"Missing usage metadata in response\")\n",
    "    \n",
    "    input_tokens = response.usage_metadata.prompt_token_count\n",
    "    output_tokens = response.usage_metadata.candidates_token_count\n",
    "    \n",
    "    # Update rate limiter and tracker\n",
    "    rate_limiter.add_request(input_tokens, output_tokens)\n",
    "    token_tracker.add_batch(input_tokens, output_tokens)\n",
    "    \n",
    "    # Add truncation check\n",
    "    if output_tokens >= 8192:\n",
    "        print(f\"Warning: Output tokens at limit ({output_tokens}), response may be truncated\")\n",
    "\n",
    "    print(response.text)\n",
    "    \n",
    "    return json.loads(response.text), input_tokens, output_tokens\n",
    "\n",
    "def test_token_count():\n",
    "    \"\"\"Test token counting with real data batch\"\"\"\n",
    "    try:\n",
    "        batch = next(get_paper_batches(batch_size=400))\n",
    "    except StopIteration:\n",
    "        print(\"No unprocessed papers available for testing\")\n",
    "        return\n",
    "\n",
    "    # Use generate_labels to test full flow\n",
    "    labels, input_toks, output_toks = generate_labels(batch)\n",
    "    \n",
    "    print(f\"\\nTest batch results:\")\n",
    "    print(f\"Input tokens: {input_toks}\")\n",
    "    print(f\"Output tokens: {output_toks}\")\n",
    "    print(f\"Total tokens: {input_toks + output_toks}\")\n",
    "    \n",
    "    # Check against limits\n",
    "    if output_toks >= 8192:\n",
    "        print(\"\\n⚠️ WARNING: Exceeded output token limit! Response may be truncated\")\n",
    "    else:\n",
    "        print(f\"\\n✅ Output tokens within limit (8192)\")\n",
    "\n",
    "    return input_toks, output_toks\n",
    "\n",
    "# Run updated test\n",
    "test_token_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Async Labeling Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "async def process_batches_async():\n",
    "    \"\"\"Main async processing loop\"\"\"\n",
    "    try:\n",
    "        semaphore = asyncio.Semaphore(10)\n",
    "        batches = list(get_paper_batches())\n",
    "        \n",
    "        async def process_batch(batch):\n",
    "            async with semaphore:\n",
    "                try:\n",
    "                    labels, input_toks, output_toks = await generate_labels(batch)\n",
    "                    \n",
    "                    # Handle label-batch size mismatch\n",
    "                    if len(labels) != len(batch):\n",
    "                        print(f\"Label mismatch: {len(labels)} vs {len(batch)}\")\n",
    "                        if len(labels) > len(batch):\n",
    "                            labels = labels[:len(batch)]\n",
    "                        else:\n",
    "                            batch = batch[:len(labels)]\n",
    "                        \n",
    "                        print(f\"Adjusted to {len(labels)} items\")\n",
    "                    \n",
    "                    # Batch update\n",
    "                    await asyncio.to_thread(update_batch_in_db, batch, labels)\n",
    "                    print(f\"Processed {len(batch)} papers | Tokens: {input_toks}→{output_toks}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Batch error: {str(e)}\")\n",
    "                    await asyncio.to_thread(conn.rollback)\n",
    "        \n",
    "        # Process all batches concurrently\n",
    "        await asyncio.gather(*[process_batch(b) for b in batches])\n",
    "        await asyncio.to_thread(token_tracker.print_summary)\n",
    "    finally:\n",
    "        await client.aio.close()  # Cleanup async resources\n",
    "\n",
    "# Update database function\n",
    "def update_batch_in_db(batch, labels):\n",
    "    with conn.cursor() as cursor:\n",
    "        for paper, label in zip(batch, labels):\n",
    "            cursor.execute('''\n",
    "                UPDATE papers\n",
    "                SET llm_category = %s\n",
    "                WHERE id = %s\n",
    "            ''', (label, paper['id']))\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 6. Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Run async pipeline (Jupyter compatible)\n",
    "await process_batches_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 7. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_labels():\n",
    "    \"\"\"Validate labeling results\"\"\"\n",
    "    cursor = conn.cursor(cursor_factory=DictCursor)\n",
    "    \n",
    "    print(\"\\n=== Labeling Quality Checks ===\")\n",
    "    \n",
    "    # Check coverage of labeling process\n",
    "    cursor.execute('''\n",
    "        SELECT \n",
    "            COUNT(*) AS total,\n",
    "            SUM(CASE WHEN llm_category IS NOT NULL THEN 1 ELSE 0 END) AS labeled,\n",
    "            SUM(CASE WHEN llm_category IS NULL THEN 1 ELSE 0 END) AS unlabeled\n",
    "        FROM papers\n",
    "    ''')\n",
    "    \n",
    "    stats = cursor.fetchone()\n",
    "    print(f\"Label coverage: {stats['labeled']}/{stats['total']} ({stats['labeled']/stats['total']*100:.1f}%)\")\n",
    "\n",
    "validate_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Save duplicates to Drive\n",
    "def backup_database():\n",
    "    \"\"\"Backup entire (now filtered) database\"\"\"\n",
    "    backup_path = \"/content/drive/MyDrive/ai-safety-papers/papers.sql\"\n",
    "    !pg_dump -U postgres -F c -f \"{backup_path}\" papers # pyright: ignore\n",
    "    print(f\"Full filtered backup saved to {backup_path}\")\n",
    "\n",
    "# Call backup after processing\n",
    "backup_database()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
