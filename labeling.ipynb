{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Paper Labeling Pipeline\n",
    "Stage 1: Individual Paper Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive # pyright: ignore [reportMissingImports]\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    !sudo apt-get -qq update && sudo apt-get -qq install postgresql postgresql-contrib # pyright: ignore\n",
    "    !sudo service postgresql start # pyright: ignore\n",
    "    !sudo sed -i 's/local\\s*all\\s*postgres\\s*peer/local all postgres trust/' /etc/postgresql/14/main/pg_hba.conf # pyright: ignore\n",
    "    !sudo service postgresql restart # pyright: ignore\n",
    "    \n",
    "    %pip install psycopg2-binary tqdm tenacity google-genai # pyright: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Load Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Create PostgreSQL connection\"\"\"\n",
    "    return psycopg2.connect(\n",
    "        host='',\n",
    "        database=\"papers\",\n",
    "        user=\"postgres\"\n",
    "    )\n",
    "\n",
    "def load_database():\n",
    "    \"\"\"Load PostgreSQL backup using psql\"\"\"\n",
    "    backup_path = \"/content/drive/MyDrive/ai-safety-papers/papers_postgres.sql\"\n",
    "    print(\"Loading PostgreSQL backup...\")\n",
    "    !psql -U postgres -d papers -f \"{backup_path}\" # pyright: ignore\n",
    "\n",
    "load_database()\n",
    "conn = get_db_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from psycopg2.extras import DictCursor\n",
    "from typing import List, TypedDict\n",
    "import time\n",
    "from google.genai import types\n",
    "\n",
    "# Configure Gemini API\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # @title Gemini API Key\n",
    "    gemini_api_key = \"\" # @param {type:\"string\"}\n",
    "    client = genai.Client(api_key=gemini_api_key)\n",
    "else:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    client = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "\n",
    "MODEL_ID = \"gemini-2.0-flash-exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "VALID_CATEGORIES = [\n",
    "    'cs.AI', 'cs.LG', 'cs.GT', 'cs.MA',\n",
    "    'cs.LO', 'cs.CY', 'cs.CR', 'cs.SE', 'cs.NE'\n",
    "]\n",
    "\n",
    "def get_category_regex():\n",
    "    \"\"\"Generate PostgreSQL regex pattern for valid categories\"\"\"\n",
    "    return r'\\m(' + '|'.join(VALID_CATEGORIES) + r')\\M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_label_columns():\n",
    "    \"\"\"Create columns for labeling results matching db_schema.md\"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            SELECT column_name \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_name = 'papers' \n",
    "            AND column_name IN ('category', 'safety_relevance', 'label_confidence')\n",
    "        ''')\n",
    "        existing_columns = {row[0] for row in cursor.fetchall()}\n",
    "        \n",
    "        if 'category' not in existing_columns:\n",
    "            cursor.execute('ALTER TABLE papers ADD COLUMN category TEXT')\n",
    "        if 'safety_relevance' not in existing_columns:\n",
    "            cursor.execute('ALTER TABLE papers ADD COLUMN safety_relevance FLOAT')\n",
    "        if 'label_confidence' not in existing_columns:\n",
    "            cursor.execute('ALTER TABLE papers ADD COLUMN label_confidence FLOAT')\n",
    "        conn.commit()\n",
    "\n",
    "# Create columns before processing\n",
    "create_label_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_paper_batches(batch_size=400):\n",
    "    \"\"\"Generator yielding batches of papers with target categories\"\"\"\n",
    "    cursor = conn.cursor(cursor_factory=DictCursor)\n",
    "    \n",
    "    # Get total count using schema-defined categories column\n",
    "    cursor.execute('''\n",
    "        SELECT COUNT(*) \n",
    "        FROM papers \n",
    "        WHERE categories ~ %s\n",
    "          AND title IS NOT NULL\n",
    "          AND abstract IS NOT NULL\n",
    "          AND category IS NULL\n",
    "    ''', (get_category_regex(),))\n",
    "    \n",
    "    total_papers = cursor.fetchone()[0]\n",
    "    \n",
    "    # Batch query using proper schema relationships\n",
    "    cursor.execute('''\n",
    "        SELECT p.id, p.title, p.abstract \n",
    "        FROM papers p\n",
    "        WHERE categories ~ %s\n",
    "          AND p.title IS NOT NULL\n",
    "          AND p.abstract IS NOT NULL\n",
    "          AND p.category IS NULL\n",
    "        ORDER BY p.id\n",
    "    ''', (get_category_regex(),))\n",
    "    \n",
    "    batch = []\n",
    "    with tqdm(total=total_papers, desc=\"Processing papers\", unit=\" papers\") as pbar:\n",
    "        for row in cursor:\n",
    "            batch.append(row)\n",
    "            if len(batch) >= batch_size:\n",
    "                pbar.update(len(batch))\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if batch:\n",
    "            pbar.update(len(batch))\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PaperLabel(TypedDict):\n",
    "    category: str\n",
    "    relevance_score: float\n",
    "    confidence: float\n",
    "\n",
    "model = genai.GenerativeModel(\"models/gemini-2.0-flash-exp\")\n",
    "generation_config = genai.GenerationConfig(\n",
    "    response_mime_type=\"application/json\",\n",
    "    response_schema=List[PaperLabel],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Rate limiter class matching Gemini 1.5 Flash limits\n",
    "class GeminiRateLimiter:\n",
    "    def __init__(self):\n",
    "        self.rpm_limit = 10  # Requests per minute\n",
    "        self.tpm_limit = 4_000_000  # Tokens per minute\n",
    "        self.rpd_limit = 1_500  # Requests per day\n",
    "        self.requests = []\n",
    "        self.tokens = []\n",
    "        self.daily_count = 0\n",
    "        \n",
    "    def can_make_request(self, input_tokens, output_tokens):\n",
    "        current_time = time.time()\n",
    "        cutoff_time = current_time - 60\n",
    "        \n",
    "        # Check daily limit\n",
    "        if self.daily_count >= self.rpd_limit:\n",
    "            print(\"Daily request limit reached\")\n",
    "            return False\n",
    "            \n",
    "        # Check RPM\n",
    "        recent_requests = [t for t in self.requests if t > cutoff_time]\n",
    "        if len(recent_requests) >= self.rpm_limit:\n",
    "            print(f\"RPM limit reached ({len(recent_requests)}/{self.rpm_limit})\")\n",
    "            return False\n",
    "            \n",
    "        # Check TPM\n",
    "        recent_tokens = sum(c for t, c in self.tokens if t > cutoff_time)\n",
    "        total_tokens = recent_tokens + input_tokens + output_tokens\n",
    "        if total_tokens > self.tpm_limit:\n",
    "            print(f\"TPM limit exceeded ({total_tokens}/{self.tpm_limit})\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def add_request(self, input_tokens, output_tokens):\n",
    "        current_time = time.time()\n",
    "        self.requests.append(current_time)\n",
    "        self.tokens.append((current_time, input_tokens + output_tokens))\n",
    "        self.daily_count += 1\n",
    "\n",
    "# Initialize rate limiter\n",
    "rate_limiter = GeminiRateLimiter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_token_count():\n",
    "    \"\"\"Test token counting with real data batch\"\"\"\n",
    "    try:\n",
    "        batch = next(get_paper_batches(batch_size=400))\n",
    "    except StopIteration:\n",
    "        print(\"No unprocessed papers available for testing\")\n",
    "        return\n",
    "\n",
    "    # Use generate_labels to test full flow\n",
    "    labels, input_toks, output_toks = generate_labels(batch)\n",
    "    \n",
    "    print(f\"\\nTest batch results:\")\n",
    "    print(f\"Input tokens: {input_toks}\")\n",
    "    print(f\"Output tokens: {output_toks}\")\n",
    "    print(f\"Total tokens: {input_toks + output_toks}\")\n",
    "    \n",
    "    # Check against limits\n",
    "    if output_toks >= 8192:\n",
    "        print(\"\\n⚠️ WARNING: Exceeded output token limit! Response may be truncated\")\n",
    "    else:\n",
    "        print(f\"\\n✅ Output tokens within limit (8192)\")\n",
    "\n",
    "    return input_toks, output_toks\n",
    "\n",
    "# Run updated test\n",
    "test_token_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        self.total_input = 0\n",
    "        self.total_output = 0\n",
    "        self.batches_processed = 0\n",
    "        \n",
    "    def add_batch(self, input_tokens, output_tokens):\n",
    "        self.total_input += input_tokens\n",
    "        self.total_output += output_tokens\n",
    "        self.batches_processed += 1\n",
    "        \n",
    "    def print_summary(self):\n",
    "        print(\"\\n=== Token Usage Summary ===\")\n",
    "        print(f\"Total batches processed: {self.batches_processed}\")\n",
    "        print(f\"Total input tokens: {self.total_input}\")\n",
    "        print(f\"Total output tokens: {self.total_output}\")\n",
    "        print(f\"Total tokens used: {self.total_input + self.total_output}\")\n",
    "\n",
    "token_tracker = TokenTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(multiplier=1, max=60), stop=stop_after_attempt(6))\n",
    "def generate_labels(batch):\n",
    "    \"\"\"Generate labels for a batch of papers using Gemini\"\"\"\n",
    "    base_prompt = \"\"\"You are an expert in AI safety and machine learning. Your task is to categorize academic papers and assess their relevance to AI safety.\n",
    "\n",
    "For each paper, provide:\n",
    "1. A specific technical category that precisely describes the primary research focus (e.g. \"Adversarial Attack Detection\" rather than \"AI Safety\", or \"Reward Modeling for RLHF\" rather than \"Reinforcement Learning\")\n",
    "2. A relevance score (0-1) indicating how relevant it is to AI safety research\n",
    "3. Your confidence (0-1) in this categorization\n",
    "\n",
    "Guidelines:\n",
    "- Use precise technical terminology\n",
    "- Categories should be specific enough to differentiate between similar papers\n",
    "- Consider both direct and indirect relevance to AI safety\n",
    "- Be consistent in scoring across papers\n",
    "\n",
    "Papers to analyze:\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = base_prompt\n",
    "    for paper in batch:\n",
    "        prompt += f\"\\n\\nTitle: {paper['title']}\\nAbstract: {paper['abstract']}\"\n",
    "    \n",
    "    # Count tokens\n",
    "    count_response = client.models.count_tokens(\n",
    "        model=MODEL_ID,\n",
    "        contents=prompt\n",
    "    )\n",
    "    input_tokens = count_response.total_tokens\n",
    "    \n",
    "    # Check rate limits (using actual output tokens)\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_ID,\n",
    "        contents=prompt,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    \n",
    "    if not response.usage_metadata:\n",
    "        raise ValueError(\"Missing usage metadata in response\")\n",
    "    \n",
    "    output_tokens = response.usage_metadata.candidates_token_count\n",
    "    \n",
    "    # Update rate limiter and tracker\n",
    "    rate_limiter.add_request(input_tokens, output_tokens)\n",
    "    token_tracker.add_batch(input_tokens, output_tokens)\n",
    "    \n",
    "    # Add truncation check\n",
    "    if output_tokens >= 8192:\n",
    "        print(f\"Warning: Output tokens at limit ({output_tokens}), response may be truncated\")\n",
    "    \n",
    "    return json.loads(response.text), input_tokens, output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_batches():\n",
    "    \"\"\"Main processing loop with rate limiting\"\"\"\n",
    "    for batch in get_paper_batches():\n",
    "        try:\n",
    "            labels, input_toks, output_toks = generate_labels(batch)\n",
    "            \n",
    "            # Print batch stats\n",
    "            print(f\"Processed batch of {len(batch)} papers\")\n",
    "            print(f\"Tokens: {input_toks} in → {output_toks} out\")\n",
    "            \n",
    "            # Update database\n",
    "            with conn.cursor() as cursor:\n",
    "                for paper, label in zip(batch, labels):\n",
    "                    cursor.execute('''\n",
    "                        UPDATE papers\n",
    "                        SET category = %s,\n",
    "                            safety_relevance = %s,\n",
    "                            label_confidence = %s\n",
    "                        WHERE id = %s\n",
    "                    ''', (\n",
    "                        label.get('category'),\n",
    "                        label.get('relevance_score'),\n",
    "                        label.get('confidence'),\n",
    "                        paper['id']\n",
    "                    ))\n",
    "                conn.commit()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {str(e)}\")\n",
    "            conn.rollback()\n",
    "    \n",
    "    # Print final summary\n",
    "    token_tracker.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Execute the pipeline\n",
    "process_batches()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_labels():\n",
    "    \"\"\"Validate labeling results using schema-defined relationships\"\"\"\n",
    "    cursor = conn.cursor(cursor_factory=DictCursor)\n",
    "    \n",
    "    print(\"\\n=== Labeling Quality Checks ===\")\n",
    "    \n",
    "    # Coverage check using proper schema columns\n",
    "    cursor.execute('''\n",
    "        SELECT \n",
    "            COUNT(*) AS total,\n",
    "            SUM(CASE WHEN category IS NOT NULL THEN 1 ELSE 0 END) AS labeled,\n",
    "            SUM(CASE WHEN category IS NULL THEN 1 ELSE 0 END) AS unlabeled\n",
    "        FROM papers\n",
    "        WHERE categories ~ %s\n",
    "    ''', (get_category_regex(),))\n",
    "    \n",
    "    stats = cursor.fetchone()\n",
    "    print(f\"Label coverage: {stats['labeled']}/{stats['total']} ({stats['labeled']/stats['total']*100:.1f}%)\")\n",
    "\n",
    "validate_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Execution Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run entire pipeline\n",
    "process_batches()\n",
    "validate_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Save duplicates to Drive\n",
    "def backup_database():\n",
    "    \"\"\"Backup only labeled papers using pg_dump\"\"\"\n",
    "    backup_path = \"/content/drive/MyDrive/ai-safety-papers/papers_labeled.sql\"\n",
    "    \n",
    "    # Create temporary view for labeled papers\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            CREATE OR REPLACE VIEW labeled_papers AS\n",
    "            SELECT * FROM papers\n",
    "            WHERE categories ~ %s\n",
    "              AND category IS NOT NULL\n",
    "        ''', (get_category_regex(),))\n",
    "    \n",
    "    # Dump the view instead of entire table\n",
    "    !pg_dump -U postgres -F p -f \"{backup_path}\" -t labeled_papers papers # pyright: ignore\n",
    "    \n",
    "    # Clean up view\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('DROP VIEW IF EXISTS labeled_papers')\n",
    "    \n",
    "    print(f\"Backup saved to {backup_path}\")\n",
    "\n",
    "# Call backup after processing\n",
    "backup_database()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
