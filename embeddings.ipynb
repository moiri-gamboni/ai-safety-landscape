{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Papers - Embedding Phase\n",
    "\n",
    "This notebook generates Voyage AI embeddings for paper titles and abstracts and stores them in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive # pyright: ignore [reportMissingImports]\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Install and configure PostgreSQL\n",
    "    !sudo apt-get -qq update && sudo apt-get -qq install postgresql postgresql-contrib # pyright: ignore\n",
    "    !sudo service postgresql start # pyright: ignore\n",
    "    !sudo sed -i 's/local\\s*all\\s*postgres\\s*peer/local all postgres trust/' /etc/postgresql/14/main/pg_hba.conf # pyright: ignore\n",
    "    !sudo service postgresql restart # pyright: ignore\n",
    "    \n",
    "    # Install Python client\n",
    "    %pip install psycopg2-binary # pyright: ignore\n",
    "    %pip install voyageai tqdm scikit-learn tenacity # pyright: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import voyageai\n",
    "import time\n",
    "from typing import List, Tuple, Optional\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "import re\n",
    "from psycopg2.extras import DictCursor\n",
    "\n",
    "# Load API key from Colab form or environment\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    print(\"Getting Voyage AI API key from Colab form\")\n",
    "    # @title Voyage AI API Key\n",
    "    voyage_api_key = \"\" # @param {type:\"string\"}\n",
    "    # Set it for the client\n",
    "    voyageai.api_key = voyage_api_key\n",
    "else:\n",
    "    from dotenv import load_dotenv\n",
    "    print(\"Running locally, loading API key from .env\")\n",
    "    load_dotenv()\n",
    "    voyageai.api_key = os.getenv('VOYAGE_API_KEY')\n",
    "\n",
    "# Initialize Voyage AI client\n",
    "vo = voyageai.Client()\n",
    "\n",
    "# Path to database\n",
    "db_path = \"/content/drive/MyDrive/ai-safety-papers/papers.sql\"\n",
    "\n",
    "def load_database():\n",
    "    \"\"\"Load PostgreSQL backup using psql\"\"\"\n",
    "    print(\"Loading PostgreSQL backup...\")\n",
    "    !createdb -U postgres papers # pyright: ignore\n",
    "    !pg_restore -U postgres --jobs=8 -d papers \"{backup_path}\" # pyright: ignore\n",
    "\n",
    "def connect_db():\n",
    "    \"\"\"Connect to PostgreSQL database with schema validation\"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        host='',\n",
    "        database=\"papers\",\n",
    "        user=\"postgres\"\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "load_database()\n",
    "conn = connect_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Column Reset (Run when reprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def drop_embedding_column():\n",
    "    \"\"\"Drop the embedding column for reprocessing\"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            ALTER TABLE papers \n",
    "            DROP COLUMN IF EXISTS abstract_embedding\n",
    "        ''')\n",
    "        conn.commit()\n",
    "    print(\"Dropped abstract_embedding column\")\n",
    "\n",
    "drop_embedding_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_columns():\n",
    "    # Check for embedding column\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            SELECT EXISTS (\n",
    "                SELECT 1 \n",
    "                FROM information_schema.columns \n",
    "                WHERE table_name = 'papers' \n",
    "                AND column_name = 'embedding'\n",
    "            )\n",
    "        ''')\n",
    "        if not cursor.fetchone()[0]:\n",
    "            print(\"Adding embedding column...\")\n",
    "            cursor.execute('''\n",
    "                ALTER TABLE papers \n",
    "                ADD COLUMN embedding BYTEA\n",
    "            ''')\n",
    "            cursor.execute('''\n",
    "                CREATE INDEX idx_embedding_not_null \n",
    "                ON papers ((embedding IS NOT NULL)) \n",
    "            ''')\n",
    "            conn.commit()\n",
    "\n",
    "create_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "max_batches = None  # Set to None to process all batches, or a number to limit processing\n",
    "batch_size = 128  # Maximum batch size for Voyage AI\n",
    "\n",
    "# Rate limit tracking\n",
    "class RateLimiter:\n",
    "    def __init__(self, rpm_limit=2000, tpm_limit=3_000_000):  # voyage-3-large limits: 2000 RPM, 3M TPM\n",
    "        self.rpm_limit = rpm_limit\n",
    "        self.tpm_limit = tpm_limit\n",
    "        self.requests = []  # List of timestamps\n",
    "        self.tokens = []    # List of (timestamp, token_count) tuples\n",
    "        \n",
    "    def can_make_request(self, token_count):\n",
    "        current_time = time.time()\n",
    "        cutoff_time = current_time - 60  # 1 minute ago\n",
    "        \n",
    "        # Clean up old entries\n",
    "        self.requests = [t for t in self.requests if t > cutoff_time]\n",
    "        self.tokens = [(t, c) for t, c in self.tokens if t > cutoff_time]\n",
    "        \n",
    "        # Check RPM limit\n",
    "        if len(self.requests) >= self.rpm_limit:\n",
    "            print(f\"\\nRate limit reached at {time.strftime('%H:%M:%S')}:\")\n",
    "            print(f\"- Request limit: {len(self.requests)}/{self.rpm_limit} RPM\")\n",
    "            return False\n",
    "            \n",
    "        # Check TPM limit\n",
    "        total_tokens = sum(count for _, count in self.tokens)\n",
    "        if total_tokens + token_count > self.tpm_limit:\n",
    "            print(f\"\\nRate limit reached at {time.strftime('%H:%M:%S')}:\")\n",
    "            print(f\"- Token limit: {total_tokens + token_count}/{self.tpm_limit} TPM\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def add_request(self, token_count):\n",
    "        current_time = time.time()\n",
    "        self.requests.append(current_time)\n",
    "        self.tokens.append((current_time, token_count))\n",
    "\n",
    "rate_limiter = RateLimiter()\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=1, max=60), stop=stop_after_attempt(6))\n",
    "def embed_with_backoff(texts, model=\"voyage-3-large\"):\n",
    "    return vo.embed(\n",
    "        texts,\n",
    "        model=model,\n",
    "        input_type=\"document\",\n",
    "        output_dimension=2048\n",
    "    )\n",
    "\n",
    "def get_csai_papers(conn: psycopg2.extensions.connection, batch_size: int = 128):\n",
    "    \"\"\"Generator that yields batches of papers with cs.AI in their categories\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        batch_size: Maximum number of papers per batch\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor(cursor_factory=DictCursor)\n",
    "    \n",
    "    # First get total count for progress bar\n",
    "    cursor.execute('''\n",
    "        WITH split_categories AS (\n",
    "            SELECT id, title, abstract\n",
    "            FROM papers\n",
    "            WHERE categories LIKE '%cs.AI%'\n",
    "              AND title IS NOT NULL\n",
    "              AND abstract IS NOT NULL\n",
    "              AND embedding IS NULL\n",
    "        )\n",
    "        SELECT COUNT(*) FROM split_categories\n",
    "    ''')\n",
    "    total_papers = cursor.fetchone()[0]\n",
    "    \n",
    "    # Then fetch papers in batches\n",
    "    cursor.execute('''\n",
    "        WITH split_categories AS (\n",
    "            SELECT id, title, abstract\n",
    "            FROM papers\n",
    "            WHERE categories LIKE '%cs.AI%'\n",
    "              AND title IS NOT NULL\n",
    "              AND abstract IS NOT NULL\n",
    "              AND embedding IS NULL\n",
    "        )\n",
    "        SELECT id, title, abstract FROM split_categories\n",
    "    ''')\n",
    "    \n",
    "    batch = []\n",
    "    with tqdm(total=total_papers, desc=\"Processing papers\", unit=\" papers\") as pbar:\n",
    "        for row in cursor:\n",
    "            batch.append((row['id'], f\"{row['title']}\\n{row['abstract']}\"))\n",
    "            if len(batch) >= batch_size:\n",
    "                pbar.update(len(batch))\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if batch:\n",
    "            pbar.update(len(batch))\n",
    "            yield batch\n",
    "\n",
    "def adjust_batch_for_token_limit(batch: List[Tuple[str, str]], model: str = \"voyage-3-large\") -> List[List[Tuple[str, str]]]:\n",
    "    \"\"\"Split a batch into sub-batches that respect the token limit\n",
    "    \n",
    "    Args:\n",
    "        batch: List of (id, text) tuples\n",
    "        model: Voyage AI model to use\n",
    "    \n",
    "    Returns:\n",
    "        List of batches, each respecting the token limit\n",
    "    \"\"\"\n",
    "    TOKEN_LIMIT = 120_000  # voyage-3-large limit\n",
    "    \n",
    "    texts = [item[1] for item in batch]\n",
    "    token_counts = [vo.count_tokens([text], model=model) for text in texts]\n",
    "    \n",
    "    sub_batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for (paper_id, text), token_count in zip(batch, token_counts):\n",
    "        # If single text exceeds limit, skip it\n",
    "        if token_count > TOKEN_LIMIT:\n",
    "            print(f\"Warning: Text {paper_id} exceeds token limit ({token_count} tokens), skipping\")\n",
    "            continue\n",
    "            \n",
    "        # If adding this text would exceed limit, start new batch\n",
    "        if current_tokens + token_count > TOKEN_LIMIT:\n",
    "            if current_batch:\n",
    "                sub_batches.append(current_batch)\n",
    "            current_batch = [(paper_id, text)]\n",
    "            current_tokens = token_count\n",
    "        else:\n",
    "            current_batch.append((paper_id, text))\n",
    "            current_tokens += token_count\n",
    "    \n",
    "    if current_batch:\n",
    "        sub_batches.append(current_batch)\n",
    "    \n",
    "    return sub_batches\n",
    "\n",
    "def process_batch(batch: List[Tuple[str, str]], model: str = \"voyage-3-large\") -> List[Tuple[str, List[float]]]:\n",
    "    \"\"\"Process a batch of papers, returning embeddings\"\"\"\n",
    "    texts = [item[1] for item in batch]\n",
    "    paper_ids = [item[0] for item in batch]\n",
    "    \n",
    "    # First count tokens for rate limiting\n",
    "    try:\n",
    "        token_count = vo.count_tokens(texts, model=model)\n",
    "    except Exception as e:\n",
    "        print(f\"Error counting tokens: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    # Check rate limits\n",
    "    if not rate_limiter.can_make_request(token_count):\n",
    "        raise Exception(\"Rate limit exceeded\")  # This will trigger exponential backoff\n",
    "    \n",
    "    # Generate embeddings with exponential backoff\n",
    "    try:\n",
    "        result = embed_with_backoff(texts, model=model)\n",
    "        \n",
    "        if isinstance(result, float):\n",
    "            raise ValueError(f\"API returned float instead of EmbeddingsObject: {result}\")\n",
    "            \n",
    "        if not hasattr(result, 'embeddings'):\n",
    "            raise ValueError(f\"Unexpected response from vo.embed: {result}\")\n",
    "            \n",
    "        # Return list of (paper_id, embedding) tuples and total tokens used\n",
    "        embeddings = result.embeddings\n",
    "        rate_limiter.add_request(token_count)\n",
    "        \n",
    "        return [(paper_id, embedding) for paper_id, embedding in zip(paper_ids, embeddings)], result.total_tokens\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"\\nError in vo.embed call:\")\n",
    "        print(f\"Exception type: {type(e)}\")\n",
    "        print(f\"Exception args: {e.args}\")\n",
    "        print(f\"Full exception: {repr(e)}\")\n",
    "        print(\"\\nInput that caused error:\")\n",
    "        print(f\"First text: {texts[0][:500]}...\")  # Truncate long texts\n",
    "        raise\n",
    "\n",
    "# Process in batches\n",
    "total_updated = 0\n",
    "batches_processed = 0\n",
    "total_tokens = 0\n",
    "\n",
    "try:\n",
    "    for batch in get_csai_papers(conn, batch_size):\n",
    "        if max_batches is not None and batches_processed >= max_batches:\n",
    "            print(f\"\\nStopping after {max_batches} batch(es) as requested\")\n",
    "            break\n",
    "            \n",
    "        # Split batch if needed to respect token limit\n",
    "        sub_batches = adjust_batch_for_token_limit(batch)\n",
    "        \n",
    "        for i, sub_batch in enumerate(sub_batches):\n",
    "            try:\n",
    "                # Process batch and get embeddings\n",
    "                results, batch_tokens = process_batch(sub_batch)\n",
    "                if not isinstance(results, list):\n",
    "                    print(f\"Warning: results is not a list, got {type(results)}\")\n",
    "                    print(f\"Results value: {results}\")\n",
    "                    raise ValueError(f\"Expected list result, got {type(results)}\")\n",
    "                \n",
    "                # Store in database\n",
    "                cursor = conn.cursor()\n",
    "                for paper_id, embedding in results:\n",
    "                    embedding_blob = np.array(embedding, dtype=np.float32).tobytes()\n",
    "                    cursor.execute('''\n",
    "                        UPDATE papers\n",
    "                        SET embedding = %s\n",
    "                        WHERE id = %s\n",
    "                    ''', (embedding_blob, paper_id))\n",
    "                \n",
    "                conn.commit()\n",
    "                total_updated += len(sub_batch)\n",
    "                \n",
    "                # Update total tokens\n",
    "                total_tokens += batch_tokens\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nFatal error processing sub-batch:\")\n",
    "                print(f\"Exception type: {type(e)}\")\n",
    "                print(f\"Exception args: {e.args}\")\n",
    "                print(f\"Full exception: {repr(e)}\")\n",
    "                print(f\"Sub-batch contents: {sub_batch[:2]}...\")\n",
    "                raise  # Stop execution here\n",
    "        \n",
    "        batches_processed += 1\n",
    "        if max_batches is not None:\n",
    "            print(f\"\\nProcessed {batches_processed}/{max_batches} batch(es)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nFatal error in embedding generation: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    print(f\"\\nProcess completed:\")\n",
    "    print(f\"- Total papers processed: {total_updated}\")\n",
    "    print(f\"- Total batches processed: {batches_processed}\")\n",
    "    print(f\"- Total tokens processed: {total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def validate_embeddings(conn):\n",
    "    \"\"\"Run quality checks on generated embeddings\"\"\"\n",
    "    # Set fixed seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    cursor = conn.cursor(cursor_factory=DictCursor)\n",
    "    \n",
    "    print(\"\\n=== Embedding Quality Checks ===\")\n",
    "    \n",
    "    # 1. Coverage Check\n",
    "    cursor.execute('''\n",
    "        SELECT \n",
    "            COUNT(*) AS total_csai,\n",
    "            SUM(CASE WHEN embedding IS NOT NULL THEN 1 ELSE 0 END) AS with_embedding,\n",
    "            SUM(CASE WHEN embedding IS NULL THEN 1 ELSE 0 END) AS without_embedding\n",
    "        FROM papers \n",
    "        WHERE categories LIKE '%cs.AI%'\n",
    "          AND title IS NOT NULL\n",
    "          AND abstract IS NOT NULL\n",
    "    ''')\n",
    "    stats = cursor.fetchone()\n",
    "    print(f\"\\n1. Coverage:\")\n",
    "    print(f\"- Total CS.AI papers: {stats['total_csai']}\")\n",
    "    print(f\"- With embeddings: {stats['with_embedding']} ({(stats['with_embedding']/stats['total_csai'])*100:.1f}%)\")\n",
    "    print(f\"- Missing embeddings: {stats['without_embedding']} ({(stats['without_embedding']/stats['total_csai'])*100:.1f}%)\")\n",
    "\n",
    "    # 2. Validity Check (NaN/Zero vectors)\n",
    "    cursor.execute('''\n",
    "        SELECT embedding \n",
    "        FROM papers \n",
    "        WHERE embedding IS NOT NULL\n",
    "    ''')\n",
    "    invalid_count = 0\n",
    "    total_checked = 0\n",
    "    for row in cursor:\n",
    "        embedding = np.frombuffer(row['embedding'], dtype=np.float32)\n",
    "        if np.isnan(embedding).any():\n",
    "            invalid_count += 1\n",
    "        elif np.all(embedding == 0):\n",
    "            invalid_count += 1\n",
    "        total_checked += 1\n",
    "    \n",
    "    print(f\"\\n2. Validity (sampled {total_checked}):\")\n",
    "    print(f\"- Invalid embeddings (NaN/zeros): {invalid_count} ({(invalid_count/total_checked)*100:.1f}%)\")\n",
    "\n",
    "    # 3. Norm Analysis\n",
    "    cursor.execute('''\n",
    "        SELECT embedding \n",
    "        FROM papers \n",
    "        WHERE embedding IS NOT NULL\n",
    "    ''')\n",
    "    norms = []\n",
    "    for row in cursor:\n",
    "        embedding = np.frombuffer(row['embedding'], dtype=np.float32)\n",
    "        norms.append(np.linalg.norm(embedding))\n",
    "    \n",
    "    print(f\"\\n3. Norm Analysis (sample):\")\n",
    "    print(f\"- Mean norm: {np.mean(norms):.2f}\")\n",
    "    print(f\"- Std dev: {np.std(norms):.2f}\")\n",
    "    print(f\"- Min/Max: {np.min(norms):.2f}/{np.max(norms):.2f}\")\n",
    "\n",
    "    # 4. Similarity Analysis\n",
    "    # Get random pairs more efficiently\n",
    "    cursor.execute('''\n",
    "        SELECT id, title, abstract, embedding \n",
    "        FROM papers \n",
    "        WHERE embedding IS NOT NULL\n",
    "          AND categories LIKE '%cs.AI%'\n",
    "          AND RANDOM() < 0.01  -- Sample ~1% of records\n",
    "        LIMIT 10000\n",
    "    ''')\n",
    "    papers = cursor.fetchall()\n",
    "    random_embeddings = np.vstack([np.frombuffer(row['embedding'], dtype=np.float32) for row in papers])\n",
    "    \n",
    "    # Analyze embedding components\n",
    "    print(\"\\n4. Embedding Analysis:\")\n",
    "    print(f\"- Shape: {random_embeddings.shape}\")\n",
    "    print(\"- Component statistics:\")\n",
    "    means = np.mean(random_embeddings, axis=0)\n",
    "    stds = np.std(random_embeddings, axis=0)\n",
    "    print(f\"  Mean of means: {np.mean(means):.3f} ± {np.std(means):.3f}\")\n",
    "    print(f\"  Mean of stds: {np.mean(stds):.3f} ± {np.std(stds):.3f}\")\n",
    "    print(f\"  Component range: [{np.min(random_embeddings):.3f}, {np.max(random_embeddings):.3f}]\")\n",
    "    \n",
    "    # Show distribution of components\n",
    "    positive_frac = np.mean(random_embeddings > 0)\n",
    "    print(f\"  Fraction of positive components: {positive_frac:.3f}\")\n",
    "    \n",
    "    # Compute similarities using dot product (faster for normalized vectors)\n",
    "    n_samples = len(random_embeddings)\n",
    "    if n_samples > 1:\n",
    "        # Generate random pairs of indices\n",
    "        idx1 = np.random.randint(0, n_samples, size=1000)\n",
    "        idx2 = np.random.randint(0, n_samples, size=1000)\n",
    "        # Exclude self-pairs\n",
    "        valid_pairs = idx1 != idx2\n",
    "        idx1, idx2 = idx1[valid_pairs], idx2[valid_pairs]\n",
    "        \n",
    "        # Compute dot products (equivalent to cosine similarity for normalized vectors)\n",
    "        similarities = np.sum(random_embeddings[idx1] * random_embeddings[idx2], axis=1)\n",
    "        \n",
    "        print(\"\\n5. Similarity Analysis:\")\n",
    "        print(f\"- Random pairs (n={len(similarities)}):\")\n",
    "        print(f\"  Mean: {np.mean(similarities):.3f} ± {np.std(similarities):.3f}\")\n",
    "        print(f\"  Range: [{np.min(similarities):.3f}, {np.max(similarities):.3f}]\")\n",
    "        \n",
    "        # Print example pairs with different similarity levels\n",
    "        print(\"\\nExample pairs:\")\n",
    "        # Most similar pair\n",
    "        most_similar_idx = np.argmax(similarities)\n",
    "        print(f\"\\nMost similar pair (similarity: {similarities[most_similar_idx]:.3f}):\")\n",
    "        print(f\"Paper 1: {papers[idx1[most_similar_idx]]['title']}\")\n",
    "        print(f\"URL 1: https://arxiv.org/abs/{papers[idx1[most_similar_idx]]['id']}\")\n",
    "        print(f\"Abstract 1:\\n{papers[idx1[most_similar_idx]]['abstract']}\")\n",
    "        print(f\"\\nPaper 2: {papers[idx2[most_similar_idx]]['title']}\")\n",
    "        print(f\"URL 2: https://arxiv.org/abs/{papers[idx2[most_similar_idx]]['id']}\")\n",
    "        print(f\"Abstract 2:\\n{papers[idx2[most_similar_idx]]['abstract']}\")\n",
    "        \n",
    "        # Least similar pair\n",
    "        least_similar_idx = np.argmin(similarities)\n",
    "        print(f\"\\nLeast similar pair (similarity: {similarities[least_similar_idx]:.3f}):\")\n",
    "        print(f\"Paper 1: {papers[idx1[least_similar_idx]]['title']}\")\n",
    "        print(f\"URL 1: https://arxiv.org/abs/{papers[idx1[least_similar_idx]]['id']}\")\n",
    "        print(f\"Abstract 1:\\n{papers[idx1[least_similar_idx]]['abstract']}\")\n",
    "        print(f\"\\nPaper 2: {papers[idx2[least_similar_idx]]['title']}\")\n",
    "        print(f\"URL 2: https://arxiv.org/abs/{papers[idx2[least_similar_idx]]['id']}\")\n",
    "        print(f\"Abstract 2:\\n{papers[idx2[least_similar_idx]]['abstract']}\")\n",
    "        \n",
    "        # Median similarity pair\n",
    "        median_idx = np.argsort(similarities)[len(similarities)//2]\n",
    "        print(f\"\\nMedian similarity pair (similarity: {similarities[median_idx]:.3f}):\")\n",
    "        print(f\"Paper 1: {papers[idx1[median_idx]]['title']}\")\n",
    "        print(f\"URL 1: https://arxiv.org/abs/{papers[idx1[median_idx]]['id']}\")\n",
    "        print(f\"Abstract 1:\\n{papers[idx1[median_idx]]['abstract']}\")\n",
    "        print(f\"\\nPaper 2: {papers[idx2[median_idx]]['title']}\")\n",
    "        print(f\"URL 2: https://arxiv.org/abs/{papers[idx2[median_idx]]['id']}\")\n",
    "        print(f\"Abstract 2:\\n{papers[idx2[median_idx]]['abstract']}\")\n",
    "    else:\n",
    "        print(\"\\n5. Similarity Analysis:\")\n",
    "        print(\"Not enough samples for similarity analysis\")\n",
    "    \n",
    "    # 5. Embedding Analysis\n",
    "    print(\"\\n5. Embedding Analysis:\")\n",
    "    \n",
    "    def find_outliers(embeddings: np.ndarray, papers: list, n_examples: int = 3):\n",
    "        \"\"\"Find outlier papers based on their similarity to other papers\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Array of embeddings (each normalized to unit length)\n",
    "            papers: List of paper records corresponding to embeddings\n",
    "            n_examples: Number of outlier examples to show\n",
    "        \"\"\"\n",
    "        # Calculate pairwise similarities\n",
    "        similarities = np.dot(embeddings, embeddings.T)  # embeddings already normalized\n",
    "        \n",
    "        # For each paper, get average similarity to other papers\n",
    "        # Exclude self-similarity (1.0) from the average\n",
    "        avg_similarities = []\n",
    "        for i in range(len(similarities)):\n",
    "            others = np.concatenate([similarities[i,:i], similarities[i,i+1:]])\n",
    "            avg_similarities.append(np.mean(others))\n",
    "        \n",
    "        avg_similarities = np.array(avg_similarities)\n",
    "        \n",
    "        # Use IQR method on average similarities\n",
    "        q1, q3 = np.percentile(avg_similarities, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr  # Papers with unusually low average similarity\n",
    "        \n",
    "        # Find outliers (papers with unusually low average similarity to others)\n",
    "        outlier_indices = np.where(avg_similarities < lower_bound)[0]\n",
    "        \n",
    "        print(f\"\\nOutlier Analysis:\")\n",
    "        print(f\"- Average similarity stats:\")\n",
    "        print(f\"  Mean: {np.mean(avg_similarities):.3f} ± {np.std(avg_similarities):.3f}\")\n",
    "        print(f\"  Range: [{np.min(avg_similarities):.3f}, {np.max(avg_similarities):.3f}]\")\n",
    "        print(f\"- Found {len(outlier_indices)} outliers ({len(outlier_indices)/len(embeddings)*100:.1f}%)\")\n",
    "        \n",
    "        if len(outlier_indices) > 0:\n",
    "            # Sort outliers by average similarity (ascending)\n",
    "            sorted_outliers = sorted(zip(outlier_indices, avg_similarities[outlier_indices]), \n",
    "                                   key=lambda x: x[1])\n",
    "            \n",
    "            print(f\"\\nTop {n_examples} outliers (lowest average similarity to other papers):\")\n",
    "            for idx, avg_sim in sorted_outliers[:n_examples]:\n",
    "                paper = papers[idx]\n",
    "                print(f\"\\nAverage similarity to other papers: {avg_sim:.3f}\")\n",
    "                print(f\"Title: {paper['title']}\")\n",
    "                print(f\"URL: https://arxiv.org/abs/{paper['id']}\")\n",
    "                print(f\"Abstract:\\n{paper['abstract'][:500]}...\")\n",
    "                \n",
    "                # Show a few most and least similar papers to this one\n",
    "                sims = similarities[idx]\n",
    "                most_similar_idx = np.argsort(sims)[-2]  # -1 would be self\n",
    "                least_similar_idx = np.argsort(sims)[0]\n",
    "                \n",
    "                print(f\"\\nMost similar paper (similarity: {sims[most_similar_idx]:.3f}):\")\n",
    "                print(f\"Title: {papers[most_similar_idx]['title']}\")\n",
    "                \n",
    "                print(f\"\\nLeast similar paper (similarity: {sims[least_similar_idx]:.3f}):\")\n",
    "                print(f\"Title: {papers[least_similar_idx]['title']}\")\n",
    "    \n",
    "    # Get sample of embeddings\n",
    "    cursor.execute('''\n",
    "        SELECT id, title, abstract, embedding\n",
    "        FROM papers \n",
    "        WHERE embedding IS NOT NULL\n",
    "        LIMIT 1000\n",
    "    ''')\n",
    "    papers = cursor.fetchall()\n",
    "    embeddings = [np.frombuffer(row['embedding'], dtype=np.float32) for row in papers]\n",
    "    all_embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Basic embedding stats\n",
    "    print(f\"- Shape: {all_embeddings.shape}\")\n",
    "    print(\"- Component statistics:\")\n",
    "    means = np.mean(all_embeddings, axis=0)\n",
    "    stds = np.std(all_embeddings, axis=0)\n",
    "    print(f\"  Mean of means: {np.mean(means):.3f} ± {np.std(means):.3f}\")\n",
    "    print(f\"  Mean of stds: {np.mean(stds):.3f} ± {np.std(stds):.3f}\")\n",
    "    print(f\"  Component range: [{np.min(all_embeddings):.3f}, {np.max(all_embeddings):.3f}]\")\n",
    "    \n",
    "    # Show distribution of components\n",
    "    positive_frac = np.mean(all_embeddings > 0)\n",
    "    print(f\"  Fraction of positive components: {positive_frac:.3f}\")\n",
    "    \n",
    "    # Find outliers using the new method\n",
    "    find_outliers(all_embeddings, papers)\n",
    "\n",
    "# Run validation\n",
    "validate_embeddings(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5. Duplicate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_duplicates(conn, similarity_threshold=0.95, output_file='duplicates.csv'):\n",
    "    \"\"\"Find potential duplicate papers based on title and embedding similarity\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        similarity_threshold: Minimum similarity to consider as duplicate\n",
    "        output_file: Path to save results (CSV)\n",
    "    \"\"\"\n",
    "    print(\"Finding potential duplicates...\")\n",
    "    \n",
    "    def normalize_title(title):\n",
    "        \"\"\"Normalize title for comparison\"\"\"\n",
    "        if not title:\n",
    "            return \"\"\n",
    "        title = title.lower()\n",
    "        title = re.sub(r'\\bv\\d+\\b', '', title)\n",
    "        title = re.sub(r'arxiv:\\d+\\.\\d+', '', title, flags=re.IGNORECASE)\n",
    "        title = re.sub(r'[^\\w\\s]', '', title)\n",
    "        return ' '.join(title.split())\n",
    "    \n",
    "    # Get papers with embeddings\n",
    "    cursor = conn.cursor(cursor_factory=DictCursor)\n",
    "    cursor.execute('''\n",
    "        SELECT id, title, embedding\n",
    "        FROM papers\n",
    "        WHERE embedding IS NOT NULL\n",
    "          AND withdrawn = false\n",
    "    ''')\n",
    "    \n",
    "    # Group by normalized title\n",
    "    title_groups = {}\n",
    "    for row in cursor:\n",
    "        norm_title = normalize_title(row['title'])\n",
    "        if norm_title:  # Skip empty titles\n",
    "            if norm_title in title_groups:\n",
    "                title_groups[norm_title].append(row)\n",
    "            else:\n",
    "                title_groups[norm_title] = [row]\n",
    "    \n",
    "    # Find duplicates based on both title and embedding similarity\n",
    "    duplicates = []\n",
    "    \n",
    "    print(f\"Analyzing {len(title_groups)} unique normalized titles...\")\n",
    "    for title, group in title_groups.items():\n",
    "        if len(group) > 1:  # Only process groups with multiple papers\n",
    "            # Compare embeddings within group\n",
    "            embeddings = np.vstack([np.frombuffer(p['embedding'], dtype=np.float32) for p in group])\n",
    "            # Use dot product directly since embeddings are already normalized\n",
    "            similarities = np.dot(embeddings, embeddings.T)  # No normalization needed\n",
    "            \n",
    "            # Find pairs above threshold (excluding self-similarity)\n",
    "            for i in range(len(similarities)):\n",
    "                for j in range(i+1, len(similarities)):\n",
    "                    if similarities[i,j] > similarity_threshold:\n",
    "                        duplicates.append({\n",
    "                            'id1': group[i]['id'],\n",
    "                            'id2': group[j]['id'],\n",
    "                            'title': group[i]['title'],\n",
    "                            'similarity': float(similarities[i,j])\n",
    "                        })\n",
    "    \n",
    "    # Sort by similarity\n",
    "    duplicates.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    # Save results\n",
    "    import csv\n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['id1', 'id2', 'title', 'similarity'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(duplicates)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nFound {len(duplicates)} potential duplicate pairs\")\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "    \n",
    "    # Print top examples\n",
    "    if duplicates:\n",
    "        print(\"\\nTop 5 potential duplicates:\")\n",
    "        for dup in duplicates[:5]:\n",
    "            print(f\"\\nSimilarity: {dup['similarity']:.3f}\")\n",
    "            print(f\"Title: {dup['title']}\")\n",
    "            print(f\"Paper 1: https://arxiv.org/abs/{dup['id1']}\")\n",
    "            print(f\"Paper 2: https://arxiv.org/abs/{dup['id2']}\")\n",
    "\n",
    "# Run duplicate analysis\n",
    "find_duplicates(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Save duplicates to Drive\n",
    "%cp duplicates.csv \"/content/drive/MyDrive/ai-safety-papers/duplicates.csv\" # pyright: ignore\n",
    "print(f\"Duplicates saved to Drive\")\n",
    "\n",
    "def backup_embeddings():\n",
    "    \"\"\"Use pg_dump for PostgreSQL backups\"\"\"\n",
    "    backup_path = \"/content/drive/MyDrive/ai-safety-papers/papers.sql\"\n",
    "    !pg_dump -U postgres -F c -f \"{backup_path}\" papers # pyright: ignore\n",
    "\n",
    "# Call backup after processing\n",
    "backup_embeddings()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
