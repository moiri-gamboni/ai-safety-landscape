{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Papers - Abstract Embedding Phase\n",
    "\n",
    "This notebook generates Voyage AI embeddings for paper abstracts and stores them in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Clone repository if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Clone the repository\n",
    "    !git clone https://github.com/moiri-gamboni/ai-safety-landscape.git\n",
    "    %cd ai-safety-landscape\n",
    "\n",
    "# Install required packages\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import voyageai\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "from typing import List, Tuple, Optional\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "import re\n",
    "\n",
    "# Load API key - try Colab form first, then .env\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    print(\"Getting Voyage AI API key from Colab form\")\n",
    "    # @title Voyage AI API Key\n",
    "    voyage_api_key = \"\" # @param {type:\"string\"}\n",
    "    # Set it for the client\n",
    "    voyageai.api_key = voyage_api_key\n",
    "else:\n",
    "    print(\"Running locally, loading API key from .env\")\n",
    "    load_dotenv()\n",
    "\n",
    "# Initialize Voyage AI client\n",
    "vo = voyageai.Client()\n",
    "\n",
    "# Path to database\n",
    "db_path = \"/content/drive/MyDrive/ai-safety-papers/papers.db\"\n",
    "local_db = \"papers.db\"\n",
    "\n",
    "# Copy database to local storage if needed\n",
    "print(f\"Copying database to local storage: {local_db}\")\n",
    "if not os.path.exists(local_db):\n",
    "    %cp \"{db_path}\" {local_db}\n",
    "\n",
    "conn = sqlite3.connect(local_db)\n",
    "conn.row_factory = sqlite3.Row\n",
    "\n",
    "# Check if abstract_embedding and token_count columns exist\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"PRAGMA table_info(papers)\")\n",
    "columns = [column[1] for column in cursor.fetchall()]\n",
    "\n",
    "if 'abstract_embedding' not in columns:\n",
    "    print(\"Adding abstract_embedding column...\")\n",
    "    conn.execute('''\n",
    "        ALTER TABLE papers \n",
    "        ADD COLUMN abstract_embedding BLOB\n",
    "    ''')\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "max_batches = None  # Set to None to process all batches, or a number to limit processing\n",
    "batch_size = 128  # Maximum batch size for Voyage AI\n",
    "\n",
    "# Rate limit tracking\n",
    "class RateLimiter:\n",
    "    def __init__(self, rpm_limit=2000, tpm_limit=3_000_000):  # voyage-3-large limits: 2000 RPM, 3M TPM\n",
    "        self.rpm_limit = rpm_limit\n",
    "        self.tpm_limit = tpm_limit\n",
    "        self.requests = []  # List of timestamps\n",
    "        self.tokens = []    # List of (timestamp, token_count) tuples\n",
    "        \n",
    "    def can_make_request(self, token_count):\n",
    "        current_time = time.time()\n",
    "        cutoff_time = current_time - 60  # 1 minute ago\n",
    "        \n",
    "        # Clean up old entries\n",
    "        self.requests = [t for t in self.requests if t > cutoff_time]\n",
    "        self.tokens = [(t, c) for t, c in self.tokens if t > cutoff_time]\n",
    "        \n",
    "        # Check RPM limit\n",
    "        if len(self.requests) >= self.rpm_limit:\n",
    "            print(f\"\\nRate limit reached at {time.strftime('%H:%M:%S')}:\")\n",
    "            print(f\"- Request limit: {len(self.requests)}/{self.rpm_limit} RPM\")\n",
    "            return False\n",
    "            \n",
    "        # Check TPM limit\n",
    "        total_tokens = sum(count for _, count in self.tokens)\n",
    "        if total_tokens + token_count > self.tpm_limit:\n",
    "            print(f\"\\nRate limit reached at {time.strftime('%H:%M:%S')}:\")\n",
    "            print(f\"- Token limit: {total_tokens + token_count}/{self.tpm_limit} TPM\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def add_request(self, token_count):\n",
    "        current_time = time.time()\n",
    "        self.requests.append(current_time)\n",
    "        self.tokens.append((current_time, token_count))\n",
    "\n",
    "rate_limiter = RateLimiter()\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=1, max=60), stop=stop_after_attempt(6))\n",
    "def embed_with_backoff(abstracts, model=\"voyage-3-large\"):\n",
    "    return vo.embed(\n",
    "        abstracts,\n",
    "        model=model,\n",
    "        input_type=\"document\",\n",
    "        output_dimension=2048\n",
    "    )\n",
    "\n",
    "def get_csai_papers(conn: sqlite3.Connection, batch_size: int = 128):\n",
    "    \"\"\"Generator that yields batches of papers with cs.AI in their categories\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        batch_size: Maximum number of papers per batch\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # First get total count for progress bar\n",
    "    cursor.execute('''\n",
    "        WITH split_categories AS (\n",
    "            SELECT id, abstract\n",
    "            FROM papers\n",
    "            WHERE categories LIKE '%cs.AI%'\n",
    "              AND abstract IS NOT NULL\n",
    "              AND abstract_embedding IS NULL\n",
    "        )\n",
    "        SELECT COUNT(*) FROM split_categories\n",
    "    ''')\n",
    "    total_papers = cursor.fetchone()[0]\n",
    "    \n",
    "    # Then fetch papers in batches\n",
    "    cursor.execute('''\n",
    "        WITH split_categories AS (\n",
    "            SELECT id, abstract\n",
    "            FROM papers\n",
    "            WHERE categories LIKE '%cs.AI%'\n",
    "              AND abstract IS NOT NULL\n",
    "              AND abstract_embedding IS NULL\n",
    "        )\n",
    "        SELECT id, abstract FROM split_categories\n",
    "    ''')\n",
    "    \n",
    "    batch = []\n",
    "    with tqdm(total=total_papers, desc=\"Processing papers\", unit=\" papers\") as pbar:\n",
    "        for row in cursor:\n",
    "            batch.append((row['id'], row['abstract']))\n",
    "            if len(batch) >= batch_size:\n",
    "                pbar.update(len(batch))\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if batch:\n",
    "            pbar.update(len(batch))\n",
    "            yield batch\n",
    "\n",
    "def adjust_batch_for_token_limit(batch: List[Tuple[str, str]], model: str = \"voyage-3-large\") -> List[List[Tuple[str, str]]]:\n",
    "    \"\"\"Split a batch into sub-batches that respect the token limit\n",
    "    \n",
    "    Args:\n",
    "        batch: List of (id, abstract) tuples\n",
    "        model: Voyage AI model to use\n",
    "    \n",
    "    Returns:\n",
    "        List of batches, each respecting the token limit\n",
    "    \"\"\"\n",
    "    TOKEN_LIMIT = 120_000  # voyage-3-large limit\n",
    "    \n",
    "    abstracts = [item[1] for item in batch]\n",
    "    token_counts = [vo.count_tokens([abstract], model=model) for abstract in abstracts]\n",
    "    \n",
    "    sub_batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for (paper_id, abstract), token_count in zip(batch, token_counts):\n",
    "        # If single abstract exceeds limit, skip it\n",
    "        if token_count > TOKEN_LIMIT:\n",
    "            print(f\"Warning: Abstract {paper_id} exceeds token limit ({token_count} tokens), skipping\")\n",
    "            continue\n",
    "            \n",
    "        # If adding this abstract would exceed limit, start new batch\n",
    "        if current_tokens + token_count > TOKEN_LIMIT:\n",
    "            if current_batch:\n",
    "                sub_batches.append(current_batch)\n",
    "            current_batch = [(paper_id, abstract)]\n",
    "            current_tokens = token_count\n",
    "        else:\n",
    "            current_batch.append((paper_id, abstract))\n",
    "            current_tokens += token_count\n",
    "    \n",
    "    if current_batch:\n",
    "        sub_batches.append(current_batch)\n",
    "    \n",
    "    return sub_batches\n",
    "\n",
    "def process_batch(batch: List[Tuple[str, str]], model: str = \"voyage-3-large\") -> List[Tuple[str, List[float]]]:\n",
    "    \"\"\"Process a batch of papers, returning embeddings\"\"\"\n",
    "    abstracts = [item[1] for item in batch]\n",
    "    paper_ids = [item[0] for item in batch]\n",
    "    \n",
    "    # First count tokens for rate limiting\n",
    "    try:\n",
    "        token_count = vo.count_tokens(abstracts, model=model)\n",
    "    except Exception as e:\n",
    "        print(f\"Error counting tokens: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    # Check rate limits\n",
    "    if not rate_limiter.can_make_request(token_count):\n",
    "        raise Exception(\"Rate limit exceeded\")  # This will trigger exponential backoff\n",
    "    \n",
    "    # Generate embeddings with exponential backoff\n",
    "    try:\n",
    "        result = embed_with_backoff(abstracts, model=model)\n",
    "        \n",
    "        if isinstance(result, float):\n",
    "            raise ValueError(f\"API returned float instead of EmbeddingsObject: {result}\")\n",
    "            \n",
    "        if not hasattr(result, 'embeddings'):\n",
    "            raise ValueError(f\"Unexpected response from vo.embed: {result}\")\n",
    "            \n",
    "        # Return list of (paper_id, embedding) tuples and total tokens used\n",
    "        embeddings = result.embeddings\n",
    "        rate_limiter.add_request(token_count)\n",
    "        \n",
    "        return [(paper_id, embedding) for paper_id, embedding in zip(paper_ids, embeddings)], result.total_tokens\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"\\nError in vo.embed call:\")\n",
    "        print(f\"Exception type: {type(e)}\")\n",
    "        print(f\"Exception args: {e.args}\")\n",
    "        print(f\"Full exception: {repr(e)}\")\n",
    "        print(\"\\nInput that caused error:\")\n",
    "        print(f\"First abstract: {abstracts[0][:500]}...\")  # Truncate long abstracts\n",
    "        raise\n",
    "\n",
    "# Process in batches\n",
    "total_updated = 0\n",
    "batches_processed = 0\n",
    "total_tokens = 0\n",
    "\n",
    "try:\n",
    "    for batch in get_csai_papers(conn, batch_size):\n",
    "        if max_batches is not None and batches_processed >= max_batches:\n",
    "            print(f\"\\nStopping after {max_batches} batch(es) as requested\")\n",
    "            break\n",
    "            \n",
    "        # Split batch if needed to respect token limit\n",
    "        sub_batches = adjust_batch_for_token_limit(batch)\n",
    "        \n",
    "        for i, sub_batch in enumerate(sub_batches):\n",
    "            try:\n",
    "                # Process batch and get embeddings\n",
    "                results, batch_tokens = process_batch(sub_batch)\n",
    "                if not isinstance(results, list):\n",
    "                    print(f\"Warning: results is not a list, got {type(results)}\")\n",
    "                    print(f\"Results value: {results}\")\n",
    "                    raise ValueError(f\"Expected list result, got {type(results)}\")\n",
    "                \n",
    "                # Store in database\n",
    "                cursor = conn.cursor()\n",
    "                for paper_id, embedding in results:\n",
    "                    embedding_blob = np.array(embedding, dtype=np.float32).tobytes()\n",
    "                    cursor.execute('''\n",
    "                        UPDATE papers\n",
    "                        SET abstract_embedding = ?\n",
    "                        WHERE id = ?\n",
    "                    ''', (embedding_blob, paper_id))\n",
    "                \n",
    "                conn.commit()\n",
    "                total_updated += len(sub_batch)\n",
    "                \n",
    "                # Update total tokens\n",
    "                total_tokens += batch_tokens\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nFatal error processing sub-batch:\")\n",
    "                print(f\"Exception type: {type(e)}\")\n",
    "                print(f\"Exception args: {e.args}\")\n",
    "                print(f\"Full exception: {repr(e)}\")\n",
    "                print(f\"Sub-batch contents: {sub_batch[:2]}...\")\n",
    "                raise  # Stop execution here\n",
    "        \n",
    "        batches_processed += 1\n",
    "        if max_batches is not None:\n",
    "            print(f\"\\nProcessed {batches_processed}/{max_batches} batch(es)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nFatal error in embedding generation: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    print(f\"\\nProcess completed:\")\n",
    "    print(f\"- Total papers processed: {total_updated}\")\n",
    "    print(f\"- Total batches processed: {batches_processed}\")\n",
    "    print(f\"- Total tokens processed: {total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def validate_embeddings(conn):\n",
    "    \"\"\"Run quality checks on generated embeddings\"\"\"\n",
    "    # Set fixed seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"\\n=== Embedding Quality Checks ===\")\n",
    "    \n",
    "    # 1. Coverage Check\n",
    "    cursor.execute('''\n",
    "        SELECT \n",
    "            COUNT(*) AS total_csai,\n",
    "            SUM(CASE WHEN abstract_embedding IS NOT NULL THEN 1 ELSE 0 END) AS with_embedding,\n",
    "            SUM(CASE WHEN abstract_embedding IS NULL THEN 1 ELSE 0 END) AS without_embedding\n",
    "        FROM papers \n",
    "        WHERE categories LIKE '%cs.AI%'\n",
    "          AND abstract IS NOT NULL\n",
    "    ''')\n",
    "    stats = cursor.fetchone()\n",
    "    print(f\"\\n1. Coverage:\")\n",
    "    print(f\"- Total CS.AI papers: {stats['total_csai']}\")\n",
    "    print(f\"- With embeddings: {stats['with_embedding']} ({(stats['with_embedding']/stats['total_csai'])*100:.1f}%)\")\n",
    "    print(f\"- Missing embeddings: {stats['without_embedding']} ({(stats['without_embedding']/stats['total_csai'])*100:.1f}%)\")\n",
    "\n",
    "    # 2. Validity Check (NaN/Zero vectors)\n",
    "    cursor.execute('''\n",
    "        SELECT abstract_embedding \n",
    "        FROM papers \n",
    "        WHERE abstract_embedding IS NOT NULL\n",
    "    ''')\n",
    "    invalid_count = 0\n",
    "    total_checked = 0\n",
    "    for row in cursor:\n",
    "        embedding = np.frombuffer(row['abstract_embedding'], dtype=np.float32)\n",
    "        if np.isnan(embedding).any():\n",
    "            invalid_count += 1\n",
    "        elif np.all(embedding == 0):\n",
    "            invalid_count += 1\n",
    "        total_checked += 1\n",
    "    \n",
    "    print(f\"\\n2. Validity (sampled {total_checked}):\")\n",
    "    print(f\"- Invalid embeddings (NaN/zeros): {invalid_count} ({(invalid_count/total_checked)*100:.1f}%)\")\n",
    "\n",
    "    # 3. Norm Analysis\n",
    "    cursor.execute('''\n",
    "        SELECT abstract_embedding \n",
    "        FROM papers \n",
    "        WHERE abstract_embedding IS NOT NULL\n",
    "    ''')\n",
    "    norms = []\n",
    "    for row in cursor:\n",
    "        embedding = np.frombuffer(row['abstract_embedding'], dtype=np.float32)\n",
    "        norms.append(np.linalg.norm(embedding))\n",
    "    \n",
    "    print(f\"\\n3. Norm Analysis (sample):\")\n",
    "    print(f\"- Mean norm: {np.mean(norms):.2f}\")\n",
    "    print(f\"- Std dev: {np.std(norms):.2f}\")\n",
    "    print(f\"- Min/Max: {np.min(norms):.2f}/{np.max(norms):.2f}\")\n",
    "\n",
    "    # 4. Similarity Analysis\n",
    "    # Get random pairs more efficiently\n",
    "    cursor.execute('''\n",
    "        SELECT id, title, abstract, abstract_embedding \n",
    "        FROM papers \n",
    "        WHERE abstract_embedding IS NOT NULL\n",
    "          AND categories LIKE '%cs.AI%'\n",
    "          AND ABS(RANDOM() % 100) = 0  -- Fast random sampling\n",
    "        LIMIT 10000\n",
    "    ''')\n",
    "    papers = cursor.fetchall()\n",
    "    random_embeddings = np.vstack([np.frombuffer(row['abstract_embedding'], dtype=np.float32) for row in papers])\n",
    "    \n",
    "    # Analyze embedding components\n",
    "    print(\"\\n4. Embedding Analysis:\")\n",
    "    print(f\"- Shape: {random_embeddings.shape}\")\n",
    "    print(\"- Component statistics:\")\n",
    "    means = np.mean(random_embeddings, axis=0)\n",
    "    stds = np.std(random_embeddings, axis=0)\n",
    "    print(f\"  Mean of means: {np.mean(means):.3f} ± {np.std(means):.3f}\")\n",
    "    print(f\"  Mean of stds: {np.mean(stds):.3f} ± {np.std(stds):.3f}\")\n",
    "    print(f\"  Component range: [{np.min(random_embeddings):.3f}, {np.max(random_embeddings):.3f}]\")\n",
    "    \n",
    "    # Show distribution of components\n",
    "    positive_frac = np.mean(random_embeddings > 0)\n",
    "    print(f\"  Fraction of positive components: {positive_frac:.3f}\")\n",
    "    \n",
    "    # Compute similarities\n",
    "    n_samples = len(random_embeddings)\n",
    "    if n_samples > 1:\n",
    "        # Generate random pairs of indices\n",
    "        idx1 = np.random.randint(0, n_samples, size=1000)\n",
    "        idx2 = np.random.randint(0, n_samples, size=1000)\n",
    "        # Exclude self-pairs\n",
    "        valid_pairs = idx1 != idx2\n",
    "        idx1, idx2 = idx1[valid_pairs], idx2[valid_pairs]\n",
    "        \n",
    "        # Compute cosine similarities properly\n",
    "        similarities = cosine_similarity(random_embeddings[idx1], random_embeddings[idx2]).diagonal()\n",
    "        \n",
    "        print(\"\\n5. Similarity Analysis:\")\n",
    "        print(f\"- Random pairs (n={len(similarities)}):\")\n",
    "        print(f\"  Mean: {np.mean(similarities):.3f} ± {np.std(similarities):.3f}\")\n",
    "        print(f\"  Range: [{np.min(similarities):.3f}, {np.max(similarities):.3f}]\")\n",
    "        \n",
    "        # Print example pairs with different similarity levels\n",
    "        print(\"\\nExample pairs:\")\n",
    "        # Most similar pair\n",
    "        most_similar_idx = np.argmax(similarities)\n",
    "        print(f\"\\nMost similar pair (similarity: {similarities[most_similar_idx]:.3f}):\")\n",
    "        print(f\"Paper 1: {papers[idx1[most_similar_idx]]['title']}\")\n",
    "        print(f\"URL 1: https://arxiv.org/abs/{papers[idx1[most_similar_idx]]['id']}\")\n",
    "        print(f\"Abstract 1:\\n{papers[idx1[most_similar_idx]]['abstract']}\")\n",
    "        print(f\"\\nPaper 2: {papers[idx2[most_similar_idx]]['title']}\")\n",
    "        print(f\"URL 2: https://arxiv.org/abs/{papers[idx2[most_similar_idx]]['id']}\")\n",
    "        print(f\"Abstract 2:\\n{papers[idx2[most_similar_idx]]['abstract']}\")\n",
    "        \n",
    "        # Least similar pair\n",
    "        least_similar_idx = np.argmin(similarities)\n",
    "        print(f\"\\nLeast similar pair (similarity: {similarities[least_similar_idx]:.3f}):\")\n",
    "        print(f\"Paper 1: {papers[idx1[least_similar_idx]]['title']}\")\n",
    "        print(f\"URL 1: https://arxiv.org/abs/{papers[idx1[least_similar_idx]]['id']}\")\n",
    "        print(f\"Abstract 1:\\n{papers[idx1[least_similar_idx]]['abstract']}\")\n",
    "        print(f\"\\nPaper 2: {papers[idx2[least_similar_idx]]['title']}\")\n",
    "        print(f\"URL 2: https://arxiv.org/abs/{papers[idx2[least_similar_idx]]['id']}\")\n",
    "        print(f\"Abstract 2:\\n{papers[idx2[least_similar_idx]]['abstract']}\")\n",
    "        \n",
    "        # Median similarity pair\n",
    "        median_idx = np.argsort(similarities)[len(similarities)//2]\n",
    "        print(f\"\\nMedian similarity pair (similarity: {similarities[median_idx]:.3f}):\")\n",
    "        print(f\"Paper 1: {papers[idx1[median_idx]]['title']}\")\n",
    "        print(f\"URL 1: https://arxiv.org/abs/{papers[idx1[median_idx]]['id']}\")\n",
    "        print(f\"Abstract 1:\\n{papers[idx1[median_idx]]['abstract']}\")\n",
    "        print(f\"\\nPaper 2: {papers[idx2[median_idx]]['title']}\")\n",
    "        print(f\"URL 2: https://arxiv.org/abs/{papers[idx2[median_idx]]['id']}\")\n",
    "        print(f\"Abstract 2:\\n{papers[idx2[median_idx]]['abstract']}\")\n",
    "    else:\n",
    "        print(\"\\n5. Similarity Analysis:\")\n",
    "        print(\"Not enough samples for similarity analysis\")\n",
    "    \n",
    "    # 5. Embedding Analysis\n",
    "    print(\"\\n5. Embedding Analysis:\")\n",
    "    \n",
    "    def find_outliers(embeddings: np.ndarray, papers: list, n_examples: int = 3):\n",
    "        \"\"\"Find outlier papers based on their similarity to other papers\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Array of embeddings (each normalized to unit length)\n",
    "            papers: List of paper records corresponding to embeddings\n",
    "            n_examples: Number of outlier examples to show\n",
    "        \"\"\"\n",
    "        # Calculate pairwise similarities\n",
    "        similarities = cosine_similarity(embeddings)\n",
    "        \n",
    "        # For each paper, get average similarity to other papers\n",
    "        # Exclude self-similarity (1.0) from the average\n",
    "        avg_similarities = []\n",
    "        for i in range(len(similarities)):\n",
    "            others = np.concatenate([similarities[i,:i], similarities[i,i+1:]])\n",
    "            avg_similarities.append(np.mean(others))\n",
    "        \n",
    "        avg_similarities = np.array(avg_similarities)\n",
    "        \n",
    "        # Use IQR method on average similarities\n",
    "        q1, q3 = np.percentile(avg_similarities, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr  # Papers with unusually low average similarity\n",
    "        \n",
    "        # Find outliers (papers with unusually low average similarity to others)\n",
    "        outlier_indices = np.where(avg_similarities < lower_bound)[0]\n",
    "        \n",
    "        print(f\"\\nOutlier Analysis:\")\n",
    "        print(f\"- Average similarity stats:\")\n",
    "        print(f\"  Mean: {np.mean(avg_similarities):.3f} ± {np.std(avg_similarities):.3f}\")\n",
    "        print(f\"  Range: [{np.min(avg_similarities):.3f}, {np.max(avg_similarities):.3f}]\")\n",
    "        print(f\"- Found {len(outlier_indices)} outliers ({len(outlier_indices)/len(embeddings)*100:.1f}%)\")\n",
    "        \n",
    "        if len(outlier_indices) > 0:\n",
    "            # Sort outliers by average similarity (ascending)\n",
    "            sorted_outliers = sorted(zip(outlier_indices, avg_similarities[outlier_indices]), \n",
    "                                   key=lambda x: x[1])\n",
    "            \n",
    "            print(f\"\\nTop {n_examples} outliers (lowest average similarity to other papers):\")\n",
    "            for idx, avg_sim in sorted_outliers[:n_examples]:\n",
    "                paper = papers[idx]\n",
    "                print(f\"\\nAverage similarity to other papers: {avg_sim:.3f}\")\n",
    "                print(f\"Title: {paper['title']}\")\n",
    "                print(f\"URL: https://arxiv.org/abs/{paper['id']}\")\n",
    "                print(f\"Abstract:\\n{paper['abstract'][:500]}...\")\n",
    "                \n",
    "                # Show a few most and least similar papers to this one\n",
    "                sims = similarities[idx]\n",
    "                most_similar_idx = np.argsort(sims)[-2]  # -1 would be self\n",
    "                least_similar_idx = np.argsort(sims)[0]\n",
    "                \n",
    "                print(f\"\\nMost similar paper (similarity: {sims[most_similar_idx]:.3f}):\")\n",
    "                print(f\"Title: {papers[most_similar_idx]['title']}\")\n",
    "                \n",
    "                print(f\"\\nLeast similar paper (similarity: {sims[least_similar_idx]:.3f}):\")\n",
    "                print(f\"Title: {papers[least_similar_idx]['title']}\")\n",
    "    \n",
    "    # Get sample of embeddings\n",
    "    cursor.execute('''\n",
    "        SELECT id, title, abstract, abstract_embedding\n",
    "        FROM papers \n",
    "        WHERE abstract_embedding IS NOT NULL\n",
    "        LIMIT 1000\n",
    "    ''')\n",
    "    papers = cursor.fetchall()\n",
    "    embeddings = [np.frombuffer(row['abstract_embedding'], dtype=np.float32) for row in papers]\n",
    "    all_embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Basic embedding stats\n",
    "    print(f\"- Shape: {all_embeddings.shape}\")\n",
    "    print(\"- Component statistics:\")\n",
    "    means = np.mean(all_embeddings, axis=0)\n",
    "    stds = np.std(all_embeddings, axis=0)\n",
    "    print(f\"  Mean of means: {np.mean(means):.3f} ± {np.std(means):.3f}\")\n",
    "    print(f\"  Mean of stds: {np.mean(stds):.3f} ± {np.std(stds):.3f}\")\n",
    "    print(f\"  Component range: [{np.min(all_embeddings):.3f}, {np.max(all_embeddings):.3f}]\")\n",
    "    \n",
    "    # Show distribution of components\n",
    "    positive_frac = np.mean(all_embeddings > 0)\n",
    "    print(f\"  Fraction of positive components: {positive_frac:.3f}\")\n",
    "    \n",
    "    # Find outliers using the new method\n",
    "    find_outliers(all_embeddings, papers)\n",
    "\n",
    "# Run validation\n",
    "validate_embeddings(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. Database Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Copy updated database back to Drive\n",
    "!cp {local_db} \"{db_path}\"\n",
    "print(\"Database backup completed to Google Drive\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
