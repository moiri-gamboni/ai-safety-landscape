{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Papers - Abstract Embedding Phase\n",
    "\n",
    "This notebook generates Voyage AI embeddings for paper abstracts and stores them in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Clone repository if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Clone the repository\n",
    "    !git clone https://github.com/moiri-gamboni/ai-safety-landscape.git\n",
    "    %cd ai-safety-landscape\n",
    "\n",
    "# Install required packages\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import voyageai\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "# Load API key - try Colab form first, then .env\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    print(\"Getting Voyage AI API key from Colab form\")\n",
    "    # @title Voyage AI API Key\n",
    "    voyage_api_key = \"\" # @param {type:\"string\"}\n",
    "    # Set it for the client\n",
    "    voyageai.api_key = voyage_api_key\n",
    "else:\n",
    "    print(\"Running locally, loading API key from .env\")\n",
    "    load_dotenv()\n",
    "\n",
    "# Initialize Voyage AI client\n",
    "vo = voyageai.Client()\n",
    "\n",
    "# Path to database\n",
    "db_path = \"/content/drive/MyDrive/ai-safety-papers/papers.db\"\n",
    "local_db = \"papers.db\"\n",
    "\n",
    "# Copy database to local storage if needed\n",
    "print(f\"Copying database to local storage: {local_db}\")\n",
    "if not os.path.exists(local_db):\n",
    "    %cp \"{db_path}\" {local_db}\n",
    "\n",
    "conn = sqlite3.connect(local_db)\n",
    "conn.row_factory = sqlite3.Row\n",
    "\n",
    "# Check if abstract_embedding and token_count columns exist\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"PRAGMA table_info(papers)\")\n",
    "columns = [column[1] for column in cursor.fetchall()]\n",
    "\n",
    "if 'abstract_embedding' not in columns:\n",
    "    print(\"Adding abstract_embedding column...\")\n",
    "    conn.execute('''\n",
    "        ALTER TABLE papers \n",
    "        ADD COLUMN abstract_embedding BLOB\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "if 'token_count' not in columns:\n",
    "    print(\"Adding token_count column...\")\n",
    "    conn.execute('''\n",
    "        ALTER TABLE papers \n",
    "        ADD COLUMN token_count INTEGER\n",
    "    ''')\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "max_batches = 1  # Set to None to process all batches, or a number to limit processing\n",
    "batch_size = 128  # Maximum batch size for Voyage AI\n",
    "\n",
    "# Rate limit tracking\n",
    "class RateLimiter:\n",
    "    def __init__(self, rpm_limit: int = 2000, tpm_limit: int = 8_000_000, window_seconds: int = 60):\n",
    "        self.rpm_limit = rpm_limit\n",
    "        self.tpm_limit = tpm_limit\n",
    "        self.window_seconds = window_seconds\n",
    "        self.requests = []\n",
    "        self.tokens = []\n",
    "    \n",
    "    def add_request(self, token_count: int = 0):\n",
    "        current_time = time.time()\n",
    "        # Clean up old entries\n",
    "        cutoff_time = current_time - self.window_seconds\n",
    "        self.requests = [t for t in self.requests if t > cutoff_time]\n",
    "        self.tokens = [t for t, _ in self.tokens if t > cutoff_time]\n",
    "        \n",
    "        # Add new request\n",
    "        self.requests.append(current_time)\n",
    "        if token_count > 0:\n",
    "            self.tokens.append((current_time, token_count))\n",
    "    \n",
    "    def can_make_request(self, token_count: int = 0) -> Tuple[bool, Optional[float]]:\n",
    "        current_time = time.time()\n",
    "        cutoff_time = current_time - self.window_seconds\n",
    "        \n",
    "        # Clean up old entries\n",
    "        self.requests = [t for t in self.requests if t > cutoff_time]\n",
    "        self.tokens = [(t, c) for t, c in self.tokens if t > cutoff_time]\n",
    "        \n",
    "        # Check RPM limit\n",
    "        if len(self.requests) >= self.rpm_limit:\n",
    "            wait_time = self.requests[0] - cutoff_time\n",
    "            return False, wait_time\n",
    "        \n",
    "        # Check TPM limit\n",
    "        current_tokens = sum(count for _, count in self.tokens)\n",
    "        if current_tokens + token_count > self.tpm_limit:\n",
    "            # Find the earliest time when we'll be under the limit\n",
    "            sorted_tokens = sorted(self.tokens)\n",
    "            running_sum = current_tokens + token_count\n",
    "            for t, c in sorted_tokens:\n",
    "                running_sum -= c\n",
    "                if running_sum <= self.tpm_limit:\n",
    "                    wait_time = t - cutoff_time\n",
    "                    return False, wait_time\n",
    "        \n",
    "        return True, None\n",
    "\n",
    "rate_limiter = RateLimiter()\n",
    "\n",
    "def get_csai_papers(conn: sqlite3.Connection, batch_size: int = 128):\n",
    "    \"\"\"Generator that yields batches of papers with cs.AI in their categories\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        batch_size: Maximum number of papers per batch\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # First get total count for progress bar\n",
    "    cursor.execute('''\n",
    "        WITH split_categories AS (\n",
    "            SELECT id, abstract\n",
    "            FROM papers\n",
    "            WHERE categories LIKE '%cs.AI%'\n",
    "              AND abstract IS NOT NULL\n",
    "              AND abstract_embedding IS NULL\n",
    "        )\n",
    "        SELECT COUNT(*) FROM split_categories\n",
    "    ''')\n",
    "    total_papers = cursor.fetchone()[0]\n",
    "    \n",
    "    # Then fetch papers in batches\n",
    "    cursor.execute('''\n",
    "        WITH split_categories AS (\n",
    "            SELECT id, abstract\n",
    "            FROM papers\n",
    "            WHERE categories LIKE '%cs.AI%'\n",
    "              AND abstract IS NOT NULL\n",
    "              AND abstract_embedding IS NULL\n",
    "        )\n",
    "        SELECT id, abstract FROM split_categories\n",
    "    ''')\n",
    "    \n",
    "    batch = []\n",
    "    with tqdm(total=total_papers, desc=\"Processing papers\", unit=\" papers\") as pbar:\n",
    "        for row in cursor:\n",
    "            batch.append((row['id'], row['abstract']))\n",
    "            if len(batch) >= batch_size:\n",
    "                pbar.update(len(batch))\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if batch:\n",
    "            pbar.update(len(batch))\n",
    "            yield batch\n",
    "\n",
    "def adjust_batch_for_token_limit(batch: List[Tuple[str, str]], model: str = \"voyage-3-large\") -> List[List[Tuple[str, str]]]:\n",
    "    \"\"\"Split a batch into sub-batches that respect the token limit\n",
    "    \n",
    "    Args:\n",
    "        batch: List of (id, abstract) tuples\n",
    "        model: Voyage AI model to use\n",
    "    \n",
    "    Returns:\n",
    "        List of batches, each respecting the token limit\n",
    "    \"\"\"\n",
    "    TOKEN_LIMIT = 120_000  # voyage-3-large limit\n",
    "    \n",
    "    abstracts = [item[1] for item in batch]\n",
    "    token_counts = [vo.count_tokens([abstract], model=model) for abstract in abstracts]\n",
    "    \n",
    "    sub_batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for (paper_id, abstract), token_count in zip(batch, token_counts):\n",
    "        # If single abstract exceeds limit, skip it\n",
    "        if token_count > TOKEN_LIMIT:\n",
    "            print(f\"Warning: Abstract {paper_id} exceeds token limit ({token_count} tokens), skipping\")\n",
    "            continue\n",
    "            \n",
    "        # If adding this abstract would exceed limit, start new batch\n",
    "        if current_tokens + token_count > TOKEN_LIMIT:\n",
    "            if current_batch:\n",
    "                sub_batches.append(current_batch)\n",
    "            current_batch = [(paper_id, abstract)]\n",
    "            current_tokens = token_count\n",
    "        else:\n",
    "            current_batch.append((paper_id, abstract))\n",
    "            current_tokens += token_count\n",
    "    \n",
    "    if current_batch:\n",
    "        sub_batches.append(current_batch)\n",
    "    \n",
    "    return sub_batches\n",
    "\n",
    "def process_batch(batch: List[Tuple[str, str]], model: str = \"voyage-3-large\") -> List[Tuple[str, List[float], int]]:\n",
    "    \"\"\"Process a batch of papers, returning embeddings and token counts\n",
    "    \n",
    "    Args:\n",
    "        batch: List of (id, abstract) tuples\n",
    "        model: Voyage AI model to use\n",
    "    \n",
    "    Returns:\n",
    "        List of (id, embedding, token_count) tuples\n",
    "    \"\"\"\n",
    "    abstracts = [item[1] for item in batch]\n",
    "    \n",
    "    # First count tokens\n",
    "    token_count = vo.count_tokens(abstracts, model=model)\n",
    "    \n",
    "    # Check rate limits and wait if necessary\n",
    "    can_proceed, wait_time = rate_limiter.can_make_request(token_count)\n",
    "    if not can_proceed:\n",
    "        print(f\"\\nRate limit reached, waiting {wait_time:.1f} seconds...\")\n",
    "        time.sleep(wait_time + 0.1)  # Add small buffer\n",
    "    \n",
    "    # Generate embeddings with 2048 dimensions\n",
    "    result = vo.embed(\n",
    "        abstracts,\n",
    "        model=model,\n",
    "        input_type=\"document\",\n",
    "        output_dimension=2048\n",
    "    )\n",
    "    \n",
    "    # Update rate limiter\n",
    "    rate_limiter.add_request(token_count)\n",
    "    \n",
    "    # Return list of (id, embedding, token_count) tuples\n",
    "    return list(zip(\n",
    "        [item[0] for item in batch],\n",
    "        result.embeddings,\n",
    "        [token_count] * len(batch)  # Each abstract's token count\n",
    "    ))\n",
    "\n",
    "# Process in batches\n",
    "total_updated = 0\n",
    "batches_processed = 0\n",
    "\n",
    "try:\n",
    "    for batch in get_csai_papers(conn, batch_size):\n",
    "        if max_batches is not None and batches_processed >= max_batches:\n",
    "            print(f\"\\nStopping after {max_batches} batch(es) as requested\")\n",
    "            break\n",
    "            \n",
    "        # Split batch if needed to respect token limit\n",
    "        sub_batches = adjust_batch_for_token_limit(batch)\n",
    "        \n",
    "        for sub_batch in sub_batches:\n",
    "            try:\n",
    "                # Process batch and get embeddings\n",
    "                results = process_batch(sub_batch)\n",
    "                \n",
    "                # Store in database\n",
    "                cursor = conn.cursor()\n",
    "                for paper_id, embedding, token_count in results:\n",
    "                    embedding_blob = np.array(embedding, dtype=np.float32).tobytes()\n",
    "                    cursor.execute('''\n",
    "                        UPDATE papers\n",
    "                        SET abstract_embedding = ?,\n",
    "                            token_count = ?\n",
    "                        WHERE id = ?\n",
    "                    ''', (embedding_blob, token_count, paper_id))\n",
    "                \n",
    "                conn.commit()\n",
    "                total_updated += len(sub_batch)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing batch: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        batches_processed += 1\n",
    "        if max_batches is not None:\n",
    "            print(f\"\\nProcessed {batches_processed}/{max_batches} batch(es)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nFatal error in embedding generation: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    print(f\"\\nProcess completed:\")\n",
    "    print(f\"- Total papers processed: {total_updated}\")\n",
    "    print(f\"- Total batches processed: {batches_processed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def validate_embeddings(conn):\n",
    "    \"\"\"Run quality checks on generated embeddings\"\"\"\n",
    "    # Set fixed seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"\\n=== Embedding Quality Checks ===\")\n",
    "    \n",
    "    # 1. Coverage Check\n",
    "    cursor.execute('''\n",
    "        SELECT \n",
    "            COUNT(*) AS total_csai,\n",
    "            SUM(CASE WHEN abstract_embedding IS NOT NULL THEN 1 ELSE 0 END) AS with_embedding,\n",
    "            SUM(CASE WHEN abstract_embedding IS NULL THEN 1 ELSE 0 END) AS without_embedding\n",
    "        FROM papers \n",
    "        WHERE categories LIKE '%cs.AI%'\n",
    "          AND abstract IS NOT NULL\n",
    "    ''')\n",
    "    stats = cursor.fetchone()\n",
    "    print(f\"\\n1. Coverage:\")\n",
    "    print(f\"- Total CS.AI papers: {stats['total_csai']}\")\n",
    "    print(f\"- With embeddings: {stats['with_embedding']} ({(stats['with_embedding']/stats['total_csai'])*100:.1f}%)\")\n",
    "    print(f\"- Missing embeddings: {stats['without_embedding']} ({(stats['without_embedding']/stats['total_csai'])*100:.1f}%)\")\n",
    "\n",
    "    # 2. Validity Check (NaN/Zero vectors)\n",
    "    cursor.execute('''\n",
    "        SELECT abstract_embedding \n",
    "        FROM papers \n",
    "        WHERE abstract_embedding IS NOT NULL\n",
    "    ''')\n",
    "    invalid_count = 0\n",
    "    total_checked = 0\n",
    "    for row in cursor:\n",
    "        embedding = np.frombuffer(row['abstract_embedding'], dtype=np.float32)\n",
    "        if np.isnan(embedding).any():\n",
    "            invalid_count += 1\n",
    "        elif np.all(embedding == 0):\n",
    "            invalid_count += 1\n",
    "        total_checked += 1\n",
    "    \n",
    "    print(f\"\\n2. Validity (sampled {total_checked}):\")\n",
    "    print(f\"- Invalid embeddings (NaN/zeros): {invalid_count} ({(invalid_count/total_checked)*100:.1f}%)\")\n",
    "\n",
    "    # 3. Norm Analysis\n",
    "    cursor.execute('''\n",
    "        SELECT abstract_embedding \n",
    "        FROM papers \n",
    "        WHERE abstract_embedding IS NOT NULL\n",
    "    ''')\n",
    "    norms = []\n",
    "    for row in cursor:\n",
    "        embedding = np.frombuffer(row['abstract_embedding'], dtype=np.float32)\n",
    "        norms.append(np.linalg.norm(embedding))\n",
    "    \n",
    "    print(f\"\\n3. Norm Analysis (sample):\")\n",
    "    print(f\"- Mean norm: {np.mean(norms):.2f}\")\n",
    "    print(f\"- Std dev: {np.std(norms):.2f}\")\n",
    "    print(f\"- Min/Max: {np.min(norms):.2f}/{np.max(norms):.2f}\")\n",
    "\n",
    "    # 4. Similarity Analysis\n",
    "    # Get random pairs more efficiently\n",
    "    cursor.execute('''\n",
    "        SELECT id, title, abstract, abstract_embedding \n",
    "        FROM papers \n",
    "        WHERE abstract_embedding IS NOT NULL\n",
    "          AND categories LIKE '%cs.AI%'\n",
    "          AND ABS(RANDOM() % 100) = 0  -- Fast random sampling\n",
    "        LIMIT 10000\n",
    "    ''')\n",
    "    papers = cursor.fetchall()\n",
    "    random_embeddings = np.vstack([np.frombuffer(row['abstract_embedding'], dtype=np.float32) for row in papers])\n",
    "    \n",
    "    # Analyze embedding components\n",
    "    print(\"\\n4. Embedding Analysis:\")\n",
    "    print(f\"- Shape: {random_embeddings.shape}\")\n",
    "    print(\"- Component statistics:\")\n",
    "    means = np.mean(random_embeddings, axis=0)\n",
    "    stds = np.std(random_embeddings, axis=0)\n",
    "    print(f\"  Mean of means: {np.mean(means):.3f} ± {np.std(means):.3f}\")\n",
    "    print(f\"  Mean of stds: {np.mean(stds):.3f} ± {np.std(stds):.3f}\")\n",
    "    print(f\"  Component range: [{np.min(random_embeddings):.3f}, {np.max(random_embeddings):.3f}]\")\n",
    "    \n",
    "    # Show distribution of components\n",
    "    positive_frac = np.mean(random_embeddings > 0)\n",
    "    print(f\"  Fraction of positive components: {positive_frac:.3f}\")\n",
    "    \n",
    "    # Compute similarities\n",
    "    n_samples = len(random_embeddings)\n",
    "    if n_samples > 1:\n",
    "        # Generate random pairs of indices\n",
    "        idx1 = np.random.randint(0, n_samples, size=1000)\n",
    "        idx2 = np.random.randint(0, n_samples, size=1000)\n",
    "        # Exclude self-pairs\n",
    "        valid_pairs = idx1 != idx2\n",
    "        idx1, idx2 = idx1[valid_pairs], idx2[valid_pairs]\n",
    "        \n",
    "        # Compute cosine similarities properly\n",
    "        similarities = cosine_similarity(random_embeddings[idx1], random_embeddings[idx2]).diagonal()\n",
    "        \n",
    "        print(\"\\n5. Similarity Analysis:\")\n",
    "        print(f\"- Random pairs (n={len(similarities)}):\")\n",
    "        print(f\"  Mean: {np.mean(similarities):.3f} ± {np.std(similarities):.3f}\")\n",
    "        print(f\"  Range: [{np.min(similarities):.3f}, {np.max(similarities):.3f}]\")\n",
    "        \n",
    "        # Print example pairs with different similarity levels\n",
    "        print(\"\\nExample pairs:\")\n",
    "        # Most similar pair\n",
    "        most_similar_idx = np.argmax(similarities)\n",
    "        print(f\"\\nMost similar pair (similarity: {similarities[most_similar_idx]:.3f}):\")\n",
    "        print(f\"Paper 1: {papers[idx1[most_similar_idx]]['title']}\")\n",
    "        print(f\"URL 1: https://arxiv.org/abs/{papers[idx1[most_similar_idx]]['id']}\")\n",
    "        print(f\"Abstract 1:\\n{papers[idx1[most_similar_idx]]['abstract']}\")\n",
    "        print(f\"\\nPaper 2: {papers[idx2[most_similar_idx]]['title']}\")\n",
    "        print(f\"URL 2: https://arxiv.org/abs/{papers[idx2[most_similar_idx]]['id']}\")\n",
    "        print(f\"Abstract 2:\\n{papers[idx2[most_similar_idx]]['abstract']}\")\n",
    "        \n",
    "        # Least similar pair\n",
    "        least_similar_idx = np.argmin(similarities)\n",
    "        print(f\"\\nLeast similar pair (similarity: {similarities[least_similar_idx]:.3f}):\")\n",
    "        print(f\"Paper 1: {papers[idx1[least_similar_idx]]['title']}\")\n",
    "        print(f\"URL 1: https://arxiv.org/abs/{papers[idx1[least_similar_idx]]['id']}\")\n",
    "        print(f\"Abstract 1:\\n{papers[idx1[least_similar_idx]]['abstract']}\")\n",
    "        print(f\"\\nPaper 2: {papers[idx2[least_similar_idx]]['title']}\")\n",
    "        print(f\"URL 2: https://arxiv.org/abs/{papers[idx2[least_similar_idx]]['id']}\")\n",
    "        print(f\"Abstract 2:\\n{papers[idx2[least_similar_idx]]['abstract']}\")\n",
    "        \n",
    "        # Median similarity pair\n",
    "        median_idx = np.argsort(similarities)[len(similarities)//2]\n",
    "        print(f\"\\nMedian similarity pair (similarity: {similarities[median_idx]:.3f}):\")\n",
    "        print(f\"Paper 1: {papers[idx1[median_idx]]['title']}\")\n",
    "        print(f\"URL 1: https://arxiv.org/abs/{papers[idx1[median_idx]]['id']}\")\n",
    "        print(f\"Abstract 1:\\n{papers[idx1[median_idx]]['abstract']}\")\n",
    "        print(f\"\\nPaper 2: {papers[idx2[median_idx]]['title']}\")\n",
    "        print(f\"URL 2: https://arxiv.org/abs/{papers[idx2[median_idx]]['id']}\")\n",
    "        print(f\"Abstract 2:\\n{papers[idx2[median_idx]]['abstract']}\")\n",
    "    else:\n",
    "        print(\"\\n5. Similarity Analysis:\")\n",
    "        print(\"Not enough samples for similarity analysis\")\n",
    "    \n",
    "    # # Get potential duplicates (papers with same title)\n",
    "    # cursor.execute('''\n",
    "    #     SELECT p1.id as id1, p2.id as id2,\n",
    "    #            p1.title as title1,\n",
    "    #            p1.abstract as abstract1,\n",
    "    #            p2.abstract as abstract2,\n",
    "    #            p1.abstract_embedding as e1,\n",
    "    #            p2.abstract_embedding as e2\n",
    "    #     FROM papers p1\n",
    "    #     JOIN papers p2 ON LOWER(TRIM(p1.title)) = LOWER(TRIM(p2.title))\n",
    "    #     WHERE p1.id < p2.id  -- Avoid self-joins and duplicates\n",
    "    #       AND p1.withdrawn = 0 AND p2.withdrawn = 0\n",
    "    #       AND p1.abstract_embedding IS NOT NULL\n",
    "    #       AND p2.abstract_embedding IS NOT NULL\n",
    "    #       AND p1.categories LIKE '%cs.AI%'\n",
    "    #       AND p2.categories LIKE '%cs.AI%'\n",
    "    #     LIMIT 50\n",
    "    # ''')\n",
    "    \n",
    "    # rows = cursor.fetchall()\n",
    "    # if rows:\n",
    "    #     # Process all pairs at once\n",
    "    #     e1 = np.vstack([np.frombuffer(row['e1'], dtype=np.float32) for row in rows])\n",
    "    #     e2 = np.vstack([np.frombuffer(row['e2'], dtype=np.float32) for row in rows])\n",
    "    #     similarities = np.sum(e1 * e2, axis=1)\n",
    "        \n",
    "    #     print(f\"\\n6. Same-title Analysis:\")\n",
    "    #     print(f\"- Same-title pairs (n={len(similarities)}):\")\n",
    "    #     print(f\"  Mean: {np.mean(similarities):.3f} ± {np.std(similarities):.3f}\")\n",
    "    #     print(f\"  Range: [{np.min(similarities):.3f}, {np.max(similarities):.3f}]\")\n",
    "        \n",
    "    #     # Print example same-title pairs\n",
    "    #     print(\"\\nExample same-title pairs:\")\n",
    "    #     # Most similar pair\n",
    "    #     most_similar_idx = np.argmax(similarities)\n",
    "    #     print(f\"\\nMost similar pair (similarity: {similarities[most_similar_idx]:.3f}):\")\n",
    "    #     print(f\"Title: {rows[most_similar_idx]['title1']}\")\n",
    "    #     print(f\"URL 1: https://arxiv.org/abs/{rows[most_similar_idx]['id1']}\")\n",
    "    #     print(f\"Abstract 1:\\n{rows[most_similar_idx]['abstract1']}\")\n",
    "    #     print(f\"\\nURL 2: https://arxiv.org/abs/{rows[most_similar_idx]['id2']}\")\n",
    "    #     print(f\"Abstract 2:\\n{rows[most_similar_idx]['abstract2']}\")\n",
    "        \n",
    "    #     # Least similar pair\n",
    "    #     least_similar_idx = np.argmin(similarities)\n",
    "    #     print(f\"\\nLeast similar pair (similarity: {similarities[least_similar_idx]:.3f}):\")\n",
    "    #     print(f\"Title: {rows[least_similar_idx]['title1']}\")\n",
    "    #     print(f\"URL 1: https://arxiv.org/abs/{rows[least_similar_idx]['id1']}\")\n",
    "    #     print(f\"Abstract 1:\\n{rows[least_similar_idx]['abstract1']}\")\n",
    "    #     print(f\"\\nURL 2: https://arxiv.org/abs/{rows[least_similar_idx]['id2']}\")\n",
    "    #     print(f\"Abstract 2:\\n{rows[least_similar_idx]['abstract2']}\")\n",
    "    # else:\n",
    "    #     print(\"\\n6. Same-title Analysis:\")\n",
    "    #     print(\"No same-title pairs found for comparison\")\n",
    "\n",
    "# Run validation\n",
    "validate_embeddings(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. Database Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Copy updated database back to Drive\n",
    "!cp {local_db} \"{db_path}\"\n",
    "print(\"Database backup completed to Google Drive\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
