{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Papers - Embedding Phase\n",
    "\n",
    "This notebook generates Voyage AI embeddings for paper titles and abstracts and stores them in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive # pyright: ignore [reportMissingImports]\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Install and configure PostgreSQL\n",
    "    !sudo apt-get -qq update && sudo apt-get -qq install postgresql postgresql-contrib # pyright: ignore\n",
    "    !sudo service postgresql start # pyright: ignore\n",
    "    !sudo sed -i 's/local\\s*all\\s*postgres\\s*peer/local all postgres trust/' /etc/postgresql/14/main/pg_hba.conf # pyright: ignore\n",
    "    !sudo service postgresql restart # pyright: ignore\n",
    "    \n",
    "    # Install Python client\n",
    "    %pip install psycopg2-binary # pyright: ignore\n",
    "    %pip install voyageai tqdm scikit-learn tenacity # pyright: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import voyageai\n",
    "import time\n",
    "from typing import List, Tuple, Optional\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "import re\n",
    "from psycopg2.extras import DictCursor\n",
    "from cuml.preprocessing import StandardScaler\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "\n",
    "# Load API key from Colab form or environment\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    print(\"Getting Voyage AI API key from Colab form\")\n",
    "    # @title Voyage AI API Key\n",
    "    voyage_api_key = \"\" # @param {type:\"string\"}\n",
    "    # Set it for the client\n",
    "    voyageai.api_key = voyage_api_key\n",
    "else:\n",
    "    from dotenv import load_dotenv\n",
    "    print(\"Running locally, loading API key from .env\")\n",
    "    load_dotenv()\n",
    "    voyageai.api_key = os.getenv('VOYAGE_API_KEY')\n",
    "\n",
    "# Initialize Voyage AI client\n",
    "vo = voyageai.Client()\n",
    "\n",
    "# Path to database\n",
    "db_path = \"/content/drive/MyDrive/ai-safety-papers/papers.sql\"\n",
    "\n",
    "def load_database():\n",
    "    \"\"\"Load PostgreSQL backup using psql\"\"\"\n",
    "    print(\"Loading PostgreSQL backup...\")\n",
    "    !createdb -U postgres papers # pyright: ignore\n",
    "    !pg_restore -U postgres --jobs=8 -d papers \"{backup_path}\" # pyright: ignore\n",
    "\n",
    "def connect_db():\n",
    "    \"\"\"Connect to PostgreSQL database with schema validation\"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        host='',\n",
    "        database=\"papers\",\n",
    "        user=\"postgres\"\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "load_database()\n",
    "conn = connect_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Column Reset (Run when reprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column():\n",
    "    # Check for embedding column\n",
    "    with conn.cursor() as cursor:\n",
    "        print(\"Adding embedding column...\")\n",
    "        cursor.execute('''\n",
    "            ALTER TABLE papers \n",
    "            ADD COLUMN IF NOT EXISTS embedding BYTEA\n",
    "        ''')\n",
    "        cursor.execute('''\n",
    "            ALTER TABLE papers \n",
    "            ADD COLUMN IF NOT EXISTS scaled_embedding BYTEA\n",
    "        ''')\n",
    "        conn.commit()\n",
    "\n",
    "create_column()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "max_batches = None  # Set to None to process all batches, or a number to limit processing\n",
    "batch_size = 128  # Maximum batch size for Voyage AI\n",
    "\n",
    "# Rate limit tracking\n",
    "class RateLimiter:\n",
    "    def __init__(self, rpm_limit=2000, tpm_limit=3_000_000):  # voyage-3-large limits: 2000 RPM, 3M TPM\n",
    "        self.rpm_limit = rpm_limit\n",
    "        self.tpm_limit = tpm_limit\n",
    "        self.requests = []  # List of timestamps\n",
    "        self.tokens = []    # List of (timestamp, token_count) tuples\n",
    "        \n",
    "    def can_make_request(self, token_count):\n",
    "        current_time = time.time()\n",
    "        cutoff_time = current_time - 60  # 1 minute ago\n",
    "        \n",
    "        # Clean up old entries\n",
    "        self.requests = [t for t in self.requests if t > cutoff_time]\n",
    "        self.tokens = [(t, c) for t, c in self.tokens if t > cutoff_time]\n",
    "        \n",
    "        # Check RPM limit\n",
    "        if len(self.requests) >= self.rpm_limit:\n",
    "            print(f\"\\nRate limit reached at {time.strftime('%H:%M:%S')}:\")\n",
    "            print(f\"- Request limit: {len(self.requests)}/{self.rpm_limit} RPM\")\n",
    "            return False\n",
    "            \n",
    "        # Check TPM limit\n",
    "        total_tokens = sum(count for _, count in self.tokens)\n",
    "        if total_tokens + token_count > self.tpm_limit:\n",
    "            print(f\"\\nRate limit reached at {time.strftime('%H:%M:%S')}:\")\n",
    "            print(f\"- Token limit: {total_tokens + token_count}/{self.tpm_limit} TPM\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def add_request(self, token_count):\n",
    "        current_time = time.time()\n",
    "        self.requests.append(current_time)\n",
    "        self.tokens.append((current_time, token_count))\n",
    "\n",
    "rate_limiter = RateLimiter()\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=1, max=60), stop=stop_after_attempt(6))\n",
    "def embed_with_backoff(texts, model=\"voyage-3-large\"):\n",
    "    return vo.embed(\n",
    "        texts,\n",
    "        model=model,\n",
    "        input_type=\"document\",\n",
    "        output_dimension=2048\n",
    "    )\n",
    "\n",
    "def get_csai_papers(conn: psycopg2.extensions.connection, batch_size: int = 128):\n",
    "    \"\"\"Generator that yields batches of papers needing embeddings\"\"\"\n",
    "    cursor = conn.cursor(cursor_factory=DictCursor)\n",
    "    \n",
    "    # First get total count for progress bar\n",
    "    cursor.execute('''\n",
    "        WITH relevant_papers AS (\n",
    "            SELECT id, title, abstract, llm_category\n",
    "            FROM papers\n",
    "            WHERE title IS NOT NULL\n",
    "              AND abstract IS NOT NULL\n",
    "              AND embedding IS NULL\n",
    "        )\n",
    "        SELECT COUNT(*) FROM relevant_papers\n",
    "    ''')\n",
    "    total_papers = cursor.fetchone()[0]\n",
    "    \n",
    "    # Then fetch papers in batches\n",
    "    cursor.execute('''\n",
    "        WITH relevant_papers AS (\n",
    "            SELECT id, title, abstract, llm_category\n",
    "            FROM papers\n",
    "            WHERE title IS NOT NULL\n",
    "              AND abstract IS NOT NULL\n",
    "              AND embedding IS NULL\n",
    "        )\n",
    "        SELECT id, title, abstract, llm_category FROM relevant_papers\n",
    "    ''')\n",
    "    \n",
    "    batch = []\n",
    "    with tqdm(total=total_papers, desc=\"Processing papers\", unit=\" papers\") as pbar:\n",
    "        for row in cursor:\n",
    "            # Combine category with title/abstract\n",
    "            combined_text = f\"Category: {row['llm_category']}\\n{row['title']}\\n{row['abstract']}\"\n",
    "            batch.append((row['id'], combined_text))\n",
    "            if len(batch) >= batch_size:\n",
    "                pbar.update(len(batch))\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if batch:\n",
    "            pbar.update(len(batch))\n",
    "            yield batch\n",
    "\n",
    "def adjust_batch_for_token_limit(batch: List[Tuple[str, str]], model: str = \"voyage-3-large\") -> List[List[Tuple[str, str]]]:\n",
    "    \"\"\"Split a batch into sub-batches that respect the token limit\n",
    "    \n",
    "    Args:\n",
    "        batch: List of (id, text) tuples\n",
    "        model: Voyage AI model to use\n",
    "    \n",
    "    Returns:\n",
    "        List of batches, each respecting the token limit\n",
    "    \"\"\"\n",
    "    TOKEN_LIMIT = 120_000  # voyage-3-large limit\n",
    "    \n",
    "    texts = [item[1] for item in batch]\n",
    "    token_counts = [vo.count_tokens([text], model=model) for text in texts]\n",
    "    \n",
    "    sub_batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for (paper_id, text), token_count in zip(batch, token_counts):\n",
    "        # If single text exceeds limit, skip it\n",
    "        if token_count > TOKEN_LIMIT:\n",
    "            print(f\"Warning: Text {paper_id} exceeds token limit ({token_count} tokens), skipping\")\n",
    "            continue\n",
    "            \n",
    "        # If adding this text would exceed limit, start new batch\n",
    "        if current_tokens + token_count > TOKEN_LIMIT:\n",
    "            if current_batch:\n",
    "                sub_batches.append(current_batch)\n",
    "            current_batch = [(paper_id, text)]\n",
    "            current_tokens = token_count\n",
    "        else:\n",
    "            current_batch.append((paper_id, text))\n",
    "            current_tokens += token_count\n",
    "    \n",
    "    if current_batch:\n",
    "        sub_batches.append(current_batch)\n",
    "    \n",
    "    return sub_batches\n",
    "\n",
    "def process_batch(batch: List[Tuple[str, str]], model: str = \"voyage-3-large\") -> List[Tuple[str, List[float]]]:\n",
    "    \"\"\"Process a batch of papers, returning embeddings\"\"\"\n",
    "    texts = [item[1] for item in batch]\n",
    "    paper_ids = [item[0] for item in batch]\n",
    "    \n",
    "    # First count tokens for rate limiting\n",
    "    try:\n",
    "        token_count = vo.count_tokens(texts, model=model)\n",
    "    except Exception as e:\n",
    "        print(f\"Error counting tokens: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    # Check rate limits\n",
    "    if not rate_limiter.can_make_request(token_count):\n",
    "        raise Exception(\"Rate limit exceeded\")  # This will trigger exponential backoff\n",
    "    \n",
    "    # Generate embeddings with exponential backoff\n",
    "    try:\n",
    "        result = embed_with_backoff(texts, model=model)\n",
    "        \n",
    "        if isinstance(result, float):\n",
    "            raise ValueError(f\"API returned float instead of EmbeddingsObject: {result}\")\n",
    "            \n",
    "        if not hasattr(result, 'embeddings'):\n",
    "            raise ValueError(f\"Unexpected response from vo.embed: {result}\")\n",
    "            \n",
    "        # Return list of (paper_id, embedding) tuples and total tokens used\n",
    "        embeddings = result.embeddings\n",
    "        rate_limiter.add_request(token_count)\n",
    "        \n",
    "        return [(paper_id, embedding) for paper_id, embedding in zip(paper_ids, embeddings)], result.total_tokens\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"\\nError in vo.embed call:\")\n",
    "        print(f\"Exception type: {type(e)}\")\n",
    "        print(f\"Exception args: {e.args}\")\n",
    "        print(f\"Full exception: {repr(e)}\")\n",
    "        print(\"\\nInput that caused error:\")\n",
    "        print(f\"First text: {texts[0][:500]}...\")  # Truncate long texts\n",
    "        raise\n",
    "\n",
    "def scale_and_store_embeddings():\n",
    "    \"\"\"Scale embeddings using StandardScaler and store in database\"\"\"\n",
    "    print(\"\\nScaling embeddings...\")\n",
    "    \n",
    "    # Load all raw embeddings\n",
    "    cursor = conn.cursor(cursor_factory=DictCursor)\n",
    "    cursor.execute('SELECT id, embedding FROM papers WHERE embedding IS NOT NULL')\n",
    "    papers = cursor.fetchall()\n",
    "    \n",
    "    if not papers:\n",
    "        print(\"No embeddings found to scale\")\n",
    "        return\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    embeddings = np.array([np.frombuffer(p['embedding'], dtype=np.float32) for p in papers])\n",
    "    \n",
    "    # Scale embeddings\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "    \n",
    "    # Store scaled embeddings\n",
    "    with conn.cursor() as update_cursor:\n",
    "        for p, scaled_emb in zip(papers, scaled_embeddings):\n",
    "            update_cursor.execute('''\n",
    "                UPDATE papers\n",
    "                SET scaled_embedding = %s\n",
    "                WHERE id = %s\n",
    "            ''', (scaled_emb.astype(np.float32).tobytes(), p['id']))\n",
    "        conn.commit()\n",
    "    print(f\"Scaled {len(papers)} embeddings\")\n",
    "\n",
    "def generate_embeddings():\n",
    "    # Process in batches\n",
    "    total_updated = 0\n",
    "    batches_processed = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    try:\n",
    "        for batch in get_csai_papers(conn, batch_size):\n",
    "            if max_batches is not None and batches_processed >= max_batches:\n",
    "                print(f\"\\nStopping after {max_batches} batch(es) as requested\")\n",
    "                break\n",
    "                \n",
    "            # Split batch if needed to respect token limit\n",
    "            sub_batches = adjust_batch_for_token_limit(batch)\n",
    "            \n",
    "            for i, sub_batch in enumerate(sub_batches):\n",
    "                try:\n",
    "                    # Process batch and get embeddings\n",
    "                    results, batch_tokens = process_batch(sub_batch)\n",
    "                    if not isinstance(results, list):\n",
    "                        print(f\"Warning: results is not a list, got {type(results)}\")\n",
    "                        print(f\"Results value: {results}\")\n",
    "                        raise ValueError(f\"Expected list result, got {type(results)}\")\n",
    "                    \n",
    "                    # Store in database\n",
    "                    cursor = conn.cursor()\n",
    "                    for paper_id, embedding in results:\n",
    "                        embedding_blob = np.array(embedding, dtype=np.float32).tobytes()\n",
    "                        cursor.execute('''\n",
    "                            UPDATE papers\n",
    "                            SET embedding = %s\n",
    "                            WHERE id = %s\n",
    "                        ''', (embedding_blob, paper_id))\n",
    "                    \n",
    "                    conn.commit()\n",
    "                    total_updated += len(sub_batch)\n",
    "                    \n",
    "                    # Update total tokens\n",
    "                    total_tokens += batch_tokens\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nFatal error processing sub-batch:\")\n",
    "                    print(f\"Exception type: {type(e)}\")\n",
    "                    print(f\"Exception args: {e.args}\")\n",
    "                    print(f\"Full exception: {repr(e)}\")\n",
    "                    print(f\"Sub-batch contents: {sub_batch[:2]}...\")\n",
    "                    raise  # Stop execution here\n",
    "            \n",
    "            batches_processed += 1\n",
    "            if max_batches is not None:\n",
    "                print(f\"\\nProcessed {batches_processed}/{max_batches} batch(es)\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFatal error in embedding generation: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        print(f\"\\nProcess completed:\")\n",
    "        print(f\"- Total papers processed: {total_updated}\")\n",
    "        print(f\"- Total batches processed: {batches_processed}\")\n",
    "        print(f\"- Total tokens processed: {total_tokens:,}\")\n",
    "\n",
    "generate_embeddings()\n",
    "scale_and_store_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. KNN Graph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_knn_graph() -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate KNN graph for clustering\"\"\"\n",
    "    print(\"\\nGenerating KNN graph...\")\n",
    "    \n",
    "    # Load all scaled embeddings\n",
    "    cursor = conn.cursor(cursor_factory=DictCursor)\n",
    "    cursor.execute('SELECT id, scaled_embedding FROM papers WHERE scaled_embedding IS NOT NULL')\n",
    "    papers = cursor.fetchall()\n",
    "    \n",
    "    if not papers:\n",
    "        raise ValueError(\"No scaled embeddings found in database\")\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    embeddings = np.array([np.frombuffer(row['scaled_embedding'], dtype=np.float32) for row in papers])\n",
    "    \n",
    "    # Create KNN graph\n",
    "    print(\"Computing 100-NN graph...\")\n",
    "    nn_model = NearestNeighbors(n_neighbors=100, metric='cosine')\n",
    "    nn_model.fit(embeddings)\n",
    "    distances, indices = nn_model.kneighbors(embeddings)\n",
    "    \n",
    "    # Save to Drive\n",
    "    save_path = \"/content/drive/MyDrive/ai-safety-papers/knn_graph.npz\"\n",
    "    np.savez_compressed(save_path, distances=distances, indices=indices)\n",
    "    print(f\"Saved KNN graph to {save_path}\")\n",
    "    \n",
    "    return distances, indices\n",
    "\n",
    "distances, indices = generate_knn_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 6. Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def validate_embeddings(distances: np.ndarray, indices: np.ndarray) -> dict:\n",
    "    \"\"\"Run quality checks using precomputed KNN graph\"\"\"\n",
    "    # Set fixed seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(\"\\n=== Embedding Quality Checks ===\")\n",
    "    \n",
    "    # Convert distances to similarities (cosine distance = 1 - cosine similarity)\n",
    "    similarities = 1 - distances\n",
    "    \n",
    "    # Calculate metrics directly from in-memory graph\n",
    "    avg_similarities = similarities.mean(axis=1)\n",
    "    similarity_variances = similarities.var(axis=1)\n",
    "    \n",
    "    # Global statistics\n",
    "    global_mean = avg_similarities.mean()\n",
    "    global_std = avg_similarities.std()\n",
    "    var_mean = similarity_variances.mean()\n",
    "    var_std = similarity_variances.std()\n",
    "    \n",
    "    # Outlier detection (2σ thresholds)\n",
    "    outlier_mask = (\n",
    "        (avg_similarities < (global_mean - 2 * global_std)) |\n",
    "        (similarity_variances > (var_mean + 2 * var_std))\n",
    "    )\n",
    "    outlier_count = np.sum(outlier_mask)\n",
    "    \n",
    "    # Distribution analysis\n",
    "    hist, bins = np.histogram(avg_similarities, bins=20, density=True)\n",
    "    \n",
    "    print(\"\\nNeighborhood Coherence Metrics:\")\n",
    "    print(f\"Global mean similarity: {global_mean:.3f} ± {global_std:.3f}\")\n",
    "    print(f\"Variance stats: {var_mean:.3f} ± {var_std:.3f}\")\n",
    "    print(f\"Outlier papers: {outlier_count} ({outlier_count/len(avg_similarities)*100:.1f}%)\")\n",
    "    print(f\"Similarity range: [{avg_similarities.min():.3f}, {avg_similarities.max():.3f}]\")\n",
    "    \n",
    "    # Plot distribution\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(avg_similarities, bins=20, alpha=0.7)\n",
    "    plt.title('Distribution of Neighborhood Similarities')\n",
    "    plt.xlabel('Average Similarity Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'global_mean': global_mean,\n",
    "        'global_std': global_std,\n",
    "        'var_mean': var_mean,\n",
    "        'var_std': var_std,\n",
    "        'outlier_count': outlier_count,\n",
    "        'similarity_distribution': (hist, bins)\n",
    "    }\n",
    "\n",
    "validation_results = validate_embeddings(distances, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 7. Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def backup_embeddings():\n",
    "    \"\"\"Use pg_dump for PostgreSQL backups\"\"\"\n",
    "    backup_path = \"/content/drive/MyDrive/ai-safety-papers/papers.sql\"\n",
    "    !pg_dump -U postgres -F c -f \"{backup_path}\" papers # pyright: ignore\n",
    "\n",
    "# Call backup after processing\n",
    "backup_embeddings()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
