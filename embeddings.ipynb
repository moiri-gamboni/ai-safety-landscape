{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Papers - Abstract Embedding Phase\n",
    "\n",
    "This notebook generates ModernBERT embeddings for paper abstracts and stores them in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Clone repository if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Clone the repository\n",
    "    !git clone https://github.com/moiri-gamboni/ai-safety-landscape.git\n",
    "    %cd ai-safety-landscape\n",
    "\n",
    "# Install required packages\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "# Verify GPU availability\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to database\n",
    "db_path = \"/content/drive/MyDrive/ai-safety-papers/papers.db\"\n",
    "local_db = \"papers.db\"\n",
    "\n",
    "# Copy database to local storage if needed\n",
    "print(f\"Copying database to local storage: {local_db}\")\n",
    "if not os.path.exists(local_db):\n",
    "    %cp \"{db_path}\" {local_db}\n",
    "\n",
    "conn = sqlite3.connect(local_db)\n",
    "conn.row_factory = sqlite3.Row\n",
    "\n",
    "# Check if abstract_embedding column exists\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"PRAGMA table_info(papers)\")\n",
    "columns = [column[1] for column in cursor.fetchall()]\n",
    "\n",
    "if 'abstract_embedding' not in columns:\n",
    "    print(\"Adding abstract_embedding column...\")\n",
    "    conn.execute('''\n",
    "        ALTER TABLE papers \n",
    "        ADD COLUMN abstract_embedding BLOB\n",
    "    ''')\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import List, Optional\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MODEL_NAME = \"answerdotai/ModernBERT-large\"\n",
    "\n",
    "# Load model and tokenizer (only once)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"eager\"  # Use standard attention implementation for broader compatibility\n",
    ").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(\n",
    "    texts: List[str],\n",
    "    pooling: str = \"mean\",\n",
    "    normalize: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate embeddings for a batch of texts using ModernBERT.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts to embed\n",
    "        pooling: Pooling strategy ('mean' or 'cls')\n",
    "            - mean: Average all token embeddings (better for retrieval/clustering)\n",
    "            - cls: Use [CLS] token embedding\n",
    "        normalize: Whether to L2-normalize embeddings (recommended for clustering)\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Array of embeddings, shape (n_texts, 1024)\n",
    "    \"\"\"\n",
    "    # Tokenize texts\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=8192,  # Use ModernBERT's full context length\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        if pooling == \"mean\":\n",
    "            # Mean pooling with attention mask\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "            sum_embeddings = torch.sum(hidden_states * mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "            embeddings = sum_embeddings / sum_mask\n",
    "        else:  # cls pooling\n",
    "            embeddings = hidden_states[:, 0]\n",
    "        \n",
    "        # Optionally normalize\n",
    "        if normalize:\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings.cpu().numpy().astype(np.float32)\n",
    "\n",
    "def get_csai_papers(conn: sqlite3.Connection, batch_size: int = 256):\n",
    "    \"\"\"Generator that yields batches of papers with cs.AI in their categories\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT id, abstract \n",
    "        FROM papers \n",
    "        WHERE categories LIKE '%cs.AI%' \n",
    "          AND abstract IS NOT NULL\n",
    "          AND abstract_embedding IS NULL\n",
    "    ''')\n",
    "    \n",
    "    batch = []\n",
    "    for row in cursor:\n",
    "        batch.append((row['id'], row['abstract']))\n",
    "        if len(batch) >= batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process in batches\n",
    "total_updated = 0\n",
    "batch_size = 256  # Adjust based on GPU memory\n",
    "\n",
    "# Get total number of papers to process\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('''\n",
    "    SELECT COUNT(*) as count\n",
    "    FROM papers \n",
    "    WHERE categories LIKE '%cs.AI%' \n",
    "      AND abstract IS NOT NULL\n",
    "      AND abstract_embedding IS NULL\n",
    "''')\n",
    "total_papers = cursor.fetchone()['count']\n",
    "total_batches = (total_papers + batch_size - 1) // batch_size  # Ceiling division\n",
    "\n",
    "try:\n",
    "    for batch_num, batch in enumerate(tqdm(get_csai_papers(conn, batch_size), \n",
    "                                         desc=\"Processing batches\",\n",
    "                                         total=total_batches), 1):\n",
    "        paper_ids = [item[0] for item in batch]\n",
    "        abstracts = [item[1] for item in batch]\n",
    "        \n",
    "        try:\n",
    "            # Generate embeddings with mean pooling and normalization\n",
    "            embeddings = generate_embeddings(\n",
    "                abstracts,\n",
    "                pooling=\"mean\",  # Better for document similarity\n",
    "                normalize=True   # Better for clustering\n",
    "            )\n",
    "            \n",
    "            # Store in database\n",
    "            cursor = conn.cursor()\n",
    "            for paper_id, embedding in zip(paper_ids, embeddings):\n",
    "                embedding_blob = embedding.tobytes()\n",
    "                cursor.execute('''\n",
    "                    UPDATE papers\n",
    "                    SET abstract_embedding = ?\n",
    "                    WHERE id = ?\n",
    "                ''', (embedding_blob, paper_id))\n",
    "            \n",
    "            conn.commit()\n",
    "            total_updated += len(batch)\n",
    "            print(f\"\\nBatch {batch_num}/{total_batches} complete:\")\n",
    "            print(f\"- Papers processed this batch: {len(batch)}\")\n",
    "            print(f\"- Total papers processed: {total_updated}/{total_papers} ({(total_updated/total_papers)*100:.1f}%)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing batch {batch_num}/{total_batches}:\")\n",
    "            print(f\"- Error message: {str(e)}\")\n",
    "            print(f\"- Batch size: {len(batch)}\")\n",
    "            conn.rollback()\n",
    "            # Optionally reduce batch size and retry\n",
    "            if batch_size > 32:\n",
    "                print(\"- Reducing batch size and continuing...\")\n",
    "                batch_size = batch_size // 2\n",
    "                # Recalculate total batches with new batch size\n",
    "                total_batches = (total_papers - total_updated + batch_size - 1) // batch_size\n",
    "            else:\n",
    "                raise  # Re-raise if batch size is already small\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nFatal error in embedding generation: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    print(f\"\\nProcess completed:\")\n",
    "    print(f\"- Total papers processed: {total_updated}/{total_papers} ({(total_updated/total_papers)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 5. Database Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy updated database back to Drive\n",
    "!cp {local_db} \"{db_path}\"\n",
    "print(\"Database backup completed to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 6. Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def validate_embeddings(conn):\n",
    "    \"\"\"Run quality checks on generated embeddings\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"\\n=== Embedding Quality Checks ===\")\n",
    "    \n",
    "    # 1. Coverage Check\n",
    "    cursor.execute('''\n",
    "        SELECT \n",
    "            COUNT(*) AS total_csai,\n",
    "            SUM(CASE WHEN abstract_embedding IS NOT NULL THEN 1 ELSE 0 END) AS with_embedding,\n",
    "            SUM(CASE WHEN abstract_embedding IS NULL THEN 1 ELSE 0 END) AS without_embedding\n",
    "        FROM papers \n",
    "        WHERE categories LIKE 'cs.AI%'\n",
    "          AND abstract IS NOT NULL\n",
    "    ''')\n",
    "    stats = cursor.fetchone()\n",
    "    print(f\"\\n1. Coverage:\")\n",
    "    print(f\"- Total CS.AI papers: {stats['total_csai']}\")\n",
    "    print(f\"- With embeddings: {stats['with_embedding']} ({(stats['with_embedding']/stats['total_csai'])*100:.1f}%)\")\n",
    "    print(f\"- Missing embeddings: {stats['without_embedding']} ({(stats['without_embedding']/stats['total_csai'])*100:.1f}%)\")\n",
    "\n",
    "    # 2. Validity Check (NaN/Zero vectors)\n",
    "    cursor.execute('''\n",
    "        SELECT abstract_embedding \n",
    "        FROM papers \n",
    "        WHERE abstract_embedding IS NOT NULL\n",
    "        LIMIT 1000\n",
    "    ''')\n",
    "    invalid_count = 0\n",
    "    total_checked = 0\n",
    "    for row in cursor:\n",
    "        embedding = np.frombuffer(row['abstract_embedding'], dtype=np.float32)\n",
    "        if np.isnan(embedding).any():\n",
    "            invalid_count += 1\n",
    "        elif np.all(embedding == 0):\n",
    "            invalid_count += 1\n",
    "        total_checked += 1\n",
    "    \n",
    "    print(f\"\\n2. Validity (sampled {total_checked}):\")\n",
    "    print(f\"- Invalid embeddings (NaN/zeros): {invalid_count} ({(invalid_count/total_checked)*100:.1f}%)\")\n",
    "\n",
    "    # 3. Norm Analysis\n",
    "    cursor.execute('''\n",
    "        SELECT abstract_embedding \n",
    "        FROM papers \n",
    "        WHERE abstract_embedding IS NOT NULL\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT 1000\n",
    "    ''')\n",
    "    norms = []\n",
    "    for row in cursor:\n",
    "        embedding = np.frombuffer(row['abstract_embedding'], dtype=np.float32)\n",
    "        norms.append(np.linalg.norm(embedding))\n",
    "    \n",
    "    print(f\"\\n3. Norm Analysis (sample):\")\n",
    "    print(f\"- Mean norm: {np.mean(norms):.2f}\")\n",
    "    print(f\"- Std dev: {np.std(norms):.2f}\")\n",
    "    print(f\"- Min/Max: {np.min(norms):.2f}/{np.max(norms):.2f}\")\n",
    "\n",
    "    # 4. Similarity Sanity Check\n",
    "    # Get random pairs\n",
    "    cursor.execute('''\n",
    "        SELECT abstract_embedding \n",
    "        FROM papers \n",
    "        WHERE abstract_embedding IS NOT NULL\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT 100\n",
    "    ''')\n",
    "    random_embeddings = [np.frombuffer(row['abstract_embedding'], dtype=np.float32) for row in cursor]\n",
    "    random_similarities = cosine_similarity(random_embeddings)\n",
    "    np.fill_diagonal(random_similarities, np.nan)  # Exclude self-similarity\n",
    "    \n",
    "    # Get duplicate candidates (from harvesting phase)\n",
    "    cursor.execute('''\n",
    "        SELECT p1.abstract_embedding, p2.abstract_embedding\n",
    "        FROM (\n",
    "            SELECT id, title\n",
    "            FROM papers\n",
    "            WHERE withdrawn = 0\n",
    "        ) p1\n",
    "        JOIN (\n",
    "            SELECT id, title\n",
    "            FROM papers\n",
    "            WHERE withdrawn = 0\n",
    "        ) p2 ON p1.title COLLATE NOCASE = p2.title COLLATE NOCASE AND p1.id < p2.id\n",
    "        WHERE p1.abstract_embedding IS NOT NULL\n",
    "          AND p2.abstract_embedding IS NOT NULL\n",
    "        LIMIT 50\n",
    "    ''')\n",
    "    duplicate_pairs = []\n",
    "    for row in cursor:\n",
    "        e1 = np.frombuffer(row[0], dtype=np.float32)\n",
    "        e2 = np.frombuffer(row[1], dtype=np.float32)\n",
    "        duplicate_pairs.append((e1, e2))\n",
    "    \n",
    "    # Calculate similarities\n",
    "    duplicate_similarities = [cosine_similarity([e1], [e2])[0][0] for e1, e2 in duplicate_pairs]\n",
    "    \n",
    "    print(f\"\\n4. Similarity Analysis:\")\n",
    "    print(f\"- Random pairs (n={len(random_similarities)**2 - len(random_similarities)}):\")\n",
    "    print(f\"  Mean: {np.nanmean(random_similarities):.2f} ± {np.nanstd(random_similarities):.2f}\")\n",
    "    if duplicate_pairs:\n",
    "        print(f\"- Duplicate candidates (n={len(duplicate_pairs)}):\")\n",
    "        print(f\"  Mean: {np.mean(duplicate_similarities):.2f} ± {np.std(duplicate_similarities):.2f}\")\n",
    "    else:\n",
    "        print(\"- No duplicate pairs found for comparison\")\n",
    "\n",
    "# Run validation\n",
    "validate_embeddings(conn)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
