{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Papers - Abstract Embedding Phase\n",
    "\n",
    "This notebook generates ModernBERT embeddings for paper abstracts and stores them in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Clone repository if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Clone the repository\n",
    "    !git clone https://github.com/moiri-gamboni/ai-safety-landscape.git\n",
    "    %cd ai-safety-landscape\n",
    "\n",
    "# Install required packages\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "# Verify GPU availability\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to database\n",
    "db_path = \"/content/drive/MyDrive/ai-safety-papers/papers.db\"\n",
    "local_db = \"papers.db\"\n",
    "\n",
    "# Copy database to local storage if needed\n",
    "print(f\"Copying database to local storage: {local_db}\")\n",
    "if not os.path.exists(local_db):\n",
    "    %cp \"{db_path}\" {local_db}\n",
    "\n",
    "conn = sqlite3.connect(local_db)\n",
    "conn.row_factory = sqlite3.Row\n",
    "\n",
    "# Check if abstract_embedding column exists\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"PRAGMA table_info(papers)\")\n",
    "columns = [column[1] for column in cursor.fetchall()]\n",
    "\n",
    "if 'abstract_embedding' not in columns:\n",
    "    print(\"Adding abstract_embedding column...\")\n",
    "    conn.execute('''\n",
    "        ALTER TABLE papers \n",
    "        ADD COLUMN abstract_embedding BLOB\n",
    "    ''')\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import List, Optional\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MODEL_NAME = \"answerdotai/ModernBERT-large\"\n",
    "\n",
    "# Load model and tokenizer (only once)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"eager\"  # Use standard attention implementation for broader compatibility\n",
    ").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(\n",
    "    texts: List[str],\n",
    "    pooling: str = \"mean\",\n",
    "    normalize: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate embeddings for a batch of texts using ModernBERT.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts to embed\n",
    "        pooling: Pooling strategy ('mean' or 'cls')\n",
    "            - mean: Average all token embeddings (better for retrieval/clustering)\n",
    "            - cls: Use [CLS] token embedding\n",
    "        normalize: Whether to L2-normalize embeddings (recommended for clustering)\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Array of embeddings, shape (n_texts, 1024)\n",
    "    \"\"\"\n",
    "    # Tokenize texts\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=8192,  # Use ModernBERT's full context length\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        if pooling == \"mean\":\n",
    "            # Mean pooling with attention mask\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "            sum_embeddings = torch.sum(hidden_states * mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "            embeddings = sum_embeddings / sum_mask\n",
    "        else:  # cls pooling\n",
    "            embeddings = hidden_states[:, 0]\n",
    "        \n",
    "        # Optionally normalize\n",
    "        if normalize:\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings.cpu().numpy().astype(np.float32)\n",
    "\n",
    "def get_csai_papers(conn: sqlite3.Connection, batch_size_ref: dict):\n",
    "    \"\"\"Generator that yields batches of papers with cs.AI in their categories\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        batch_size_ref: Dictionary containing current batch size, allows for dynamic updates\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # First get total count for progress bar\n",
    "    cursor.execute('''\n",
    "        WITH split_categories AS (\n",
    "            SELECT id, abstract\n",
    "            FROM papers\n",
    "            WHERE categories LIKE '%cs.AI%'\n",
    "              AND abstract IS NOT NULL\n",
    "              AND abstract_embedding IS NULL\n",
    "        )\n",
    "        SELECT COUNT(*) FROM split_categories\n",
    "    ''')\n",
    "    total_papers = cursor.fetchone()[0]\n",
    "    \n",
    "    # Then fetch papers in batches\n",
    "    cursor.execute('''\n",
    "        WITH split_categories AS (\n",
    "            SELECT id, abstract\n",
    "            FROM papers\n",
    "            WHERE categories LIKE '%cs.AI%'\n",
    "              AND abstract IS NOT NULL\n",
    "              AND abstract_embedding IS NULL\n",
    "        )\n",
    "        SELECT id, abstract FROM split_categories\n",
    "    ''')\n",
    "    \n",
    "    batch = []\n",
    "    with tqdm(total=total_papers, desc=\"Processing papers\", unit=\" papers\") as pbar:\n",
    "        for row in cursor:\n",
    "            batch.append((row['id'], row['abstract']))\n",
    "            if len(batch) >= batch_size_ref['size']:\n",
    "                pbar.update(len(batch))\n",
    "                yield batch\n",
    "                batch = []\n",
    "                # Clear CUDA cache after each batch\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "        if batch:\n",
    "            pbar.update(len(batch))\n",
    "            yield batch\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "def get_gpu_memory_free():\n",
    "    \"\"\"Get actual free GPU memory in GB, accounting for all reserved memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        reserved = torch.cuda.max_memory_reserved() / 1024**3\n",
    "        return total - reserved\n",
    "    return 0\n",
    "\n",
    "def get_gpu_memory_stats():\n",
    "    \"\"\"Get GPU memory statistics in GB\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        reserved = torch.cuda.max_memory_reserved() / 1024**3\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        return {\n",
    "            'total': total,\n",
    "            'reserved': reserved,\n",
    "            'allocated': allocated,\n",
    "            'cached': cached,\n",
    "            'free_total': total - reserved,\n",
    "            'free_cached': cached - allocated\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def calculate_batch_adjustment(current_size: int, peak_memory: float) -> Optional[int]:\n",
    "    \"\"\"Calculate new batch size based on memory usage.\n",
    "    \n",
    "    Args:\n",
    "        current_size: Current batch size\n",
    "        peak_memory: Peak memory used by current batch in GB\n",
    "        \n",
    "    Returns:\n",
    "        Optional[int]: New batch size if adjustment needed, None otherwise\n",
    "    \"\"\"\n",
    "    stats = get_gpu_memory_stats()\n",
    "    if not stats:\n",
    "        return None\n",
    "        \n",
    "    # How many times our current batch could fit in memory (leaving 2GB buffer)\n",
    "    memory_headroom = (stats['total'] - 2.0) / peak_memory if peak_memory > 0 else 1\n",
    "    \n",
    "    # Target range: 1.2-1.4x headroom\n",
    "    if memory_headroom > 1.4:  # Using too little memory\n",
    "        # Increase batch size to target 1.3x headroom\n",
    "        increase_factor = min(memory_headroom / 1.3, 1.3)\n",
    "        return int(current_size * increase_factor)\n",
    "    elif memory_headroom < 1.2:  # Using too much memory\n",
    "        # Reduce batch size to target 1.3x headroom\n",
    "        decrease_factor = 1.3 / memory_headroom\n",
    "        return int(current_size / decrease_factor)\n",
    "    return None  # We're in the optimal range\n",
    "\n",
    "# Process in batches\n",
    "total_updated = 0\n",
    "batch_size_ref = {'size': 256}  # Mutable reference to batch size\n",
    "\n",
    "try:\n",
    "    while True:  # Keep trying until we succeed or hit an unrecoverable error\n",
    "        try:\n",
    "            for batch in get_csai_papers(conn, batch_size_ref):\n",
    "                # Reset peak stats before processing\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.reset_peak_memory_stats()\n",
    "                \n",
    "                paper_ids = [item[0] for item in batch]\n",
    "                abstracts = [item[1] for item in batch]\n",
    "                \n",
    "                # Generate embeddings with mean pooling and normalization\n",
    "                embeddings = generate_embeddings(\n",
    "                    abstracts,\n",
    "                    pooling=\"mean\",  # Better for document similarity\n",
    "                    normalize=True   # Better for clustering\n",
    "                )\n",
    "                \n",
    "                # Calculate actual memory used by this batch\n",
    "                peak_memory = torch.cuda.max_memory_allocated() / 1024**3  # GB\n",
    "                \n",
    "                # Store in database\n",
    "                cursor = conn.cursor()\n",
    "                for paper_id, embedding in zip(paper_ids, embeddings):\n",
    "                    embedding_blob = embedding.tobytes()\n",
    "                    cursor.execute('''\n",
    "                        UPDATE papers\n",
    "                        SET abstract_embedding = ?\n",
    "                        WHERE id = ?\n",
    "                    ''', (embedding_blob, paper_id))\n",
    "                \n",
    "                conn.commit()\n",
    "                total_updated += len(batch)\n",
    "                \n",
    "                # Check if we should adjust batch size\n",
    "                new_size = calculate_batch_adjustment(batch_size_ref['size'], peak_memory)\n",
    "                if new_size:\n",
    "                    old_size = batch_size_ref['size']\n",
    "                    batch_size_ref['size'] = new_size\n",
    "                    print(f\"\\nAdjusting batch size: {old_size} → {new_size} (peak memory: {peak_memory:.1f}GB)\")\n",
    "                \n",
    "                # Clear CUDA cache after each batch\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            # If we complete the loop without OOM errors, we're done\n",
    "            break\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                old_size = batch_size_ref['size']\n",
    "                # Reduce by 20%\n",
    "                batch_size_ref['size'] = int(old_size * 0.8)\n",
    "                print(f\"\\nCUDA out of memory - reducing batch size: {old_size} → {batch_size_ref['size']}\")\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.reset_peak_memory_stats()\n",
    "                continue\n",
    "            raise  # Re-raise if it's not an OOM error\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nFatal error in embedding generation: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    print(f\"\\nProcess completed:\")\n",
    "    print(f\"- Total papers processed: {total_updated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def validate_embeddings(conn):\n",
    "    \"\"\"Run quality checks on generated embeddings\"\"\"\n",
    "    # Set fixed seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"\\n=== Embedding Quality Checks ===\")\n",
    "    \n",
    "    # 1. Coverage Check\n",
    "    cursor.execute('''\n",
    "        SELECT \n",
    "            COUNT(*) AS total_csai,\n",
    "            SUM(CASE WHEN abstract_embedding IS NOT NULL THEN 1 ELSE 0 END) AS with_embedding,\n",
    "            SUM(CASE WHEN abstract_embedding IS NULL THEN 1 ELSE 0 END) AS without_embedding\n",
    "        FROM papers \n",
    "        WHERE categories LIKE '%cs.AI%'\n",
    "          AND abstract IS NOT NULL\n",
    "    ''')\n",
    "    stats = cursor.fetchone()\n",
    "    print(f\"\\n1. Coverage:\")\n",
    "    print(f\"- Total CS.AI papers: {stats['total_csai']}\")\n",
    "    print(f\"- With embeddings: {stats['with_embedding']} ({(stats['with_embedding']/stats['total_csai'])*100:.1f}%)\")\n",
    "    print(f\"- Missing embeddings: {stats['without_embedding']} ({(stats['without_embedding']/stats['total_csai'])*100:.1f}%)\")\n",
    "\n",
    "    # 2. Validity Check (NaN/Zero vectors)\n",
    "    cursor.execute('''\n",
    "        SELECT abstract_embedding \n",
    "        FROM papers \n",
    "        WHERE abstract_embedding IS NOT NULL\n",
    "    ''')\n",
    "    invalid_count = 0\n",
    "    total_checked = 0\n",
    "    for row in cursor:\n",
    "        embedding = np.frombuffer(row['abstract_embedding'], dtype=np.float32)\n",
    "        if np.isnan(embedding).any():\n",
    "            invalid_count += 1\n",
    "        elif np.all(embedding == 0):\n",
    "            invalid_count += 1\n",
    "        total_checked += 1\n",
    "    \n",
    "    print(f\"\\n2. Validity (sampled {total_checked}):\")\n",
    "    print(f\"- Invalid embeddings (NaN/zeros): {invalid_count} ({(invalid_count/total_checked)*100:.1f}%)\")\n",
    "\n",
    "    # 3. Norm Analysis\n",
    "    cursor.execute('''\n",
    "        SELECT abstract_embedding \n",
    "        FROM papers \n",
    "        WHERE abstract_embedding IS NOT NULL\n",
    "    ''')\n",
    "    norms = []\n",
    "    for row in cursor:\n",
    "        embedding = np.frombuffer(row['abstract_embedding'], dtype=np.float32)\n",
    "        norms.append(np.linalg.norm(embedding))\n",
    "    \n",
    "    print(f\"\\n3. Norm Analysis (sample):\")\n",
    "    print(f\"- Mean norm: {np.mean(norms):.2f}\")\n",
    "    print(f\"- Std dev: {np.std(norms):.2f}\")\n",
    "    print(f\"- Min/Max: {np.min(norms):.2f}/{np.max(norms):.2f}\")\n",
    "\n",
    "    # 4. Similarity Analysis\n",
    "    # Get random pairs more efficiently\n",
    "    cursor.execute('''\n",
    "        SELECT id, title, abstract, abstract_embedding \n",
    "        FROM papers \n",
    "        WHERE abstract_embedding IS NOT NULL\n",
    "          AND categories LIKE '%cs.AI%'\n",
    "          AND ABS(RANDOM() % 100) = 0  -- Fast random sampling\n",
    "        LIMIT 10000\n",
    "    ''')\n",
    "    papers = cursor.fetchall()\n",
    "    random_embeddings = np.vstack([np.frombuffer(row['abstract_embedding'], dtype=np.float32) for row in papers])\n",
    "    \n",
    "    # Analyze embedding components\n",
    "    print(\"\\n4. Embedding Analysis:\")\n",
    "    print(f\"- Shape: {random_embeddings.shape}\")\n",
    "    print(\"- Component statistics:\")\n",
    "    means = np.mean(random_embeddings, axis=0)\n",
    "    stds = np.std(random_embeddings, axis=0)\n",
    "    print(f\"  Mean of means: {np.mean(means):.3f} ± {np.std(means):.3f}\")\n",
    "    print(f\"  Mean of stds: {np.mean(stds):.3f} ± {np.std(stds):.3f}\")\n",
    "    print(f\"  Component range: [{np.min(random_embeddings):.3f}, {np.max(random_embeddings):.3f}]\")\n",
    "    \n",
    "    # Show distribution of components\n",
    "    positive_frac = np.mean(random_embeddings > 0)\n",
    "    print(f\"  Fraction of positive components: {positive_frac:.3f}\")\n",
    "    \n",
    "    # Compute similarities\n",
    "    n_samples = len(random_embeddings)\n",
    "    if n_samples > 1:\n",
    "        # Generate random pairs of indices\n",
    "        idx1 = np.random.randint(0, n_samples, size=1000)\n",
    "        idx2 = np.random.randint(0, n_samples, size=1000)\n",
    "        # Exclude self-pairs\n",
    "        valid_pairs = idx1 != idx2\n",
    "        idx1, idx2 = idx1[valid_pairs], idx2[valid_pairs]\n",
    "        \n",
    "        # Compute cosine similarities properly\n",
    "        similarities = cosine_similarity(random_embeddings[idx1], random_embeddings[idx2]).diagonal()\n",
    "        \n",
    "        print(\"\\n5. Similarity Analysis:\")\n",
    "        print(f\"- Random pairs (n={len(similarities)}):\")\n",
    "        print(f\"  Mean: {np.mean(similarities):.3f} ± {np.std(similarities):.3f}\")\n",
    "        print(f\"  Range: [{np.min(similarities):.3f}, {np.max(similarities):.3f}]\")\n",
    "        \n",
    "        # Print example pairs with different similarity levels\n",
    "        print(\"\\nExample pairs:\")\n",
    "        # Most similar pair\n",
    "        most_similar_idx = np.argmax(similarities)\n",
    "        print(f\"\\nMost similar pair (similarity: {similarities[most_similar_idx]:.3f}):\")\n",
    "        print(f\"Paper 1: {papers[idx1[most_similar_idx]]['title']}\")\n",
    "        print(f\"URL 1: https://arxiv.org/abs/{papers[idx1[most_similar_idx]]['id']}\")\n",
    "        print(f\"Abstract 1:\\n{papers[idx1[most_similar_idx]]['abstract']}\")\n",
    "        print(f\"\\nPaper 2: {papers[idx2[most_similar_idx]]['title']}\")\n",
    "        print(f\"URL 2: https://arxiv.org/abs/{papers[idx2[most_similar_idx]]['id']}\")\n",
    "        print(f\"Abstract 2:\\n{papers[idx2[most_similar_idx]]['abstract']}\")\n",
    "        \n",
    "        # Least similar pair\n",
    "        least_similar_idx = np.argmin(similarities)\n",
    "        print(f\"\\nLeast similar pair (similarity: {similarities[least_similar_idx]:.3f}):\")\n",
    "        print(f\"Paper 1: {papers[idx1[least_similar_idx]]['title']}\")\n",
    "        print(f\"URL 1: https://arxiv.org/abs/{papers[idx1[least_similar_idx]]['id']}\")\n",
    "        print(f\"Abstract 1:\\n{papers[idx1[least_similar_idx]]['abstract']}\")\n",
    "        print(f\"\\nPaper 2: {papers[idx2[least_similar_idx]]['title']}\")\n",
    "        print(f\"URL 2: https://arxiv.org/abs/{papers[idx2[least_similar_idx]]['id']}\")\n",
    "        print(f\"Abstract 2:\\n{papers[idx2[least_similar_idx]]['abstract']}\")\n",
    "        \n",
    "        # Median similarity pair\n",
    "        median_idx = np.argsort(similarities)[len(similarities)//2]\n",
    "        print(f\"\\nMedian similarity pair (similarity: {similarities[median_idx]:.3f}):\")\n",
    "        print(f\"Paper 1: {papers[idx1[median_idx]]['title']}\")\n",
    "        print(f\"URL 1: https://arxiv.org/abs/{papers[idx1[median_idx]]['id']}\")\n",
    "        print(f\"Abstract 1:\\n{papers[idx1[median_idx]]['abstract']}\")\n",
    "        print(f\"\\nPaper 2: {papers[idx2[median_idx]]['title']}\")\n",
    "        print(f\"URL 2: https://arxiv.org/abs/{papers[idx2[median_idx]]['id']}\")\n",
    "        print(f\"Abstract 2:\\n{papers[idx2[median_idx]]['abstract']}\")\n",
    "    else:\n",
    "        print(\"\\n5. Similarity Analysis:\")\n",
    "        print(\"Not enough samples for similarity analysis\")\n",
    "    \n",
    "    # # Get potential duplicates (papers with same title)\n",
    "    # cursor.execute('''\n",
    "    #     SELECT p1.id as id1, p2.id as id2,\n",
    "    #            p1.title as title1,\n",
    "    #            p1.abstract as abstract1,\n",
    "    #            p2.abstract as abstract2,\n",
    "    #            p1.abstract_embedding as e1,\n",
    "    #            p2.abstract_embedding as e2\n",
    "    #     FROM papers p1\n",
    "    #     JOIN papers p2 ON LOWER(TRIM(p1.title)) = LOWER(TRIM(p2.title))\n",
    "    #     WHERE p1.id < p2.id  -- Avoid self-joins and duplicates\n",
    "    #       AND p1.withdrawn = 0 AND p2.withdrawn = 0\n",
    "    #       AND p1.abstract_embedding IS NOT NULL\n",
    "    #       AND p2.abstract_embedding IS NOT NULL\n",
    "    #       AND p1.categories LIKE '%cs.AI%'\n",
    "    #       AND p2.categories LIKE '%cs.AI%'\n",
    "    #     LIMIT 50\n",
    "    # ''')\n",
    "    \n",
    "    # rows = cursor.fetchall()\n",
    "    # if rows:\n",
    "    #     # Process all pairs at once\n",
    "    #     e1 = np.vstack([np.frombuffer(row['e1'], dtype=np.float32) for row in rows])\n",
    "    #     e2 = np.vstack([np.frombuffer(row['e2'], dtype=np.float32) for row in rows])\n",
    "    #     similarities = np.sum(e1 * e2, axis=1)\n",
    "        \n",
    "    #     print(f\"\\n6. Same-title Analysis:\")\n",
    "    #     print(f\"- Same-title pairs (n={len(similarities)}):\")\n",
    "    #     print(f\"  Mean: {np.mean(similarities):.3f} ± {np.std(similarities):.3f}\")\n",
    "    #     print(f\"  Range: [{np.min(similarities):.3f}, {np.max(similarities):.3f}]\")\n",
    "        \n",
    "    #     # Print example same-title pairs\n",
    "    #     print(\"\\nExample same-title pairs:\")\n",
    "    #     # Most similar pair\n",
    "    #     most_similar_idx = np.argmax(similarities)\n",
    "    #     print(f\"\\nMost similar pair (similarity: {similarities[most_similar_idx]:.3f}):\")\n",
    "    #     print(f\"Title: {rows[most_similar_idx]['title1']}\")\n",
    "    #     print(f\"URL 1: https://arxiv.org/abs/{rows[most_similar_idx]['id1']}\")\n",
    "    #     print(f\"Abstract 1:\\n{rows[most_similar_idx]['abstract1']}\")\n",
    "    #     print(f\"\\nURL 2: https://arxiv.org/abs/{rows[most_similar_idx]['id2']}\")\n",
    "    #     print(f\"Abstract 2:\\n{rows[most_similar_idx]['abstract2']}\")\n",
    "        \n",
    "    #     # Least similar pair\n",
    "    #     least_similar_idx = np.argmin(similarities)\n",
    "    #     print(f\"\\nLeast similar pair (similarity: {similarities[least_similar_idx]:.3f}):\")\n",
    "    #     print(f\"Title: {rows[least_similar_idx]['title1']}\")\n",
    "    #     print(f\"URL 1: https://arxiv.org/abs/{rows[least_similar_idx]['id1']}\")\n",
    "    #     print(f\"Abstract 1:\\n{rows[least_similar_idx]['abstract1']}\")\n",
    "    #     print(f\"\\nURL 2: https://arxiv.org/abs/{rows[least_similar_idx]['id2']}\")\n",
    "    #     print(f\"Abstract 2:\\n{rows[least_similar_idx]['abstract2']}\")\n",
    "    # else:\n",
    "    #     print(\"\\n6. Same-title Analysis:\")\n",
    "    #     print(\"No same-title pairs found for comparison\")\n",
    "\n",
    "# Run validation\n",
    "validate_embeddings(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 6. Database Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy updated database back to Drive\n",
    "!cp {local_db} \"{db_path}\"\n",
    "print(\"Database backup completed to Google Drive\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
