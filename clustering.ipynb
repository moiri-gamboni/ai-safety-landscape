{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Papers - Clustering Phase\n",
    "\n",
    "This notebook performs clustering analysis on the paper embeddings to identify AI Safety relevant clusters:\n",
    "1. Loads paper embeddings from the database\n",
    "2. Performs UMAP dimensionality reduction (stored separately for reuse)\n",
    "3. Applies HDBSCAN clustering using stored UMAP embeddings\n",
    "4. Evaluates cluster quality and stores results\n",
    "\n",
    "Note: For visualizations and analysis, see visualizations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive # pyright: ignore [reportMissingImports]\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    %pip install numpy optuna permetrics hdbscan umap-learn # pyright: ignore\n",
    "    !git clone https://github.com/rapidsai/rapidsai-csp-utils.git # pyright: ignore\n",
    "    !python rapidsai-csp-utils/colab/pip-install.py # pyright: ignore\n",
    "\n",
    "# Core imports\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "# ML imports\n",
    "from cuml import UMAP\n",
    "from cuml.preprocessing import StandardScaler\n",
    "from cuml.cluster.hdbscan import HDBSCAN\n",
    "from cuml.metrics.trustworthiness import trustworthiness\n",
    "\n",
    "# Optimization imports\n",
    "import optuna\n",
    "from permetrics import ClusteringMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Database configuration\n",
    "db_path = \"/content/drive/MyDrive/ai-safety-papers/papers.db\"\n",
    "local_db = \"papers.db\"\n",
    "\n",
    "# Initialize database connection\n",
    "print(f\"Copying database to local storage: {local_db}\")\n",
    "if not os.path.exists(local_db):\n",
    "    %cp \"{db_path}\" {local_db} # pyright: ignore\n",
    "\n",
    "conn = sqlite3.connect(local_db)\n",
    "conn.row_factory = sqlite3.Row\n",
    "\n",
    "# Create tables\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# UMAP tables\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS umap_runs (\n",
    "    run_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    n_components INTEGER,\n",
    "    n_neighbors INTEGER,\n",
    "    min_dist REAL\n",
    ")''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS umap_results (\n",
    "    run_id INTEGER,\n",
    "    paper_id TEXT,\n",
    "    embedding BLOB,\n",
    "    PRIMARY KEY (run_id, paper_id),\n",
    "    FOREIGN KEY (run_id) REFERENCES umap_runs(run_id),\n",
    "    FOREIGN KEY (paper_id) REFERENCES papers(id)\n",
    ")''')\n",
    "\n",
    "# Clustering tables\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS clustering_runs (\n",
    "    run_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    umap_run_id INTEGER,\n",
    "    is_optimal BOOLEAN DEFAULT 0,\n",
    "    min_cluster_size INTEGER,\n",
    "    min_samples INTEGER,\n",
    "    cluster_selection_method TEXT,\n",
    "    cluster_selection_epsilon REAL,\n",
    "    trust_score REAL,\n",
    "    dbcvi_score REAL,\n",
    "    noise_ratio REAL,\n",
    "    n_clusters INTEGER,\n",
    "    mean_persistence REAL,\n",
    "    std_persistence REAL,\n",
    "    mean_cluster_size REAL,\n",
    "    std_cluster_size REAL,\n",
    "    cluster_size_ratio REAL,\n",
    "    FOREIGN KEY (umap_run_id) REFERENCES umap_runs(run_id)\n",
    ")''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS clustering_results (\n",
    "    run_id INTEGER,\n",
    "    paper_id TEXT,\n",
    "    cluster_id INTEGER,\n",
    "    cluster_prob REAL,\n",
    "    PRIMARY KEY (run_id, paper_id),\n",
    "    FOREIGN KEY (run_id) REFERENCES clustering_runs(run_id),\n",
    "    FOREIGN KEY (paper_id) REFERENCES papers(id)\n",
    ")''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS cluster_hierarchy (\n",
    "    run_id INTEGER,\n",
    "    parent_cluster_id INTEGER,\n",
    "    child_cluster_id INTEGER,\n",
    "    lambda_val REAL,\n",
    "    child_size INTEGER,\n",
    "    persistence REAL,\n",
    "    PRIMARY KEY (run_id, parent_cluster_id, child_cluster_id),\n",
    "    FOREIGN KEY (run_id) REFERENCES clustering_runs(run_id)\n",
    ")''')\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "    \"\"\"Load and standardize embeddings once\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT id, abstract_embedding \n",
    "        FROM papers \n",
    "        WHERE abstract_embedding IS NOT NULL AND withdrawn = 0\n",
    "    ''')\n",
    "    \n",
    "    results = cursor.fetchall()\n",
    "    if not results:\n",
    "        raise ValueError(\"No embeddings found in database\")\n",
    "    \n",
    "    # Initialize arrays\n",
    "    paper_ids = [row[0] for row in results]\n",
    "    \n",
    "    # Standardize once during initial load\n",
    "    scaler = StandardScaler()\n",
    "    raw_embeddings = np.array([np.frombuffer(row[1], dtype=np.float32) for row in results])\n",
    "    scaled_embeddings = scaler.fit_transform(raw_embeddings)\n",
    "    \n",
    "    print(f\"Loaded {len(paper_ids)} papers with standardized embeddings\")\n",
    "    return paper_ids, scaled_embeddings\n",
    "\n",
    "paper_ids, embeddings = load_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_existing_umap_run(n_components, n_neighbors, min_dist):\n",
    "    \"\"\"Check for existing UMAP run with matching parameters\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT run_id FROM umap_runs\n",
    "        WHERE n_components = ? AND n_neighbors = ? AND min_dist = ?\n",
    "    ''', (n_components, n_neighbors, min_dist))\n",
    "    result = cursor.fetchone()\n",
    "    return result['run_id'] if result else None\n",
    "\n",
    "def perform_umap_reduction(embeddings, n_components, n_neighbors, min_dist):\n",
    "    \"\"\"Use pre-scaled embeddings, only apply UMAP if needed\"\"\"\n",
    "    if n_components == 0:\n",
    "        print(\"Using pre-standardized embeddings without reduction\")\n",
    "        return embeddings\n",
    "    \n",
    "    print(f\"Performing {n_components}D UMAP reduction...\")\n",
    "    reducer = UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric='cosine',\n",
    "        verbose=True\n",
    "    )\n",
    "    return reducer.fit_transform(embeddings)\n",
    "\n",
    "def save_umap_run(paper_ids, embeddings, n_components, n_neighbors, min_dist):\n",
    "    \"\"\"Save UMAP results to database\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN')\n",
    "    \n",
    "    try:\n",
    "        cursor.execute('''\n",
    "            INSERT INTO umap_runs (n_components, n_neighbors, min_dist)\n",
    "            VALUES (?, ?, ?)\n",
    "        ''', (n_components, n_neighbors, min_dist))\n",
    "        run_id = cursor.lastrowid\n",
    "        \n",
    "        for pid, emb in zip(paper_ids, embeddings):\n",
    "            cursor.execute('''\n",
    "                INSERT INTO umap_results (run_id, paper_id, embedding)\n",
    "                VALUES (?, ?, ?)\n",
    "            ''', (run_id, pid, emb.astype(np.float32).tobytes()))\n",
    "        \n",
    "        conn.commit()\n",
    "        return run_id\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Failed to save UMAP run: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_umap_embeddings(run_id):\n",
    "    \"\"\"Load UMAP embeddings from database for a given run\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT paper_id, embedding \n",
    "        FROM umap_results \n",
    "        WHERE run_id = ?\n",
    "        ORDER BY paper_id\n",
    "    ''', (run_id,))\n",
    "    \n",
    "    results = cursor.fetchall()\n",
    "    if not results:\n",
    "        raise ValueError(f\"No embeddings found for run {run_id}\")\n",
    "    \n",
    "    paper_ids = [row[0] for row in results]\n",
    "    embeddings = np.array([np.frombuffer(row[1], dtype=np.float32) for row in results])\n",
    "    \n",
    "    return embeddings, paper_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. Optimization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def objective(trial, embeddings):\n",
    "    \"\"\"Optuna optimization objective function\"\"\"\n",
    "    # UMAP configuration\n",
    "    use_umap = trial.suggest_categorical('use_umap', [True, False])\n",
    "    umap_params = {\n",
    "        'min_dist': 0.0,\n",
    "        'n_components': 0,  # Default for no UMAP\n",
    "        'n_neighbors': 0    # Unused\n",
    "    }\n",
    "    \n",
    "    if use_umap:\n",
    "        umap_params.update({\n",
    "            'n_components': trial.suggest_int('n_components', 15, 100),\n",
    "            'n_neighbors': trial.suggest_int('n_neighbors', 30, 100)\n",
    "        })\n",
    "    \n",
    "    # Check for existing UMAP run\n",
    "    existing_umap_id = check_existing_umap_run(**umap_params)\n",
    "    \n",
    "    if existing_umap_id:\n",
    "        reduced_embeddings, _ = load_umap_embeddings(existing_umap_id)\n",
    "    else:\n",
    "        # Perform and save new UMAP reduction\n",
    "        reduced_embeddings = perform_umap_reduction(embeddings, **umap_params)\n",
    "        existing_umap_id = save_umap_run(paper_ids, reduced_embeddings, **umap_params)\n",
    "\n",
    "    # HDBSCAN parameters\n",
    "    min_cluster_size = trial.suggest_int('min_cluster_size', 20, 100)\n",
    "    hdbscan_params = {\n",
    "        'min_cluster_size': min_cluster_size,\n",
    "        'min_samples': trial.suggest_int('min_samples', 5, min_cluster_size//2),\n",
    "        'cluster_selection_method': trial.suggest_categorical('cluster_selection_method', ['eom', 'leaf']),\n",
    "        'cluster_selection_epsilon': trial.suggest_float('cluster_selection_epsilon', 0.0, 0.5)\n",
    "    }\n",
    "    \n",
    "    # Check for existing clustering run\n",
    "    existing_cluster_id = check_existing_clustering_run(\n",
    "        umap_run_id=existing_umap_id,\n",
    "        **hdbscan_params\n",
    "    )\n",
    "    \n",
    "    if existing_cluster_id:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('SELECT dbcvi_score FROM clustering_runs WHERE run_id = ?', (existing_cluster_id,))\n",
    "        return cursor.fetchone()['dbcvi_score']\n",
    "    \n",
    "    # Perform clustering\n",
    "    clusterer = HDBSCAN(\n",
    "        **hdbscan_params,\n",
    "        metric='euclidean',\n",
    "        prediction_data=True,\n",
    "        gen_min_span_tree=True\n",
    "    )\n",
    "    labels = clusterer.fit_predict(reduced_embeddings)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if not use_umap:\n",
    "        trust_score = 1.0  # Max score when using original embeddings\n",
    "    else:\n",
    "        trust_score = trustworthiness(embeddings, reduced_embeddings)\n",
    "    \n",
    "    cm = ClusteringMetric(X=reduced_embeddings, y_pred=labels)\n",
    "    dbcvi_score = cm.DBCVI()\n",
    "    \n",
    "    # Save results to DB\n",
    "    run_id = save_optimized_run(\n",
    "        existing_umap_id,\n",
    "        hdbscan_params,\n",
    "        clusterer,\n",
    "        trust_score,\n",
    "        dbcvi_score\n",
    "    )\n",
    "    \n",
    "    # Store hierarchy and create visualization embedding\n",
    "    save_cluster_hierarchy(run_id, clusterer.condensed_tree_)\n",
    "    create_visualization_embedding(umap_params, existing_umap_id)\n",
    "    \n",
    "    trial.set_user_attr('db_run_id', run_id)  # Store actual DB ID\n",
    "    return dbcvi_score\n",
    "\n",
    "def create_visualization_embedding(umap_params, main_run_id):\n",
    "    \"\"\"Ensure 2D visualization embedding exists\"\"\"\n",
    "    if umap_params['n_components'] == 2:\n",
    "        return\n",
    "    \n",
    "    viz_params = umap_params.copy()\n",
    "    viz_params['n_components'] = 2\n",
    "    viz_run_id = check_existing_umap_run(**viz_params)\n",
    "    \n",
    "    if not viz_run_id:\n",
    "        print(\"Creating 2D visualization embedding...\")\n",
    "        viz_embeddings = perform_umap_reduction(embeddings, **viz_params)\n",
    "        viz_run_id = save_umap_run(paper_ids, viz_embeddings, **viz_params)\n",
    "\n",
    "def check_existing_clustering_run(umap_run_id, **hdbscan_params):\n",
    "    \"\"\"Check for existing clustering run with these parameters\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT run_id FROM clustering_runs\n",
    "        WHERE umap_run_id = ?\n",
    "        AND min_cluster_size = ?\n",
    "        AND min_samples = ?\n",
    "        AND cluster_selection_method = ?\n",
    "        AND cluster_selection_epsilon = ?\n",
    "    ''', (\n",
    "        umap_run_id,\n",
    "        hdbscan_params['min_cluster_size'],\n",
    "        hdbscan_params['min_samples'],\n",
    "        hdbscan_params['cluster_selection_method'],\n",
    "        hdbscan_params['cluster_selection_epsilon']\n",
    "    ))\n",
    "    result = cursor.fetchone()\n",
    "    return result['run_id'] if result else None\n",
    "\n",
    "def analyze_hierarchy(clusterer):\n",
    "    \"\"\"Remove error suppression for persistence metrics\"\"\"\n",
    "    persistence = clusterer.cluster_persistence_\n",
    "    stats = {\n",
    "        'mean_persistence': np.mean(persistence),\n",
    "        'std_persistence': np.std(persistence)\n",
    "    }\n",
    "    \n",
    "    cluster_sizes = [np.sum(clusterer.labels_ == label) \n",
    "                    for label in np.unique(clusterer.labels_) if label != -1]\n",
    "    \n",
    "    # Require valid clusters\n",
    "    if not cluster_sizes:\n",
    "        raise ValueError(\"No clusters found - all points labeled as noise\")\n",
    "    \n",
    "    stats.update({\n",
    "        'mean_cluster_size': np.mean(cluster_sizes),\n",
    "        'std_cluster_size': np.std(cluster_sizes),\n",
    "        'cluster_size_ratio': (max(cluster_sizes) / min(cluster_sizes))\n",
    "    })\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def save_optimized_run(umap_run_id, hdbscan_params, clusterer, trust_score, dbcvi_score):\n",
    "    \"\"\"Save optimized clustering results to database\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    hierarchy_stats = analyze_hierarchy(clusterer)\n",
    "    \n",
    "    cursor.execute('''\n",
    "        INSERT INTO clustering_runs (\n",
    "            umap_run_id, min_cluster_size, min_samples, cluster_selection_method, cluster_selection_epsilon,\n",
    "            trust_score, dbcvi_score, noise_ratio, n_clusters,\n",
    "            mean_persistence, std_persistence, mean_cluster_size, std_cluster_size, cluster_size_ratio\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        umap_run_id,\n",
    "        hdbscan_params['min_cluster_size'],\n",
    "        hdbscan_params['min_samples'],\n",
    "        hdbscan_params['cluster_selection_method'],\n",
    "        hdbscan_params['cluster_selection_epsilon'],\n",
    "        trust_score,\n",
    "        dbcvi_score,\n",
    "        np.sum(clusterer.labels_ == -1) / len(clusterer.labels_),\n",
    "        len(np.unique(clusterer.labels_[clusterer.labels_ != -1])),\n",
    "        hierarchy_stats['mean_persistence'],\n",
    "        hierarchy_stats['std_persistence'],\n",
    "        hierarchy_stats.get('mean_cluster_size', 0),\n",
    "        hierarchy_stats.get('std_cluster_size', 0),\n",
    "        hierarchy_stats.get('cluster_size_ratio', 0)\n",
    "    ))\n",
    "    \n",
    "    run_id = cursor.lastrowid\n",
    "    conn.commit()\n",
    "    return run_id\n",
    "\n",
    "def save_cluster_hierarchy(run_id, condensed_tree):\n",
    "    \"\"\"Save cluster hierarchy data\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    for row in condensed_tree.itertuples():\n",
    "        if row.child_size > 1:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO cluster_hierarchy\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                run_id,\n",
    "                int(row.parent),\n",
    "                int(row.child),\n",
    "                float(row.lambda_val),\n",
    "                int(row.child_size),\n",
    "                float(row.persistence)\n",
    "            ))\n",
    "    conn.commit()\n",
    "\n",
    "def optimize_clustering(embeddings, n_trials=100):\n",
    "    \"\"\"Run optimization study\"\"\"\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize'\n",
    "    )\n",
    "    \n",
    "    # Add logging callback\n",
    "    def log_trial(study, trial):\n",
    "        print(f\"\\nTrial {trial.number} finished:\")\n",
    "        print(f\"Params: {trial.params}\")\n",
    "        print(f\"Value: {trial.value:.3f}\")\n",
    "    \n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, embeddings),\n",
    "        n_trials=n_trials,\n",
    "        callbacks=[log_trial]\n",
    "    )\n",
    "    \n",
    "    # Fix best run marking\n",
    "    best_run_id = study.best_trial.user_attrs['db_run_id']\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('UPDATE clustering_runs SET is_optimal = 1 WHERE run_id = ?', \n",
    "                  (best_run_id,))\n",
    "    conn.commit()\n",
    "    \n",
    "    print(f\"\\nBest trial ({study.best_trial.number}):\")\n",
    "    print(f\"Score: {study.best_trial.value:.3f}\")\n",
    "    print(\"Parameters:\", study.best_trial.params)\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 6. Run Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optimize_clustering(embeddings, n_trials=100)\n",
    "print(\"Optimization complete! Best parameters saved to database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 7. Database Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "# Copy updated database back to Drive\n",
    "%cp {local_db} \"{db_path}\" # pyright: ignore\n",
    "print(\"Database backup completed to Google Drive\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
