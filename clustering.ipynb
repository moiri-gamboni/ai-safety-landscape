{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Papers - Clustering Phase\n",
    "\n",
    "This notebook performs clustering analysis on the paper embeddings to identify AI Safety relevant clusters:\n",
    "1. Loads paper embeddings from the database\n",
    "2. Performs UMAP dimensionality reduction (stored separately for reuse)\n",
    "3. Applies HDBSCAN clustering using stored UMAP embeddings\n",
    "4. Evaluates cluster quality and stores results\n",
    "\n",
    "Note: For visualizations and analysis, see visualizations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive # pyright: ignore [reportMissingImports]\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    %pip install optuna hdbscan umap-learn numpy cupy-cuda12x # pyright: ignore\n",
    "    !git clone https://github.com/rapidsai/rapidsai-csp-utils.git # pyright: ignore\n",
    "    !python rapidsai-csp-utils/colab/pip-install.py # pyright: ignore\n",
    "\n",
    "# Core imports\n",
    "import sqlite3\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "# ML imports\n",
    "from cuml import UMAP\n",
    "from cuml.preprocessing import StandardScaler\n",
    "from cuml.cluster.hdbscan import HDBSCAN\n",
    "from cuml.metrics.trustworthiness import trustworthiness\n",
    "import cuml\n",
    "cuml.set_global_output_type('cupy')\n",
    "\n",
    "# Optimization imports\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Database configuration\n",
    "db_path = \"/content/drive/MyDrive/ai-safety-papers/papers.db\"\n",
    "local_db = \"papers.db\"\n",
    "\n",
    "# Initialize database connection\n",
    "print(f\"Copying database to local storage: {local_db}\")\n",
    "if not os.path.exists(local_db):\n",
    "    %cp \"{db_path}\" {local_db} # pyright: ignore\n",
    "\n",
    "conn = sqlite3.connect(local_db)\n",
    "conn.row_factory = sqlite3.Row\n",
    "\n",
    "# Create tables\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# UMAP tables\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS umap_runs (\n",
    "    run_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    n_components INTEGER,\n",
    "    n_neighbors INTEGER,\n",
    "    min_dist REAL\n",
    ")''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS umap_results (\n",
    "    run_id INTEGER,\n",
    "    paper_id TEXT,\n",
    "    embedding BLOB,\n",
    "    PRIMARY KEY (run_id, paper_id),\n",
    "    FOREIGN KEY (run_id) REFERENCES umap_runs(run_id),\n",
    "    FOREIGN KEY (paper_id) REFERENCES papers(id)\n",
    ")''')\n",
    "\n",
    "# Clustering tables\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS clustering_runs (\n",
    "    run_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    umap_run_id INTEGER,\n",
    "    is_optimal BOOLEAN DEFAULT 0,\n",
    "    min_cluster_size INTEGER,\n",
    "    min_samples INTEGER,\n",
    "    cluster_selection_method TEXT,\n",
    "    cluster_selection_epsilon REAL,\n",
    "    trust_score REAL,\n",
    "    dbcvi_score REAL,\n",
    "    noise_ratio REAL,\n",
    "    n_clusters INTEGER,\n",
    "    mean_persistence REAL,\n",
    "    std_persistence REAL,\n",
    "    mean_cluster_size REAL,\n",
    "    std_cluster_size REAL,\n",
    "    cluster_size_ratio REAL,\n",
    "    FOREIGN KEY (umap_run_id) REFERENCES umap_runs(run_id)\n",
    ")''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS clustering_results (\n",
    "    run_id INTEGER,\n",
    "    paper_id TEXT,\n",
    "    cluster_id INTEGER,\n",
    "    cluster_prob REAL,\n",
    "    PRIMARY KEY (run_id, paper_id),\n",
    "    FOREIGN KEY (run_id) REFERENCES clustering_runs(run_id),\n",
    "    FOREIGN KEY (paper_id) REFERENCES papers(id)\n",
    ")''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS cluster_hierarchy (\n",
    "    run_id INTEGER,\n",
    "    parent_cluster_id INTEGER,\n",
    "    child_cluster_id INTEGER,\n",
    "    lambda_val REAL,\n",
    "    child_size INTEGER,\n",
    "    PRIMARY KEY (run_id, parent_cluster_id, child_cluster_id),\n",
    "    FOREIGN KEY (run_id) REFERENCES clustering_runs(run_id)\n",
    ")''')\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "    \"\"\"Load and standardize embeddings once\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT id, abstract_embedding \n",
    "        FROM papers \n",
    "        WHERE abstract_embedding IS NOT NULL AND withdrawn = 0\n",
    "    ''')\n",
    "    \n",
    "    results = cursor.fetchall()\n",
    "    if not results:\n",
    "        raise ValueError(\"No embeddings found in database\")\n",
    "    \n",
    "    # Initialize arrays\n",
    "    paper_ids = [row[0] for row in results]\n",
    "    \n",
    "    # Pure cupy buffer conversion\n",
    "    raw_embeddings = cp.array([cp.frombuffer(row[1], dtype=cp.float32) for row in results])\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(raw_embeddings)\n",
    "    \n",
    "    print(f\"Loaded {len(paper_ids)} papers with standardized embeddings\")\n",
    "    return paper_ids, scaled_embeddings\n",
    "\n",
    "paper_ids, embeddings = load_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_existing_umap_run(n_components, n_neighbors, min_dist):\n",
    "    \"\"\"Check for existing UMAP run with matching parameters\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT run_id FROM umap_runs\n",
    "        WHERE n_components = ? AND n_neighbors = ? AND min_dist = ?\n",
    "    ''', (n_components, n_neighbors, min_dist))\n",
    "    result = cursor.fetchone()\n",
    "    return result['run_id'] if result else None\n",
    "\n",
    "def perform_umap_reduction(embeddings, n_components, n_neighbors, min_dist):\n",
    "    \"\"\"Use pre-scaled embeddings, only apply UMAP if needed\"\"\"\n",
    "    if n_components == 0:\n",
    "        print(\"Using pre-standardized embeddings without reduction\")\n",
    "        return embeddings\n",
    "    \n",
    "    print(f\"\\nPerforming {n_components}D UMAP reduction with parameters:\")\n",
    "    print(f\"n_neighbors: {n_neighbors}, min_dist: {min_dist}\")\n",
    "    \n",
    "    reducer = UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric='cosine',\n",
    "        verbose=True,\n",
    "        output_type='cupy'\n",
    "    )\n",
    "    result = reducer.fit_transform(embeddings)\n",
    "    print(f\"UMAP reduction complete. Output shape: {result.shape}\")\n",
    "    return result\n",
    "\n",
    "def save_umap_run(paper_ids, embeddings, n_components, n_neighbors, min_dist):\n",
    "    \"\"\"Save UMAP results to database\"\"\"\n",
    "    print(f\"\\nSaving UMAP results to database (n={len(paper_ids)})...\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN')\n",
    "    \n",
    "    try:\n",
    "        cursor.execute('''\n",
    "            INSERT INTO umap_runs (n_components, n_neighbors, min_dist)\n",
    "            VALUES (?, ?, ?)\n",
    "        ''', (n_components, n_neighbors, min_dist))\n",
    "        run_id = cursor.lastrowid\n",
    "        \n",
    "        for pid, emb in zip(paper_ids, embeddings):\n",
    "            cursor.execute('''\n",
    "                INSERT INTO umap_results (run_id, paper_id, embedding)\n",
    "                VALUES (?, ?, ?)\n",
    "            ''', (run_id, pid, emb.astype(cp.float32).get().tobytes()))\n",
    "        \n",
    "        conn.commit()\n",
    "        print(f\"Saved UMAP run {run_id} with {len(paper_ids)} entries\")\n",
    "        return run_id\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Failed to save UMAP run: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_umap_embeddings(run_id):\n",
    "    \"\"\"Load UMAP embeddings from database for a given run\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT paper_id, embedding \n",
    "        FROM umap_results \n",
    "        WHERE run_id = ?\n",
    "        ORDER BY paper_id\n",
    "    ''', (run_id,))\n",
    "    \n",
    "    results = cursor.fetchall()\n",
    "    if not results:\n",
    "        raise ValueError(f\"No embeddings found for run {run_id}\")\n",
    "    \n",
    "    paper_ids = [row[0] for row in results]\n",
    "    embeddings = cp.array([cp.frombuffer(row[1], dtype=cp.float32) for row in results])\n",
    "    \n",
    "    return embeddings, paper_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. Optimization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_relative_validity(minimum_spanning_tree, labels):\n",
    "    \"\"\"CPU-based relative validity score using HDBSCAN's MST\"\"\"\n",
    "    # Convert labels to numpy array for CPU operations\n",
    "    labels = cp.asnumpy(labels)  # Move to CPU\n",
    "    \n",
    "    # Extract edge information from MST (already CPU-based)\n",
    "    mst_df = minimum_spanning_tree.to_pandas()\n",
    "    \n",
    "    # Initialize metrics\n",
    "    noise_mask = labels == -1\n",
    "    valid_labels = labels[~noise_mask]\n",
    "    \n",
    "    if valid_labels.size == 0:\n",
    "        return -1.0  # All noise case\n",
    "    \n",
    "    cluster_sizes = np.bincount(valid_labels)\n",
    "    num_clusters = len(cluster_sizes)\n",
    "    total = len(labels)\n",
    "    \n",
    "    # Use numpy instead of cupy\n",
    "    DSC = np.zeros(num_clusters, dtype=np.float32)\n",
    "    DSPC_wrt = np.ones(num_clusters, dtype=np.float32) * np.inf\n",
    "    max_distance = 0.0\n",
    "    min_outlier_sep = np.inf\n",
    "\n",
    "    # Process edges using vectorized operations\n",
    "    edge_data = mst_df[['from', 'to', 'distance']].values\n",
    "    for from_idx, to_idx, length in edge_data:\n",
    "        max_distance = max(max_distance, length)\n",
    "        \n",
    "        label1 = labels[int(from_idx)]\n",
    "        label2 = labels[int(to_idx)]\n",
    "        \n",
    "        if label1 == -1 and label2 == -1:\n",
    "            continue\n",
    "        elif label1 == -1 or label2 == -1:\n",
    "            min_outlier_sep = min(min_outlier_sep, length)\n",
    "            continue\n",
    "            \n",
    "        if label1 == label2:\n",
    "            DSC[label1] = max(length, DSC[label1])\n",
    "        else:\n",
    "            DSPC_wrt[label1] = min(length, DSPC_wrt[label1])\n",
    "            DSPC_wrt[label2] = min(length, DSPC_wrt[label2])\n",
    "\n",
    "    # Handle edge cases\n",
    "    if np.isinf(min_outlier_sep):\n",
    "        min_outlier_sep = max_distance if num_clusters > 1 else max_distance\n",
    "        \n",
    "    # Correct infinite values\n",
    "    correction = 2.0 * (max_distance if num_clusters > 1 else min_outlier_sep)\n",
    "    DSPC_wrt = np.where(DSPC_wrt == np.inf, correction, DSPC_wrt)\n",
    "    \n",
    "    # Compute final score\n",
    "    V_index = (DSPC_wrt - DSC) / np.maximum(DSPC_wrt, DSC)\n",
    "    weighted_V = (cluster_sizes * V_index) / total\n",
    "    return float(np.sum(weighted_V))\n",
    "\n",
    "def objective(trial, embeddings):\n",
    "    \"\"\"Optuna optimization objective function\"\"\"\n",
    "    # UMAP configuration\n",
    "    use_umap = trial.suggest_categorical('use_umap', [True, False])\n",
    "    umap_params = {\n",
    "        'min_dist': 0.0,\n",
    "        'n_components': 0,  # Default for no UMAP\n",
    "        'n_neighbors': 0    # Unused\n",
    "    }\n",
    "    \n",
    "    if use_umap:\n",
    "        umap_params.update({\n",
    "            'n_components': trial.suggest_int('n_components', 15, 100),\n",
    "            'n_neighbors': trial.suggest_int('n_neighbors', 30, 100)\n",
    "        })\n",
    "    \n",
    "    # Check for existing UMAP run\n",
    "    existing_umap_id = check_existing_umap_run(**umap_params)\n",
    "    \n",
    "    if existing_umap_id:\n",
    "        reduced_embeddings, _ = load_umap_embeddings(existing_umap_id)\n",
    "    else:\n",
    "        # Perform and save new UMAP reduction\n",
    "        reduced_embeddings = perform_umap_reduction(embeddings, **umap_params)\n",
    "        existing_umap_id = save_umap_run(paper_ids, reduced_embeddings, **umap_params)\n",
    "\n",
    "    # HDBSCAN parameters\n",
    "    min_cluster_size = trial.suggest_int('min_cluster_size', 20, 100)\n",
    "    hdbscan_params = {\n",
    "        'min_cluster_size': min_cluster_size,\n",
    "        'min_samples': trial.suggest_int('min_samples', 5, min_cluster_size//2),\n",
    "        'cluster_selection_method': trial.suggest_categorical('cluster_selection_method', ['eom', 'leaf']),\n",
    "        'cluster_selection_epsilon': trial.suggest_float('cluster_selection_epsilon', 0.0, 0.5)\n",
    "    }\n",
    "    \n",
    "    # Add HDBSCAN parameter print\n",
    "    print(\"\\nUsing HDBSCAN parameters:\")\n",
    "    print(f\"min_cluster_size: {hdbscan_params['min_cluster_size']}\")\n",
    "    print(f\"min_samples: {hdbscan_params['min_samples']}\")\n",
    "    print(f\"cluster_selection_method: {hdbscan_params['cluster_selection_method']}\")\n",
    "    print(f\"cluster_selection_epsilon: {hdbscan_params['cluster_selection_epsilon']}\")\n",
    "    \n",
    "    # Check for existing clustering run\n",
    "    existing_cluster_id = check_existing_clustering_run(\n",
    "        umap_run_id=existing_umap_id,\n",
    "        **hdbscan_params\n",
    "    )\n",
    "    \n",
    "    if existing_cluster_id:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('SELECT dbcvi_score FROM clustering_runs WHERE run_id = ?', (existing_cluster_id,))\n",
    "        score = cursor.fetchone()['dbcvi_score']\n",
    "        \n",
    "        # Add this critical line to propagate the existing run ID\n",
    "        trial.set_user_attr('db_run_id', existing_cluster_id)\n",
    "        return score\n",
    "    \n",
    "    # Perform clustering\n",
    "    clusterer = HDBSCAN(\n",
    "        **hdbscan_params,\n",
    "        metric='euclidean',\n",
    "        prediction_data=True,\n",
    "        gen_min_span_tree=True,\n",
    "        output_type='cupy'\n",
    "    )\n",
    "    labels = clusterer.fit_predict(reduced_embeddings)\n",
    "    \n",
    "    print(f\"\\nClustering complete. Found {len(cp.unique(labels))-1} clusters \"\n",
    "          f\"({cp.sum(labels == -1).item()} noise points)\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(\"\\nCalculating trustworthiness...\")\n",
    "    if not use_umap:\n",
    "        trust_score = 1.0  # Max score when using original embeddings\n",
    "    else:\n",
    "        trust_score = trustworthiness(embeddings, reduced_embeddings)\n",
    "    \n",
    "    print(\"\\nCalculating DBCVI score...\")\n",
    "    dbcvi_score = compute_relative_validity(clusterer.minimum_spanning_tree_, labels)\n",
    "    \n",
    "    # Save results to DB\n",
    "    run_id = save_optimized_run(\n",
    "        existing_umap_id,\n",
    "        hdbscan_params,\n",
    "        clusterer,\n",
    "        trust_score,\n",
    "        dbcvi_score\n",
    "    )\n",
    "    \n",
    "    # Store hierarchy and create visualization embedding\n",
    "    save_cluster_hierarchy(run_id, clusterer.condensed_tree_)\n",
    "    create_visualization_embedding(umap_params, existing_umap_id)\n",
    "    \n",
    "    trial.set_user_attr('db_run_id', run_id)  # Store actual DB ID\n",
    "    return dbcvi_score\n",
    "\n",
    "def create_visualization_embedding(umap_params, main_run_id):\n",
    "    \"\"\"Ensure 2D visualization embedding exists\"\"\"\n",
    "    if umap_params['n_components'] == 2:\n",
    "        return\n",
    "    \n",
    "    viz_params = umap_params.copy()\n",
    "    viz_params['n_components'] = 2\n",
    "    viz_run_id = check_existing_umap_run(**viz_params)\n",
    "    \n",
    "    if viz_run_id:\n",
    "        print(f\"Using existing 2D visualization embedding (run {viz_run_id})\")\n",
    "    else:\n",
    "        print(\"\\nCreating new 2D visualization embedding...\")\n",
    "        viz_embeddings = perform_umap_reduction(embeddings, **viz_params)\n",
    "        viz_run_id = save_umap_run(paper_ids, viz_embeddings, **viz_params)\n",
    "        print(f\"Created 2D visualization embedding (run {viz_run_id})\")\n",
    "\n",
    "def check_existing_clustering_run(umap_run_id, **hdbscan_params):\n",
    "    \"\"\"Check for existing clustering run with these parameters\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT run_id FROM clustering_runs\n",
    "        WHERE umap_run_id = ?\n",
    "        AND min_cluster_size = ?\n",
    "        AND min_samples = ?\n",
    "        AND cluster_selection_method = ?\n",
    "        AND cluster_selection_epsilon = ?\n",
    "    ''', (\n",
    "        umap_run_id,\n",
    "        hdbscan_params['min_cluster_size'],\n",
    "        hdbscan_params['min_samples'],\n",
    "        hdbscan_params['cluster_selection_method'],\n",
    "        hdbscan_params['cluster_selection_epsilon']\n",
    "    ))\n",
    "    result = cursor.fetchone()\n",
    "    return result['run_id'] if result else None\n",
    "\n",
    "def analyze_hierarchy(clusterer):\n",
    "    \"\"\"Remove error suppression for persistence metrics\"\"\"\n",
    "    persistence = clusterer.cluster_persistence_\n",
    "    stats = {\n",
    "        'mean_persistence': cp.mean(persistence),\n",
    "        'std_persistence': cp.std(persistence)\n",
    "    }\n",
    "    \n",
    "    # Get valid labels (exclude noise)\n",
    "    labels = clusterer.labels_\n",
    "    valid_mask = labels != -1\n",
    "    valid_labels = labels[valid_mask]\n",
    "    \n",
    "    if valid_labels.size == 0:\n",
    "        raise ValueError(\"No clusters found - all points labeled as noise\")\n",
    "    \n",
    "    # Calculate cluster sizes using bincount\n",
    "    cluster_sizes = cp.bincount(valid_labels)\n",
    "    \n",
    "    stats.update({\n",
    "        'mean_cluster_size': cluster_sizes.mean(),\n",
    "        'std_cluster_size': cluster_sizes.std(),\n",
    "        'cluster_size_ratio': (cluster_sizes.max() / cluster_sizes.min())\n",
    "    })\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def save_optimized_run(umap_run_id, hdbscan_params, clusterer, trust_score, dbcvi_score):\n",
    "    \"\"\"Save optimized clustering results to database\"\"\"\n",
    "    print(\"\\nSaving clustering metrics to database...\")\n",
    "    cursor = conn.cursor()\n",
    "    hierarchy_stats = analyze_hierarchy(clusterer)\n",
    "    \n",
    "    cursor.execute('''\n",
    "        INSERT INTO clustering_runs (\n",
    "            umap_run_id, min_cluster_size, min_samples, cluster_selection_method, cluster_selection_epsilon,\n",
    "            trust_score, dbcvi_score, noise_ratio, n_clusters,\n",
    "            mean_persistence, std_persistence, mean_cluster_size, std_cluster_size, cluster_size_ratio\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        umap_run_id,\n",
    "        hdbscan_params['min_cluster_size'],\n",
    "        hdbscan_params['min_samples'],\n",
    "        hdbscan_params['cluster_selection_method'],\n",
    "        hdbscan_params['cluster_selection_epsilon'],\n",
    "        trust_score,\n",
    "        dbcvi_score,\n",
    "        cp.sum(clusterer.labels_ == -1).item() / len(clusterer.labels_),\n",
    "        len(cp.unique(clusterer.labels_[clusterer.labels_ != -1])),\n",
    "        hierarchy_stats['mean_persistence'].item(),\n",
    "        hierarchy_stats['std_persistence'].item(),\n",
    "        hierarchy_stats.get('mean_cluster_size', 0).item(),\n",
    "        hierarchy_stats.get('std_cluster_size', 0).item(),\n",
    "        hierarchy_stats.get('cluster_size_ratio', 0).item()\n",
    "    ))\n",
    "    \n",
    "    run_id = cursor.lastrowid\n",
    "    conn.commit()\n",
    "    print(f\"Saved clustering run {run_id} with {len(cp.unique(clusterer.labels_[clusterer.labels_ != -1]))} clusters\")\n",
    "    return run_id\n",
    "\n",
    "def save_cluster_hierarchy(run_id, condensed_tree):\n",
    "    \"\"\"Save cluster hierarchy relationships (CPU-based)\"\"\"\n",
    "    print(\"\\nSaving cluster hierarchy relationships...\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Convert condensed tree to pandas DataFrame\n",
    "    tree_df = condensed_tree.to_pandas()\n",
    "    \n",
    "    # Filter meaningful relationships (exclude single-point clusters)\n",
    "    meaningful_edges = tree_df[tree_df.child_size > 1]\n",
    "\n",
    "    # Batch insert using executemany with correct columns\n",
    "    cursor.executemany('''\n",
    "        INSERT INTO cluster_hierarchy\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    ''', [\n",
    "        (run_id, int(row.parent), int(row.child), \n",
    "         float(row.lambda_val), int(row.child_size))\n",
    "        for row in meaningful_edges.itertuples()\n",
    "    ])\n",
    "    \n",
    "    conn.commit()\n",
    "    print(f\"Saved {len(meaningful_edges)} hierarchy relationships for run {run_id}\")\n",
    "\n",
    "def optimize_clustering(embeddings, n_trials=50):\n",
    "    \"\"\"Run optimization study\"\"\"\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    # Track best run across all trials\n",
    "    best_score = -float('inf')\n",
    "    best_run_id = None\n",
    "    \n",
    "    def log_and_update_best(study, trial):\n",
    "        \"\"\"Enhanced callback with continuous best run tracking\"\"\"\n",
    "        nonlocal best_score, best_run_id\n",
    "        \n",
    "        print(f\"\\nTrial {trial.number} finished:\")\n",
    "        print(f\"Params: {trial.params}\")\n",
    "        print(f\"Value: {trial.value:.3f}\")\n",
    "        \n",
    "        # Update best run if improved\n",
    "        current_best = study.best_trial\n",
    "        if current_best.value > best_score:\n",
    "            best_score = current_best.value\n",
    "            new_best_id = current_best.user_attrs['db_run_id']\n",
    "            \n",
    "            if new_best_id != best_run_id:\n",
    "                print(f\"New best run found! Updating marker to run {new_best_id}\")\n",
    "                cursor = conn.cursor()\n",
    "                cursor.execute('UPDATE clustering_runs SET is_optimal = 0')\n",
    "                cursor.execute('''\n",
    "                    UPDATE clustering_runs \n",
    "                    SET is_optimal = 1 \n",
    "                    WHERE run_id = ?\n",
    "                ''', (new_best_id,))\n",
    "                conn.commit()\n",
    "                best_run_id = new_best_id\n",
    "    \n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, embeddings),\n",
    "        n_trials=n_trials,\n",
    "        callbacks=[log_and_update_best]  # Use enhanced callback\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFinal best trial ({study.best_trial.number}):\")\n",
    "    print(f\"Score: {study.best_trial.value:.3f}\")\n",
    "    print(\"Parameters:\", study.best_trial.params)\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 6. Run Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optimize_clustering(embeddings, n_trials=100)\n",
    "print(\"Optimization complete! Best parameters saved to database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 7. Database Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "# Copy updated database back to Drive\n",
    "print(\"\\nStarting database backup to Google Drive...\")\n",
    "%cp {local_db} \"{db_path}\" # pyright: ignore\n",
    "print(\"Backup completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
