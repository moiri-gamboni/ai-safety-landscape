{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Papers - Clustering Phase\n",
    "\n",
    "This notebook performs clustering analysis on the paper embeddings to identify AI Safety relevant clusters:\n",
    "1. Loads paper embeddings from the database\n",
    "2. Performs UMAP dimensionality reduction (stored separately for reuse)\n",
    "3. Applies HDBSCAN clustering using stored UMAP embeddings\n",
    "4. Evaluates cluster quality and stores results\n",
    "\n",
    "Note: For visualizations and analysis, see visualizations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Install only the packages needed for this notebook\n",
    "    %pip install --extra-index-url=https://pypi.nvidia.com numpy scikit-learn cuml-cu12==24.12.* tqdm\n",
    "\n",
    "# Core imports\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ML imports - fail fast if GPU versions aren't available\n",
    "from cuml import UMAP  # GPU-accelerated UMAP\n",
    "from cuml.preprocessing import StandardScaler\n",
    "from cuml.cluster.hdbscan import HDBSCAN  # GPU-accelerated HDBSCAN\n",
    "from cuml.metrics.cluster.silhouette_score import cython_silhouette_score as silhouette_score\n",
    "from cuml.metrics.trustworthiness import trustworthiness\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Path to database\n",
    "db_path = \"/content/drive/MyDrive/ai-safety-papers/papers.db\"\n",
    "local_db = \"papers.db\"\n",
    "\n",
    "# Copy database to local storage if needed\n",
    "print(f\"Copying database to local storage: {local_db}\")\n",
    "if not os.path.exists(local_db):\n",
    "    %cp \"{db_path}\" {local_db}\n",
    "\n",
    "conn = sqlite3.connect(local_db)\n",
    "conn.row_factory = sqlite3.Row\n",
    "\n",
    "# Create UMAP runs table\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS umap_runs (\n",
    "    run_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    \n",
    "    -- UMAP parameters\n",
    "    n_components INTEGER,\n",
    "    n_neighbors INTEGER,\n",
    "    min_dist REAL\n",
    ")\n",
    "''')\n",
    "\n",
    "# Create UMAP results table\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS umap_results (\n",
    "    run_id INTEGER,\n",
    "    paper_id TEXT,\n",
    "    embedding BLOB,\n",
    "    PRIMARY KEY (run_id, paper_id),\n",
    "    FOREIGN KEY (run_id) REFERENCES umap_runs(run_id),\n",
    "    FOREIGN KEY (paper_id) REFERENCES papers(id)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Create clustering runs table\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS clustering_runs (\n",
    "    run_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    umap_run_id INTEGER,\n",
    "    \n",
    "    -- HDBSCAN parameters\n",
    "    min_cluster_size INTEGER,\n",
    "    min_samples INTEGER,\n",
    "    cluster_selection_method TEXT,\n",
    "    \n",
    "    -- Metrics\n",
    "    n_clusters INTEGER,\n",
    "    noise_ratio REAL,\n",
    "    silhouette_score REAL,\n",
    "    davies_bouldin_score REAL,\n",
    "    calinski_harabasz_score REAL,\n",
    "    avg_coherence REAL,\n",
    "    avg_separation REAL,\n",
    "    \n",
    "    FOREIGN KEY (umap_run_id) REFERENCES umap_runs(run_id)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Create clustering results table\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS clustering_results (\n",
    "    run_id INTEGER,\n",
    "    paper_id TEXT,\n",
    "    cluster_id INTEGER,\n",
    "    cluster_prob REAL,\n",
    "    PRIMARY KEY (run_id, paper_id),\n",
    "    FOREIGN KEY (run_id) REFERENCES clustering_runs(run_id),\n",
    "    FOREIGN KEY (paper_id) REFERENCES papers(id)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Create hierarchy table\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS cluster_hierarchy (\n",
    "    run_id INTEGER,\n",
    "    parent_cluster_id INTEGER,\n",
    "    child_cluster_id INTEGER,\n",
    "    lambda_val REAL,           -- Split level in hierarchy\n",
    "    child_size INTEGER,        -- Number of papers in child cluster\n",
    "    PRIMARY KEY (run_id, parent_cluster_id, child_cluster_id),\n",
    "    FOREIGN KEY (run_id) REFERENCES clustering_runs(run_id)\n",
    ")\n",
    "''')\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "    \"\"\"Load paper embeddings from database\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"Loading papers with embeddings...\")\n",
    "    \n",
    "    # First get the total count\n",
    "    cursor.execute('''\n",
    "        SELECT COUNT(*) as count\n",
    "        FROM papers\n",
    "        WHERE abstract_embedding IS NOT NULL\n",
    "          AND withdrawn = 0\n",
    "    ''')\n",
    "    total_count = cursor.fetchone()['count']\n",
    "    \n",
    "    # Now get all embeddings\n",
    "    cursor.execute('''\n",
    "        SELECT id, abstract_embedding\n",
    "        FROM papers\n",
    "        WHERE abstract_embedding IS NOT NULL\n",
    "          AND withdrawn = 0\n",
    "    ''')\n",
    "    \n",
    "    # Get first row to determine embedding dimension\n",
    "    first_row = cursor.fetchone()\n",
    "    first_embedding = np.frombuffer(first_row['abstract_embedding'], dtype=np.float32)\n",
    "    embedding_dim = len(first_embedding)\n",
    "    \n",
    "    # Pre-allocate arrays\n",
    "    papers = [first_row['id']]\n",
    "    embeddings = np.empty((total_count, embedding_dim), dtype=np.float32)\n",
    "    embeddings[0] = first_embedding  # Store first embedding\n",
    "    \n",
    "    # Load the rest\n",
    "    for i, row in enumerate(tqdm(cursor, total=total_count-1), start=1):\n",
    "        papers.append(row['id'])\n",
    "        embeddings[i] = np.frombuffer(row['abstract_embedding'], dtype=np.float32)\n",
    "    \n",
    "    print(f\"\\nLoaded {len(papers)} papers with embeddings\")\n",
    "    print(f\"Embedding shape: {embeddings.shape}\")\n",
    "    \n",
    "    return papers, embeddings\n",
    "\n",
    "# Load the data\n",
    "paper_ids, embeddings = load_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. UMAP Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# @title UMAP Parameters {\"run\": \"auto\"}\n",
    "n_components = 4  # @param {type:\"slider\", min:2, max:8, step:1}\n",
    "n_neighbors = 15  # @param {type:\"slider\", min:5, max:50, step:5}\n",
    "min_dist = 0.05  # @param {type:\"slider\", min:0.01, max:0.5, step:0.05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_existing_umap_run(n_components, n_neighbors, min_dist):\n",
    "    \"\"\"Check if a UMAP run with these parameters exists\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT run_id \n",
    "        FROM umap_runs \n",
    "        WHERE n_components = ? \n",
    "          AND n_neighbors = ? \n",
    "          AND min_dist = ?\n",
    "    ''', (n_components, n_neighbors, min_dist))\n",
    "    \n",
    "    result = cursor.fetchone()\n",
    "    return result['run_id'] if result else None\n",
    "\n",
    "def perform_umap_reduction(embeddings, n_components, n_neighbors, min_dist):\n",
    "    \"\"\"Reduce dimensionality using UMAP\"\"\"\n",
    "    print(f\"Performing {n_components}D UMAP reduction...\")\n",
    "    \n",
    "    # Scale the embeddings\n",
    "    scaler = StandardScaler(\n",
    "        with_mean=True,  # Center the data\n",
    "        with_std=True,   # Scale to unit variance\n",
    "        copy=True        # Create a copy to avoid modifying original data\n",
    "    )\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings.astype(np.float32))\n",
    "    \n",
    "    # UMAP reduction with cuML-specific parameters\n",
    "    reducer = UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric='cosine',\n",
    "        init='spectral',\n",
    "        random_state=42,  # For reproducibility\n",
    "        n_epochs=None,    # Auto-select based on dataset size\n",
    "        negative_sample_rate=5,\n",
    "        transform_queue_size=4.0,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    reduced = reducer.fit_transform(scaled_embeddings)\n",
    "    print(f\"Generated {n_components}D embeddings\")\n",
    "    \n",
    "    return reduced\n",
    "\n",
    "def save_umap_run(paper_ids, embeddings, n_components, n_neighbors, min_dist):\n",
    "    \"\"\"Save UMAP run to database\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Start transaction\n",
    "    cursor.execute('BEGIN')\n",
    "    \n",
    "    try:\n",
    "        # Save run parameters\n",
    "        cursor.execute('''\n",
    "            INSERT INTO umap_runs (\n",
    "                n_components,\n",
    "                n_neighbors,\n",
    "                min_dist\n",
    "            ) VALUES (?, ?, ?)\n",
    "        ''', (n_components, n_neighbors, min_dist))\n",
    "        \n",
    "        run_id = cursor.lastrowid\n",
    "        \n",
    "        # Save embeddings\n",
    "        for i, paper_id in enumerate(tqdm(paper_ids, desc=\"Saving embeddings\")):\n",
    "            # Store as float32 to match the original embeddings\n",
    "            embedding_bytes = embeddings[i].astype(np.float32).tobytes()\n",
    "            cursor.execute('''\n",
    "                INSERT INTO umap_results (\n",
    "                    run_id,\n",
    "                    paper_id,\n",
    "                    embedding\n",
    "                ) VALUES (?, ?, ?)\n",
    "            ''', (run_id, paper_id, embedding_bytes))\n",
    "        \n",
    "        conn.commit()\n",
    "        print(f\"Saved UMAP run {run_id}\")\n",
    "        return run_id\n",
    "    except:\n",
    "        # If anything fails, rollback the transaction\n",
    "        conn.rollback()\n",
    "        print(\"Failed to save UMAP run, rolling back changes\")\n",
    "        return None\n",
    "\n",
    "# Check for existing run with requested dimensions\n",
    "cluster_run_id = check_existing_umap_run(n_components, n_neighbors, min_dist)\n",
    "\n",
    "if cluster_run_id:\n",
    "    print(f\"Found existing {n_components}D UMAP run: {cluster_run_id}\")\n",
    "else:\n",
    "    # Perform and save nD reduction\n",
    "    reduced_nd = perform_umap_reduction(embeddings, n_components, n_neighbors, min_dist)\n",
    "    cluster_run_id = save_umap_run(paper_ids, reduced_nd, n_components, n_neighbors, min_dist)\n",
    "\n",
    "# Check if we need a separate 2D run for visualization\n",
    "if n_components != 2:\n",
    "    viz_run_id = check_existing_umap_run(2, n_neighbors, min_dist)\n",
    "    \n",
    "    if viz_run_id:\n",
    "        print(f\"Found existing 2D UMAP run: {viz_run_id}\")\n",
    "    else:\n",
    "        # Perform and save 2D reduction with same parameters\n",
    "        reduced_2d = perform_umap_reduction(embeddings, 2, n_neighbors, min_dist)\n",
    "        viz_run_id = save_umap_run(paper_ids, reduced_2d, 2, n_neighbors, min_dist)\n",
    "        \n",
    "else:\n",
    "    viz_run_id = cluster_run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### UMAP Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved UMAP run for validation\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('''\n",
    "    SELECT paper_id, embedding\n",
    "    FROM umap_results\n",
    "    WHERE run_id = ?\n",
    "    ORDER BY paper_id\n",
    "''', (cluster_run_id,))\n",
    "\n",
    "results = cursor.fetchall()\n",
    "reduced_embeddings = np.vstack([np.frombuffer(r[1], dtype=np.float32) for r in results])\n",
    "\n",
    "# Check for NaN/Inf values\n",
    "if np.any(np.isnan(reduced_embeddings)) or np.any(np.isinf(reduced_embeddings)):\n",
    "    print(\"WARNING: UMAP produced NaN or Inf values!\")\n",
    "\n",
    "# Check for degenerate cases (all points same/too close)\n",
    "distances = cdist(reduced_embeddings, reduced_embeddings)\n",
    "np.fill_diagonal(distances, np.inf)  # Ignore self-distances\n",
    "min_distances = np.min(distances, axis=1)\n",
    "\n",
    "# Calculate trustworthiness score\n",
    "trust_score = trustworthiness(\n",
    "    embeddings,\n",
    "    reduced_embeddings,\n",
    "    n_neighbors=n_neighbors,\n",
    "    metric='euclidean',\n",
    "    convert_dtype=True\n",
    ")\n",
    "\n",
    "validation_metrics = {\n",
    "    'min_distance': float(np.min(min_distances)),\n",
    "    'max_distance': float(np.max(distances)),\n",
    "    'mean_distance': float(np.mean(distances)),\n",
    "    'std_distance': float(np.std(distances)),\n",
    "    'trustworthiness': float(trust_score)\n",
    "}\n",
    "\n",
    "if validation_metrics['min_distance'] < 1e-10:\n",
    "    print(\"WARNING: Some points are extremely close together!\")\n",
    "if validation_metrics['std_distance'] < 1e-6:\n",
    "    print(\"WARNING: Points might be too uniformly distributed!\")\n",
    "\n",
    "print(\"\\nUMAP Validation Metrics:\")\n",
    "print(f\"- Min distance between points: {validation_metrics['min_distance']:.3e}\")\n",
    "print(f\"- Max distance between points: {validation_metrics['max_distance']:.3f}\")\n",
    "print(f\"- Mean distance between points: {validation_metrics['mean_distance']:.3f}\")\n",
    "print(f\"- Std of distances: {validation_metrics['std_distance']:.3f}\")\n",
    "print(f\"- Trustworthiness score: {validation_metrics['trustworthiness']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5. HDBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# @title HDBSCAN Parameters {\"run\": \"auto\"}\n",
    "min_cluster_size = 20  # @param {type:\"slider\", min:5, max:100, step:5}\n",
    "min_samples = 5  # @param {type:\"slider\", min:1, max:20, step:1}\n",
    "cluster_selection_method = \"eom\"  # @param [\"leaf\", \"eom\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_existing_clustering_run(umap_run_id, min_cluster_size, min_samples, cluster_selection_method):\n",
    "    \"\"\"Check if a clustering run with these parameters exists\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT run_id \n",
    "        FROM clustering_runs \n",
    "        WHERE umap_run_id = ?\n",
    "          AND min_cluster_size = ? \n",
    "          AND min_samples = ? \n",
    "          AND cluster_selection_method = ?\n",
    "    ''', (umap_run_id, min_cluster_size, min_samples, cluster_selection_method))\n",
    "    \n",
    "    result = cursor.fetchone()\n",
    "    return result['run_id'] if result else None\n",
    "\n",
    "def load_umap_embeddings(run_id):\n",
    "    \"\"\"Load UMAP embeddings from database\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get paper IDs and embeddings\n",
    "    cursor.execute('''\n",
    "        SELECT paper_id, embedding\n",
    "        FROM umap_results\n",
    "        WHERE run_id = ?\n",
    "        ORDER BY paper_id\n",
    "    ''', (run_id,))\n",
    "    \n",
    "    results = cursor.fetchall()\n",
    "    # The embeddings are stored as float32 in the database\n",
    "    embeddings = np.vstack([np.frombuffer(r[1], dtype=np.float32) for r in results])\n",
    "    paper_ids = [r[0] for r in results]\n",
    "    \n",
    "    return embeddings, paper_ids\n",
    "\n",
    "def evaluate_clustering(embeddings, reduced_embeddings, labels, probabilities):\n",
    "    \"\"\"Calculate comprehensive clustering quality metrics\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Only evaluate assigned points (not noise)\n",
    "    mask = labels != -1\n",
    "    if np.sum(mask) > 1:\n",
    "        # Standard clustering metrics using cuML's silhouette score\n",
    "        metrics['silhouette'] = cython_silhouette_score(\n",
    "            reduced_embeddings[mask],\n",
    "            labels[mask],\n",
    "            metric='euclidean',\n",
    "            convert_dtype=True\n",
    "        )\n",
    "        \n",
    "        metrics['davies_bouldin'] = davies_bouldin_score(\n",
    "            reduced_embeddings[mask],\n",
    "            labels[mask]\n",
    "        )\n",
    "        metrics['calinski_harabasz'] = calinski_harabasz_score(\n",
    "            reduced_embeddings[mask],\n",
    "            labels[mask]\n",
    "        )\n",
    "        \n",
    "        # Cluster statistics\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = np.sum(labels == -1)\n",
    "        metrics['n_clusters'] = n_clusters\n",
    "        metrics['noise_ratio'] = n_noise / len(labels)\n",
    "        metrics['avg_cluster_size'] = np.sum(mask) / n_clusters if n_clusters > 0 else 0\n",
    "        metrics['avg_probability'] = np.mean(probabilities[mask])\n",
    "        \n",
    "        # Semantic coherence using reduced embeddings\n",
    "        unique_labels = sorted(set(labels[mask]))\n",
    "        coherence_scores = []\n",
    "        separation_scores = []\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            cluster_mask = labels == label\n",
    "            cluster_embeddings = reduced_embeddings[cluster_mask]\n",
    "            \n",
    "            # Calculate average pairwise similarity within cluster\n",
    "            similarities = 1 - cdist(cluster_embeddings, cluster_embeddings, metric='euclidean')\n",
    "            np.fill_diagonal(similarities, 0)  # Exclude self-similarity\n",
    "            coherence_scores.append(np.mean(similarities))\n",
    "\n",
    "            # Calculate separation from other clusters\n",
    "            other_embeddings = reduced_embeddings[~cluster_mask]\n",
    "            if len(other_embeddings) > 0:\n",
    "                between_similarities = 1 - cdist(cluster_embeddings, other_embeddings, metric='euclidean')\n",
    "                separation_scores.append(np.mean(between_similarities))\n",
    "                \n",
    "        metrics['avg_coherence'] = np.mean(coherence_scores)\n",
    "        metrics['avg_separation'] = np.mean(separation_scores) if separation_scores else 0\n",
    "\n",
    "    else:\n",
    "        # Default values for failed clustering\n",
    "        metrics.update({\n",
    "            'silhouette': 0,\n",
    "            'davies_bouldin': float('inf'),\n",
    "            'calinski_harabasz': 0,\n",
    "            'n_clusters': 0,\n",
    "            'noise_ratio': 1.0,\n",
    "            'avg_cluster_size': 0,\n",
    "            'avg_probability': 0,\n",
    "            'avg_coherence': 0,\n",
    "            'avg_separation': 0\n",
    "        })\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def save_clustering_run(umap_run_id, paper_ids, labels, probabilities, metrics, clusterer):\n",
    "    \"\"\"Save clustering results to database\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Start transaction\n",
    "    cursor.execute('BEGIN')\n",
    "    \n",
    "    try:\n",
    "        # Save run parameters and metrics\n",
    "        cursor.execute('''\n",
    "            INSERT INTO clustering_runs (\n",
    "                umap_run_id,\n",
    "                min_cluster_size,\n",
    "                min_samples,\n",
    "                cluster_selection_method,\n",
    "                n_clusters,\n",
    "                noise_ratio,\n",
    "                silhouette_score,\n",
    "                davies_bouldin_score,\n",
    "                calinski_harabasz_score,\n",
    "                avg_coherence,\n",
    "                avg_separation\n",
    "            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            umap_run_id,\n",
    "            min_cluster_size,\n",
    "            min_samples,\n",
    "            cluster_selection_method,\n",
    "            metrics['n_clusters'],\n",
    "            metrics['noise_ratio'],\n",
    "            metrics['silhouette'],\n",
    "            metrics['davies_bouldin'],\n",
    "            metrics['calinski_harabasz'],\n",
    "            metrics['avg_coherence'],\n",
    "            metrics['avg_separation']\n",
    "        ))\n",
    "        \n",
    "        run_id = cursor.lastrowid\n",
    "        \n",
    "        # Save hierarchical structure\n",
    "        print(\"Saving hierarchical structure...\")\n",
    "        tree = clusterer.condensed_tree_\n",
    "        for row in tqdm(tree.itertuples(), desc=\"Saving hierarchy\"):\n",
    "            if row.child_size > 1:  # Skip individual points\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO cluster_hierarchy (\n",
    "                        run_id,\n",
    "                        parent_cluster_id,\n",
    "                        child_cluster_id,\n",
    "                        lambda_val,\n",
    "                        child_size\n",
    "                    ) VALUES (?, ?, ?, ?, ?)\n",
    "                ''', (\n",
    "                    run_id,\n",
    "                    int(row.parent),\n",
    "                    int(row.child),\n",
    "                    float(row.lambda_val),\n",
    "                    int(row.child_size)\n",
    "                ))\n",
    "        \n",
    "        # Save paper results\n",
    "        print(\"Saving paper assignments...\")\n",
    "        for i, paper_id in enumerate(tqdm(paper_ids, desc=\"Saving results\")):\n",
    "            cursor.execute('''\n",
    "                INSERT INTO clustering_results (\n",
    "                    run_id,\n",
    "                    paper_id,\n",
    "                    cluster_id,\n",
    "                    cluster_prob\n",
    "                ) VALUES (?, ?, ?, ?)\n",
    "            ''', (\n",
    "                run_id,\n",
    "                paper_id,\n",
    "                int(labels[i]),\n",
    "                float(probabilities[i])\n",
    "            ))\n",
    "        \n",
    "        # Commit all changes\n",
    "        conn.commit()\n",
    "        print(f\"Saved clustering run {run_id}\")\n",
    "        return run_id\n",
    "    except:\n",
    "        # If anything fails, rollback the transaction\n",
    "        conn.rollback()\n",
    "        print(\"Failed to save clustering run, rolling back changes\")\n",
    "        return None\n",
    "\n",
    "# Check for existing run\n",
    "existing_run_id = check_existing_clustering_run(cluster_run_id, min_cluster_size, min_samples, cluster_selection_method)\n",
    "\n",
    "if existing_run_id:\n",
    "    print(f\"Found existing clustering run: {existing_run_id}\")\n",
    "else:\n",
    "    # Load UMAP embeddings\n",
    "    reduced_embeddings, paper_ids = load_umap_embeddings(cluster_run_id)\n",
    "    \n",
    "    # Initialize clusterer\n",
    "    reduced_embeddings = reduced_embeddings.astype(np.float32)\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method=cluster_selection_method,\n",
    "        prediction_data=True,  # Enable prediction capabilities\n",
    "        gen_min_span_tree=True,  # Required for visualization\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Perform clustering\n",
    "    print(\"Performing HDBSCAN clustering...\")\n",
    "    labels = clusterer.fit_predict(reduced_embeddings)\n",
    "    probabilities = clusterer.probabilities_\n",
    "    \n",
    "    # Evaluate results\n",
    "    metrics = evaluate_clustering(embeddings, reduced_embeddings, labels, probabilities)\n",
    "    \n",
    "    # Print clustering statistics\n",
    "    print(\"\\nClustering Results:\")\n",
    "    print(f\"- Number of clusters: {metrics['n_clusters']}\")\n",
    "    print(f\"- Noise points: {metrics['noise_ratio']*100:.1f}%\")\n",
    "    print(f\"- Average cluster size: {metrics['avg_cluster_size']:.1f}\")\n",
    "    print(f\"- Average cluster probability: {metrics['avg_probability']:.3f}\")\n",
    "    print(f\"\\nQuality Metrics:\")\n",
    "    print(f\"- Silhouette Score: {metrics['silhouette']:.3f}\")\n",
    "    print(f\"- Davies-Bouldin Index: {metrics['davies_bouldin']:.3f}\")\n",
    "    print(f\"- Calinski-Harabasz Index: {metrics['calinski_harabasz']:.3f}\")\n",
    "    print(f\"- Average Semantic Coherence: {metrics['avg_coherence']:.3f}\")\n",
    "    print(f\"- Average Cluster Separation: {metrics['avg_separation']:.3f}\")\n",
    "    \n",
    "    # Save results\n",
    "    run_id = save_clustering_run(cluster_run_id, paper_ids, labels, probabilities, metrics, clusterer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 6. Database Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy updated database back to Drive\n",
    "!cp {local_db} \"{db_path}\"\n",
    "print(\"Database backup completed to Google Drive\") "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
