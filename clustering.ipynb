{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI Safety Papers - Clustering Phase\n",
    "\n",
    "This notebook performs clustering analysis on the paper embeddings to identify AI Safety relevant clusters:\n",
    "1. Loads paper embeddings from the database\n",
    "2. Performs UMAP dimensionality reduction (stored separately for reuse)\n",
    "3. Applies HDBSCAN clustering using stored UMAP embeddings\n",
    "4. Evaluates cluster quality and stores results\n",
    "\n",
    "Note: For visualizations and analysis, see visualizations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive # pyright: ignore [reportMissingImports]\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages if running in Colab\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Install and configure PostgreSQL\n",
    "    !sudo apt-get -qq update && sudo apt-get -qq install postgresql postgresql-contrib # pyright: ignore\n",
    "    !sudo service postgresql start # pyright: ignore\n",
    "    !sudo sed -i 's/local\\s*all\\s*postgres\\s*peer/local all postgres trust/' /etc/postgresql/14/main/pg_hba.conf # pyright: ignore\n",
    "    !sudo service postgresql restart # pyright: ignore\n",
    "    \n",
    "    %pip install psycopg2-binary optuna hdbscan umap-learn numpy cupy-cuda12x # pyright: ignore\n",
    "    !git clone https://github.com/rapidsai/rapidsai-csp-utils.git # pyright: ignore\n",
    "    !python rapidsai-csp-utils/colab/pip-install.py # pyright: ignore\n",
    "\n",
    "# Core imports\n",
    "import sqlite3\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "# ML imports\n",
    "from cuml import UMAP\n",
    "from cuml.preprocessing import StandardScaler\n",
    "from cuml.cluster.hdbscan import HDBSCAN\n",
    "from cuml.metrics.trustworthiness import trustworthiness\n",
    "import cuml\n",
    "cuml.set_global_output_type('cupy')\n",
    "\n",
    "# Optimization imports\n",
    "import optuna\n",
    "\n",
    "# Locale fix after install https://github.com/googlecolab/colabtools/issues/3409\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "# Add to Core imports\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "\n",
    "# Additional imports\n",
    "import pickle\n",
    "from itertools import islice\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database configuration\n",
    "db_backup_path = \"/content/drive/MyDrive/ai-safety-papers/papers_postgres.sql\"\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Create PostgreSQL connection with retries\"\"\"\n",
    "    import psycopg2\n",
    "    from psycopg2.extras import DictCursor\n",
    "    \n",
    "    return psycopg2.connect(\n",
    "        host='',  # Empty string for Unix socket connection\n",
    "        database=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        cursor_factory=DictCursor\n",
    "    )\n",
    "\n",
    "# After creating connection but before creating tables:\n",
    "print(\"Loading existing database...\")\n",
    "!psql -U postgres -d postgres -f \"{db_backup_path}\" # pyright: ignore\n",
    "conn = get_db_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create tables\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS artifacts (\n",
    "    trial_id INTEGER NOT NULL,\n",
    "    paper_id TEXT NOT NULL,\n",
    "    umap_embedding BYTEA,\n",
    "    cluster_id INTEGER,\n",
    "    cluster_prob REAL,\n",
    "    PRIMARY KEY (trial_id, paper_id),\n",
    "    FOREIGN KEY (paper_id) REFERENCES papers(id)\n",
    ")''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS cluster_trees (\n",
    "    trial_id INTEGER NOT NULL,\n",
    "    parent_cluster_id INTEGER,\n",
    "    child_cluster_id INTEGER,\n",
    "    lambda_val REAL,\n",
    "    child_size INTEGER,\n",
    "    PRIMARY KEY (trial_id, parent_cluster_id, child_cluster_id)\n",
    ")''')\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "    \"\"\"Load embeddings and precompute k-NN graph\"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            SELECT id, embedding \n",
    "            FROM papers \n",
    "            WHERE embedding IS NOT NULL AND withdrawn = FALSE\n",
    "        ''')\n",
    "        print(f\"Loading embeddings\")\n",
    "        results = cursor.fetchall()\n",
    "    if not results:\n",
    "        raise ValueError(\"No embeddings found in database\")\n",
    "    \n",
    "    # Initialize arrays\n",
    "    paper_ids = [row[0] for row in results]\n",
    "    \n",
    "    print(f\"Scaling embeddings\")\n",
    "    # Pure cupy buffer conversion\n",
    "    raw_embeddings = cp.array([cp.frombuffer(row[1], dtype=cp.float32) for row in results])\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(raw_embeddings)\n",
    "    del raw_embeddings, results  # Free original data\n",
    "    \n",
    "    # Precompute k-NN graph with max neighbors needed\n",
    "    print(\"Precomputing k-NN graph for UMAP...\")\n",
    "    nn_model = NearestNeighbors(n_neighbors=100, metric='cosine')\n",
    "    nn_model.fit(scaled_embeddings)\n",
    "    knn_graph = nn_model.kneighbors_graph(scaled_embeddings, mode='distance')\n",
    "    \n",
    "    print(f\"Done precomputing graph\")\n",
    "    return paper_ids, scaled_embeddings, knn_graph\n",
    "\n",
    "paper_ids, embeddings, knn_graph = load_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 4. Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perform_umap_reduction(embeddings, n_components, n_neighbors, min_dist, knn_graph):\n",
    "    \"\"\"UMAP using precomputed k-NN graph\"\"\"\n",
    "    print(f\"\\nPerforming {n_components}D UMAP reduction with parameters:\")\n",
    "    print(f\"n_neighbors: {n_neighbors}, min_dist: {min_dist}\")\n",
    "    \n",
    "    reducer = UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        precomputed_knn=knn_graph,\n",
    "        metric='cosine',\n",
    "        output_type='cupy'\n",
    "    )\n",
    "    result = reducer.fit_transform(embeddings)\n",
    "    print(f\"UMAP reduction complete. Output shape: {result.shape}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 5. Optimization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_relative_validity(minimum_spanning_tree, labels):\n",
    "    \"\"\"CPU-based relative validity score using HDBSCAN's MST\"\"\"\n",
    "    # Convert labels to numpy array for CPU operations\n",
    "    labels = cp.asnumpy(labels)  # Move to CPU\n",
    "    \n",
    "    # Extract edge information from MST (already CPU-based)\n",
    "    mst_df = minimum_spanning_tree.to_pandas()\n",
    "    \n",
    "    # Initialize metrics\n",
    "    noise_mask = labels == -1\n",
    "    valid_labels = labels[~noise_mask]\n",
    "    \n",
    "    if valid_labels.size == 0:\n",
    "        return -1.0  # All noise case\n",
    "    \n",
    "    cluster_sizes = np.bincount(valid_labels)\n",
    "    num_clusters = len(cluster_sizes)\n",
    "    total = len(labels)\n",
    "    \n",
    "    # Use numpy instead of cupy\n",
    "    DSC = np.zeros(num_clusters, dtype=np.float32)\n",
    "    DSPC_wrt = np.ones(num_clusters, dtype=np.float32) * np.inf\n",
    "    max_distance = 0.0\n",
    "    min_outlier_sep = np.inf\n",
    "\n",
    "    # Process edges using vectorized operations\n",
    "    edge_data = mst_df[['from', 'to', 'distance']].values\n",
    "    for from_idx, to_idx, length in edge_data:\n",
    "        max_distance = max(max_distance, length)\n",
    "        \n",
    "        label1 = labels[int(from_idx)]\n",
    "        label2 = labels[int(to_idx)]\n",
    "        \n",
    "        if label1 == -1 and label2 == -1:\n",
    "            continue\n",
    "        elif label1 == -1 or label2 == -1:\n",
    "            min_outlier_sep = min(min_outlier_sep, length)\n",
    "            continue\n",
    "            \n",
    "        if label1 == label2:\n",
    "            DSC[label1] = max(length, DSC[label1])\n",
    "        else:\n",
    "            DSPC_wrt[label1] = min(length, DSPC_wrt[label1])\n",
    "            DSPC_wrt[label2] = min(length, DSPC_wrt[label2])\n",
    "\n",
    "    # Handle edge cases\n",
    "    if np.isinf(min_outlier_sep):\n",
    "        min_outlier_sep = max_distance if num_clusters > 1 else max_distance\n",
    "        \n",
    "    # Correct infinite values\n",
    "    correction = 2.0 * (max_distance if num_clusters > 1 else min_outlier_sep)\n",
    "    DSPC_wrt = np.where(DSPC_wrt == np.inf, correction, DSPC_wrt)\n",
    "    \n",
    "    # Compute final score\n",
    "    V_index = (DSPC_wrt - DSC) / np.maximum(DSPC_wrt, DSC)\n",
    "    weighted_V = (cluster_sizes * V_index) / total\n",
    "    result = float(np.sum(weighted_V))\n",
    "    \n",
    "    # Explicit cleanup\n",
    "    del labels, mst_df\n",
    "    return result\n",
    "\n",
    "def get_optuna_storage():\n",
    "    return \"postgresql://postgres@/postgres\"  # Omit host entirely for Unix socket\n",
    "\n",
    "def save_sampler(study):\n",
    "    \"\"\"Save sampler state to Google Drive\"\"\"\n",
    "    drive_path = \"/content/drive/MyDrive/ai-safety-papers\"\n",
    "    sampler_path = f\"{drive_path}/sampler.pkl\"\n",
    "    with open(sampler_path, \"wb\") as f:\n",
    "        pickle.dump(study.sampler, f)\n",
    "\n",
    "def load_sampler():\n",
    "    \"\"\"Load sampler from Google Drive if exists\"\"\"\n",
    "    drive_path = \"/content/drive/MyDrive/ai-safety-papers\"\n",
    "    sampler_path = f\"{drive_path}/sampler.pkl\"\n",
    "    if os.path.exists(sampler_path):\n",
    "        with open(sampler_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def calculate_metrics(cluster_persistences, labels, use_umap, original_embeddings, processed_embeddings):\n",
    "    \"\"\"Calculate all metrics while maintaining GPU arrays where possible\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    if use_umap:\n",
    "        # Keep data on GPU for trustworthiness calculation\n",
    "        metrics['trust_score'] = trustworthiness(original_embeddings, processed_embeddings)\n",
    "    else:\n",
    "        metrics['trust_score'] = None\n",
    "    \n",
    "    valid_mask = labels != -1\n",
    "    valid_labels = labels[valid_mask]\n",
    "    \n",
    "    if valid_labels.size > 0:\n",
    "        # Use Cupy for GPU-accelerated calculations\n",
    "        cluster_sizes = cp.bincount(valid_labels)\n",
    "        persistence = cluster_persistences\n",
    "        \n",
    "        metrics.update({\n",
    "            'noise_ratio': cp.sum(~valid_mask).item() / len(labels),\n",
    "            'n_clusters': len(cluster_sizes),\n",
    "            'mean_persistence': cp.mean(persistence).item(),\n",
    "            'std_persistence': cp.std(persistence).item(),\n",
    "            'mean_cluster_size': cluster_sizes.mean().item(),\n",
    "            'std_cluster_size': cluster_sizes.std().item(),\n",
    "            'cluster_size_ratio': (cluster_sizes.max() / cluster_sizes.min()).item()\n",
    "        })\n",
    "    else:\n",
    "        metrics.update({\n",
    "            'noise_ratio': 1.0,\n",
    "            'n_clusters': 0,\n",
    "            'mean_persistence': 0.0,\n",
    "            'std_persistence': 0.0,\n",
    "            'mean_cluster_size': 0.0,\n",
    "            'std_cluster_size': 0.0,\n",
    "            'cluster_size_ratio': 0.0\n",
    "        })\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "def batched(iterable, n):\n",
    "    iterator = iter(iterable)\n",
    "    while batch := list(islice(iterator, n)):\n",
    "        yield batch\n",
    "\n",
    "def objective(trial, scaled_embeddings, knn_graph):\n",
    "    \"\"\"Optuna optimization objective function\"\"\"\n",
    "    # UMAP configuration\n",
    "    use_umap = trial.suggest_categorical('use_umap', [True, False])\n",
    "    \n",
    "    if use_umap:\n",
    "        umap_params = {\n",
    "            'n_components': trial.suggest_int('n_components', 15, 100),\n",
    "            'n_neighbors': trial.suggest_int('n_neighbors', 30, 100),\n",
    "            'min_dist': 0.0\n",
    "        }\n",
    "        \n",
    "        # Always compute fresh UMAP\n",
    "        reducer = UMAP(\n",
    "            **umap_params,\n",
    "            precomputed_knn=knn_graph,\n",
    "            metric='cosine',\n",
    "            output_type='cupy'\n",
    "        )\n",
    "        reduced_embeddings = reducer.fit_transform(scaled_embeddings).astype(cp.float32)\n",
    "        del reducer\n",
    "    else:\n",
    "        reduced_embeddings = scaled_embeddings  # Already cupy\n",
    "\n",
    "    # HDBSCAN parameters\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=trial.suggest_int('min_cluster_size', 20, 100),\n",
    "        min_samples=trial.suggest_int('min_samples', 5, 50),\n",
    "        cluster_selection_epsilon=trial.suggest_float('cluster_selection_epsilon', 0.0, 0.5),\n",
    "        cluster_selection_method='leaf',\n",
    "        gen_min_span_tree=True,\n",
    "        output_type='cupy'\n",
    "    )\n",
    "    \n",
    "    # Extract needed components first\n",
    "    labels = clusterer.fit_predict(reduced_embeddings)\n",
    "    mst = clusterer.minimum_spanning_tree_\n",
    "    tree_df = clusterer.condensed_tree_.to_pandas()\n",
    "    probabilities = clusterer.probabilities_\n",
    "    cluster_persistences = clusterer.cluster_persistence_\n",
    "    del clusterer  # ← Release hierarchy data\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(cluster_persistences, labels, use_umap, scaled_embeddings, reduced_embeddings)\n",
    "    dbcvi_score = compute_relative_validity(mst, labels)\n",
    "\n",
    "    # Print metrics in readable format\n",
    "    print(f\"\\nTrial {trial.number} results:\")\n",
    "    print(f\"  • Noise ratio: {metrics['noise_ratio']:.1%}\")\n",
    "    print(f\"  • Clusters: {metrics['n_clusters']}\")\n",
    "    print(f\"  • Avg cluster size: {metrics['mean_cluster_size']:.1f} ± {metrics['std_cluster_size']:.1f}\")\n",
    "    print(f\"  • Trust score: {metrics['trust_score']:.3f}\" if metrics['trust_score'] else \"  • Trust score: N/A\")\n",
    "    print(f\"  • DBCVI score: {dbcvi_score:.3f}\")\n",
    "\n",
    "    # Store metrics (excluding dbcvi_score which is the objective value)\n",
    "    for k, v in metrics.items():\n",
    "        trial.set_user_attr(k, v)\n",
    "    \n",
    "    # Save combined artifacts\n",
    "    for batch in batched((\n",
    "        (trial.number, pid, \n",
    "         emb.tobytes() if use_umap else None,\n",
    "         int(cluster.item()), \n",
    "         float(prob.item()))\n",
    "        for pid, emb, cluster, prob in zip(\n",
    "            paper_ids, \n",
    "            reduced_embeddings,\n",
    "            labels.get(),  # Convert cupy→numpy once for entire array\n",
    "            probabilities.get()  # Same here\n",
    "        )\n",
    "    ), BATCH_SIZE):\n",
    "        cursor.executemany('''\n",
    "            INSERT INTO artifacts\n",
    "            VALUES (%s, %s, %s, %s, %s)\n",
    "        ''', batch)\n",
    "        conn.commit()\n",
    "    \n",
    "    # Save hierarchy tree\n",
    "    meaningful_edges = tree_df[tree_df.child_size > 1]\n",
    "    cursor.executemany('''\n",
    "        INSERT INTO cluster_trees\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "    ''', [\n",
    "        (trial.number, int(row.parent), int(row.child), \n",
    "         float(row.lambda_val), int(row.child_size))\n",
    "        for row in meaningful_edges.itertuples()\n",
    "    ])\n",
    "    \n",
    "    conn.commit()\n",
    "    gc.collect()  # Force garbage collection after each trial\n",
    "    return dbcvi_score\n",
    "\n",
    "def optimize_clustering(embeddings, knn_graph, n_jobs, n_trials):\n",
    "    \"\"\"Run optimization study with Optuna integration\"\"\"\n",
    "    study = optuna.create_study(\n",
    "        study_name=\"ai-papers-clustering\",\n",
    "        storage=get_optuna_storage(),\n",
    "        direction='maximize',\n",
    "        load_if_exists=True,\n",
    "        sampler=load_sampler()\n",
    "    )\n",
    "    \n",
    "    # Save sampler periodically\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, embeddings, knn_graph),\n",
    "        n_jobs=n_jobs,\n",
    "        n_trials=n_trials,\n",
    "        callbacks=[lambda study, trial: save_sampler(study)]\n",
    "    )\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 6. Run Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "study = optimize_clustering(embeddings, knn_graph, n_jobs=3, n_trials=50)\n",
    "print(\"Optimization complete! Best parameters saved to database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 7. Database Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup_database():\n",
    "    \"\"\"Backup PostgreSQL database to Google Drive\"\"\"\n",
    "    backup_path = \"/content/drive/MyDrive/ai-safety-papers/papers_postgres.sql\"\n",
    "    print(f\"Creating PostgreSQL backup at {backup_path}\")\n",
    "    !pg_dump -U postgres -F p -f \"{backup_path}\" postgres  # pyright: ignore\n",
    "    print(\"Backup completed successfully\")\n",
    "\n",
    "# Run backup after saving data\n",
    "backup_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Unassign GPU to free up resources\n",
    "from google.colab import runtime # pyright: ignore [reportMissingImports]\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
